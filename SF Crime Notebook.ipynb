{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UC Berkeley W207 - Machine Learning\n",
    "Miki Seltzer, Eric Freeman, Christopher Llop\n",
    "\n",
    "This Notebook contains our solution to the Kaggle SF Crime problem available at:\n",
    "https://www.kaggle.com/c/sf-crime\n",
    "\n",
    "The notebook covers the following methods and includes discussion of key decisions along the way (as of 2015-06-17). If you are not part of our W207 class, you are welcome to look through our methods and leverage our code to develop new solutions. We may also continue work after creating this report - just go by the \"last modified\" date on GitHub.\n",
    "\n",
    "Concepts Covered:\n",
    "1. Basic data exploration\n",
    "2. K-Nearest Neighbors\n",
    "3. Logistic Regression\n",
    "4. Bernoulli Naive Bayes\n",
    "5. Gradient Boosting\n",
    "6. Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Libraries for Neural Networks. The versions currently available via \"pip\" are not up to date, and do\n",
    "# not sync with each other. To use these packages, install directly from github using the instructions\n",
    "# here:\n",
    "# https://github.com/dnouri/nolearn\n",
    "from lasagne import layers\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a basic exploration of the data. For the purposes of this report only some of those checks are\n",
    "shown below. You can examine these searches further in the \"Load and Examine Data\" iPython notebook. \n",
    "\n",
    "Some tasks:    \n",
    "- Count unique values for each variable\n",
    "- View most common values\n",
    "- XY plot of lat/long w. circles to indicate number of crimes\n",
    "- Time series plots to see how category use changes over time\n",
    "\n",
    "Interesting Points:\n",
    "- Most crime on Friday, then Wednesday. Least on Sunday.\n",
    "- X and Y latitude have same number of distinct values. Seem to be somehow linked to locations\n",
    "  since, despite there being a lots of sig fig, they still can be frequency counted\n",
    "- 800 Block of BRYANT ST has 4x+ more data points than anyplace else. Seems to link w/ most freq X and Y\n",
    "- \"Other Offenses\" are common\n",
    "- The dates with the most crime are new years day. Also the first of months.\n",
    "- Note: Strange max value of Y = 90 for 67 values. These appear to be in Chicago, but the data has addresses in SF. We removed this data from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 878,049, values.\n",
      "There are a total of 389,257 Dates.\n",
      "There are a total of 39 Category.\n",
      "There are a total of 879 Descript.\n",
      "There are a total of 7 DayOfWeek.\n",
      "There are a total of 10 PdDistrict.\n",
      "There are a total of 17 Resolution.\n",
      "There are a total of 23,228 Address.\n",
      "There are a total of 34,243 X.\n",
      "There are a total of 34,243 Y.\n",
      "-------------------------------------------------------------------------\n",
      "There are a total of 39 distinct Category values, as follows: \n",
      "LARCENY/THEFT                  0.199192\n",
      "OTHER OFFENSES                 0.143707\n",
      "NON-CRIMINAL                   0.105124\n",
      "ASSAULT                        0.087553\n",
      "DRUG/NARCOTIC                  0.061467\n",
      "VEHICLE THEFT                  0.061251\n",
      "VANDALISM                      0.050937\n",
      "WARRANTS                       0.048077\n",
      "BURGLARY                       0.041860\n",
      "SUSPICIOUS OCC                 0.035777\n",
      "MISSING PERSON                 0.029599\n",
      "ROBBERY                        0.026194\n",
      "FRAUD                          0.018996\n",
      "FORGERY/COUNTERFEITING         0.012082\n",
      "SECONDARY CODES                0.011372\n",
      "WEAPON LAWS                    0.009743\n",
      "PROSTITUTION                   0.008523\n",
      "TRESPASS                       0.008343\n",
      "STOLEN PROPERTY                0.005171\n",
      "SEX OFFENSES FORCIBLE          0.004997\n",
      "DISORDERLY CONDUCT             0.004920\n",
      "DRUNKENNESS                    0.004874\n",
      "RECOVERED VEHICLE              0.003574\n",
      "KIDNAPPING                     0.002666\n",
      "DRIVING UNDER THE INFLUENCE    0.002583\n",
      "RUNAWAY                        0.002216\n",
      "LIQUOR LAWS                    0.002167\n",
      "ARSON                          0.001723\n",
      "LOITERING                      0.001395\n",
      "EMBEZZLEMENT                   0.001328\n",
      "SUICIDE                        0.000579\n",
      "FAMILY OFFENSES                0.000559\n",
      "BAD CHECKS                     0.000462\n",
      "BRIBERY                        0.000329\n",
      "EXTORTION                      0.000292\n",
      "SEX OFFENSES NON FORCIBLE      0.000169\n",
      "GAMBLING                       0.000166\n",
      "PORNOGRAPHY/OBSCENE MAT        0.000025\n",
      "TREA                           0.000007\n",
      "dtype: float64\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "There are a total of 10 distinct PdDistrict values, as follows: \n",
      "SOUTHERN      0.179013\n",
      "MISSION       0.136562\n",
      "NORTHERN      0.119920\n",
      "BAYVIEW       0.101852\n",
      "CENTRAL       0.097329\n",
      "TENDERLOIN    0.093171\n",
      "INGLESIDE     0.089796\n",
      "TARAVAL       0.074707\n",
      "PARK          0.056162\n",
      "RICHMOND      0.051488\n",
      "dtype: float64\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "There are a total of 17 distinct Resolution values, as follows: \n",
      "NONE                                      0.599955\n",
      "ARREST, BOOKED                            0.235070\n",
      "ARREST, CITED                             0.087699\n",
      "LOCATED                                   0.019476\n",
      "PSYCHOPATHIC CASE                         0.016553\n",
      "UNFOUNDED                                 0.010916\n",
      "JUVENILE BOOKED                           0.006337\n",
      "COMPLAINANT REFUSES TO PROSECUTE          0.004528\n",
      "DISTRICT ATTORNEY REFUSES TO PROSECUTE    0.004480\n",
      "NOT PROSECUTED                            0.004230\n",
      "JUVENILE CITED                            0.003795\n",
      "PROSECUTED BY OUTSIDE AGENCY              0.002852\n",
      "EXCEPTIONAL CLEARANCE                     0.001742\n",
      "JUVENILE ADMONISHED                       0.001657\n",
      "JUVENILE DIVERTED                         0.000404\n",
      "CLEARED-CONTACT JUVENILE FOR MORE INFO    0.000247\n",
      "PROSECUTED FOR LESSER OFFENSE             0.000058\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_orig = pd.read_csv(\"train.csv\")\n",
    "test_orig = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Count distinct for each variable:\n",
    "print \"There are a total of {:,}, values.\".format(len(data_orig))\n",
    "\n",
    "for var, series in data_orig.iteritems():\n",
    "    print \"There are a total of {:,} {}.\".format(len(series.value_counts()), var)\n",
    "# View All of Categories, PdDistrict, Resolution, DayOfWeek\n",
    "variables = [\"Category\", \"PdDistrict\", \"Resolution\"]\n",
    "x = data_orig[\"Category\"].value_counts()/len(data_orig)\n",
    "for col in variables:\n",
    "    print \"-------------------------------------------------------------------------\"\n",
    "    print \"There are a total of {:,} distinct {} values, as follows: \".format(len(data_orig[col].value_counts()), col)\n",
    "    print data_orig[col].value_counts()/len(data_orig)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added several new date related features to the dataset right away, which were used in data investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_clean = data_orig[data_orig.Y != 90]\n",
    "\n",
    "def add_date_vars(data):\n",
    "    data['Dates'] = pd.to_datetime(data['Dates'])\n",
    "    data['Year'] = data.Dates.dt.year\n",
    "    data['Month'] = data.Dates.dt.month\n",
    "    data['Day'] = data.Dates.dt.day\n",
    "    data['Date'] = data.Dates.dt.date\n",
    "    data['Hour'] = data.Dates.dt.hour\n",
    "    data['DayOfYear'] = data.Dates.dt.dayofyear\n",
    "    data['WeekDay'] = data.Dates.dt.weekday\n",
    "    \n",
    "    datetime_vector = data['Dates']\n",
    "    date_vector = datetime_vector.dt.date\n",
    "    date_diff_vector = (date_vector - date_vector.min()) / np.timedelta64(1, 'D')\n",
    "    data['DateDiff'] = date_diff_vector\n",
    "\n",
    "\n",
    "add_date_vars(data_clean)\n",
    "add_date_vars(test_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a standard \"output\" function that we used across sensitivites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function that will create a correctly formatted output file for submission to Kaggle.\n",
    "\n",
    "def create_submission(preds):\n",
    "    labels = [\"Id\",\"ARSON\",\"ASSAULT\",\"BAD CHECKS\",\"BRIBERY\",\"BURGLARY\",\"DISORDERLY CONDUCT\",\"DRIVING UNDER THE INFLUENCE\",\n",
    "              \"DRUG/NARCOTIC\",\"DRUNKENNESS\",\"EMBEZZLEMENT\",\"EXTORTION\",\"FAMILY OFFENSES\",\"FORGERY/COUNTERFEITING\",\n",
    "              \"FRAUD\",\"GAMBLING\",\"KIDNAPPING\",\"LARCENY/THEFT\",\"LIQUOR LAWS\",\"LOITERING\",\"MISSING PERSON\",\"NON-CRIMINAL\",\n",
    "              \"OTHER OFFENSES\",\"PORNOGRAPHY/OBSCENE MAT\",\"PROSTITUTION\",\"RECOVERED VEHICLE\",\"ROBBERY\",\"RUNAWAY\",\n",
    "              \"SECONDARY CODES\",\"SEX OFFENSES FORCIBLE\",\"SEX OFFENSES NON FORCIBLE\",\"STOLEN PROPERTY\",\"SUICIDE\",\n",
    "              \"SUSPICIOUS OCC\",\"TREA\",\"TRESPASS\",\"VANDALISM\",\"VEHICLE THEFT\",\"WARRANTS\",\"WEAPON LAWS\"]\n",
    "    head_str = ','.join(labels)\n",
    "\n",
    "    num_cats = len(labels)\n",
    "    \n",
    "    # Make a dummy row to append to\n",
    "    ids = np.arange(preds.shape[0])[np.newaxis].transpose()\n",
    "    \n",
    "    results = np.column_stack((ids, preds))\n",
    "\n",
    "    num_form = ['%6f'] * (num_cats - 1)\n",
    "    num_form.insert(0, '%d')\n",
    "    # Write results to csv\n",
    "    np.savetxt('sample.csv', results, fmt=num_form, delimiter=',', header=head_str, comments='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model was a simple KNN model. We tried a variety of N's for neighbors and both normalized and regular data. We also looked at just including a subset of crimes for it to predict. We first submitted a model using N=1, normalized data, and only attempted to predict the top 4 crimes. \n",
    "\n",
    "KNN makes classification predictions based on the nearest neighbors - the predicted category is set to 1 and all others to 0. This resulted in a low score (27) because the ranking algorithm expected us to submit the likelihood of each potential classification, instead of just an absolute guess.\n",
    "\n",
    "For the final KNN submission, we replaced the 0's with average probability of the crime as observed in the entire dataset. This simple change improved submission score from 27 to 2.92.\n",
    "\n",
    "We then tried to optimize N, and found that N = 1000 made an improvement to accuracy, but only by about 2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only look at top 4 crimes\n",
    "stk_list = ['LARCENY/THEFT','OTHER OFFENSES','NON-CRIMINAL','ASSAULT']\n",
    "data = data_clean[data_clean.Category.isin(stk_list)]\n",
    "\n",
    "# Create random dev sample so we can see how that accuracy compares to our Kaggle results\n",
    "np.random.seed(100)\n",
    "\n",
    "rows = np.random.choice(data.index, size = len(data) / 10, replace = False)\n",
    "\n",
    "dev = data.ix[rows]\n",
    "train = data.drop(rows)\n",
    "\n",
    "# Convert to Numpy Format with only DateDiff, X and Y features\n",
    "train_data = np.array(train[['DateDiff','X','Y']].values)\n",
    "train_labels = np.array(train[['Category']].values.ravel())\n",
    "\n",
    "dev_data = np.array(dev[['DateDiff','X','Y']].values)\n",
    "dev_labels = np.array(dev[['Category']].values.ravel())\n",
    "\n",
    "full_data = np.array(data[['DateDiff','X','Y']].values)\n",
    "full_labels = np.array(data[['Category']].values.ravel())\n",
    "\n",
    "test_data = np.array(test_orig[['DateDiff','X','Y']].values)\n",
    "\n",
    "# Normalize Data to Between 0-1\n",
    "#a + (x-A)*(b-a)/(B-A) \n",
    "\n",
    "def normalize(data):\n",
    "    return 0 + (np.abs(data) - np.abs(data).min(axis=0))*(1-0)/(np.abs(data).max(axis=0) - np.abs(data).min(axis=0))\n",
    "\n",
    "train_normed = normalize(train_data)\n",
    "dev_normed = normalize(dev_data)\n",
    "test_normed = normalize(test_data)\n",
    "full_normed = normalize(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores for each k value was [mean: 0.22013, std: 0.03731, params: {'n_neighbors': 1}, mean: 0.22913, std: 0.04194, params: {'n_neighbors': 5}, mean: 0.23831, std: 0.04801, params: {'n_neighbors': 10}, mean: 0.26714, std: 0.06603, params: {'n_neighbors': 100}, mean: 0.30380, std: 0.05516, params: {'n_neighbors': 1000}] \n",
      "\n",
      "The best k value was {'n_neighbors': 1000} with accuracy 0.3038\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to find a good number of neighbors.\n",
    "\n",
    "# ks = {'n_neighbors': [1,2,3,4,5,6,7,8,9,10]}\n",
    "ks = {'n_neighbors': [1, 5, 10, 100, 1000]}\n",
    "KNNGridSearch = GridSearchCV(KNeighborsClassifier(), ks, scoring='accuracy')\n",
    "KNNGridSearch.fit(train_normed, train_labels)\n",
    "\n",
    "# Report out on the accuracies    \n",
    "print \"The scores for each N value was %s \" % (KNNGridSearch.grid_scores_)\n",
    "print \"\\nThe best N value was %s with accuracy %.4f\" % (KNNGridSearch.best_params_, KNNGridSearch.best_score_)\n",
    "best_n = KNNGridSearch.best_params_['n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAFHCAYAAAD+ygs0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f944f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors\n",
      "\n",
      "model           neighbors accuracy  \n",
      "-----------------------------------\n",
      "baseline        5         0.4179    \n",
      "optimized n     1000      0.4397    \n"
     ]
    }
   ],
   "source": [
    "res = zip(*[(f1m, f1s.std(), p['n_neighbors']) \n",
    "            for p, f1m, f1s in KNNGridSearch.grid_scores_])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.log10(res[2]),res[0],'-o')\n",
    "plt.xlabel(\"Log(N)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.log10(res[2]),res[1],'-o')\n",
    "plt.xlabel(\"Log(N)\")\n",
    "plt.ylabel(\"StDev of Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "result_form = \"{:<16s}{:<10d}{:<10.4f}\".format\n",
    "print \"K-Nearest Neighbors\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}\".format(\"model\", \"neighbors\", \"accuracy\")\n",
    "print \"-----------------------------------\"\n",
    "\n",
    "# Now that we've done this, let's run the KNN on the full train, apply to the test, then format.\n",
    "KNNmodel = KNeighborsClassifier()\n",
    "KNNmodel.fit(train_normed, train_labels)\n",
    "base_n = 5\n",
    "base_accuracy = KNNmodel.score(dev_normed, dev_labels)\n",
    "print result_form(\"baseline\", base_n, base_accuracy)\n",
    "\n",
    "# Use optimized N\n",
    "KNNmodel = KNeighborsClassifier(n_neighbors = best_n)\n",
    "KNNmodel.fit(train_normed, train_labels)\n",
    "accuracy = KNNmodel.score(dev_normed, dev_labels)\n",
    "print result_form(\"optimized n\", best_n, accuracy)\n",
    "# test_predict = KNNmodel.predict_proba(test_normed).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that entering the other 35 categories with their average probability was done by hand in Excel. We looked at doing it in python, but realized we wanted to pursue other models instead. The create_submission function needs all categories to be input, so in this case we did not use the create_submission function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model was logistic regression. A logistic regression method (and all others examined in this notebook) allows output of predicted probabilites, removing the need to use the simple occurance averages.\n",
    "\n",
    "We tried several values of C, which determines the regularization strength, before settling on C=0.001. This improved our score from 2.92 to 2.69.\n",
    "\n",
    "After submitting this model, we tried averaging the KNN and LR in various ways, but that did not improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# Let's go back to using all crimes\n",
    "data = data_clean.copy()\n",
    "\n",
    "np.random.seed()\n",
    "rows = np.random.choice(data.index, size = len(data) / 10, replace = False)\n",
    "\n",
    "dev = data.ix[rows]\n",
    "train = data.drop(rows)\n",
    "\n",
    "# Convert to Numpy Format with only DateDiff, X and Y features\n",
    "train_data = np.array(train[['DateDiff','X','Y']].values)\n",
    "train_labels = np.array(train[['Category']].values.ravel())\n",
    "\n",
    "dev_data = np.array(dev[['DateDiff','X','Y']].values)\n",
    "dev_labels = np.array(dev[['Category']].values.ravel())\n",
    "\n",
    "full_data = np.array(data[['DateDiff','X','Y']].values)\n",
    "full_labels = np.array(data[['Category']].values.ravel())\n",
    "\n",
    "test_data = np.array(test_orig[['DateDiff','X','Y']].values)\n",
    "\n",
    "# We need to re-normalize the data sets containing all crimes, not just the top 4 crimes\n",
    "train_normed = normalize(train_data)\n",
    "dev_normed = normalize(dev_data)\n",
    "test_normed = normalize(test_data)\n",
    "full_normed = normalize(full_data)\n",
    "\n",
    "print len(dev['Category'].unique())\n",
    "print len(train['Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
    "clf.fit(train_data, train_labels)\n",
    "print clf.best_params_\n",
    "best_c = clf.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAFHCAYAAADz1HtzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERE1ECStoiIiIga+P+3YHYTbmzJYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e9966a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\n",
      "model           C         accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "baseline        1.0000    0.2156    2.6448    \n",
      "optimized C     0.0010    0.2148    2.6977    \n"
     ]
    }
   ],
   "source": [
    "res = zip(*[(f1m, f1s.std(), p['C']) \n",
    "            for p, f1m, f1s in clf.grid_scores_])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.log10(res[2]),res[0],'-o')\n",
    "plt.xlabel(\"Log(C)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.log10(res[2]),res[1],'-o')\n",
    "plt.xlabel(\"Log(C)\")\n",
    "plt.ylabel(\"StDev of Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "result_form = \"{:<16s}{:<10.4f}{:<10.4f}{:<10.4f}\".format\n",
    "print \"Logistic Regression\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"model\", \"C\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Logistic Regression: baseline\n",
    "model = LogisticRegression()\n",
    "model.fit(train_normed, train_labels)\n",
    "base_c = 1.0\n",
    "base_accuracy = model.score(dev_normed, dev_labels)\n",
    "base_logloss = log_loss(dev_labels, model.predict_proba(dev_normed)) \n",
    "print result_form(\"baseline\", base_c, base_accuracy, base_logloss)\n",
    "\n",
    "# Logistic Regression: optimized C\n",
    "model.set_params(C=best_c)\n",
    "model.fit(train_normed, train_labels)\n",
    "test_predict = model.predict_proba(test_normed)\n",
    "accuracy = model.score(dev_normed, dev_labels)\n",
    "logloss = log_loss(dev_labels, model.predict_proba(dev_normed)) \n",
    "print result_form(\"optimized C\", best_c, accuracy, logloss)\n",
    "\n",
    "#results = create_submission(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bernoulli Naive Bayes\n",
    "##Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BernoulliNB requires binary data, so we created dummy variables for each variable in the data set (all date and district variables). We also left all variables in the data set so that we could include and exclude features in our models easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates' 'Category' 'Descript' 'DayOfWeek' 'PdDistrict' 'Resolution'\n",
      " 'Address' 'X' 'Y' 'DateTime' 'Year' 'Month' 'Day' 'Hour' 'SecondsDelta'\n",
      " 'Weekend' 2003L 2004L 2005L 2006L 2007L 2008L 2009L 2010L 2011L 2012L\n",
      " 2013L 2014L 2015L 'Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep'\n",
      " 'Oct' 'Nov' 'Dec' 1L 2L 3L 4L 5L 6L 7L 8L 9L 10L 11L 12L 13L 14L 15L 16L\n",
      " 17L 18L 19L 20L 21L 22L 23L 24L 25L 26L 27L 28L 29L 30L 31L 'Friday'\n",
      " 'Monday' 'Saturday' 'Sunday' 'Thursday' 'Tuesday' 'Wednesday' '12AM' '1AM'\n",
      " '2AM' '3AM' '4AM' '5AM' '6AM' '7AM' '8AM' '9AM' '10AM' '11AM' '12PM' '1PM'\n",
      " '2PM' '3PM' '4PM' '5PM' '6PM' '7PM' '8PM' '9PM' '10PM' '11PM' 'BAYVIEW'\n",
      " 'CENTRAL' 'INGLESIDE' 'MISSION' 'NORTHERN' 'PARK' 'RICHMOND' 'SOUTHERN'\n",
      " 'TARAVAL' 'TENDERLOIN']\n"
     ]
    }
   ],
   "source": [
    "# Extract new features in Pandas\n",
    "def time_features(data):\n",
    "    data['DateTime'] = pd.to_datetime(data['Dates'])\n",
    "    data['Year'] = pd.DatetimeIndex(data['DateTime']).year\n",
    "    data['Month'] = pd.DatetimeIndex(data['DateTime']).month\n",
    "    data['Day'] = pd.DatetimeIndex(data['DateTime']).day\n",
    "    data['Hour'] = pd.DatetimeIndex(data['DateTime']).hour\n",
    "    data['SecondsDelta'] = (data.DateTime - pd.Timestamp('2013-01-01')) / np.timedelta64(1,'s')\n",
    "    data['Weekend'] = (data.DayOfWeek == \"Saturday\") | (data.DayOfWeek == \"Sunday\")\n",
    "    years = pd.get_dummies(data.Year)\n",
    "    months = pd.get_dummies(data.Month)\n",
    "    months.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    days = pd.get_dummies(data.Day)\n",
    "    daysofweek = pd.get_dummies(data.DayOfWeek)\n",
    "    hours = pd.get_dummies(data.Hour)\n",
    "    hours.columns = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM',\n",
    "                     '6AM', '7AM', '8AM', '9AM', '10AM', '11AM',\n",
    "                     '12PM', '1PM', '2PM', '3PM', '4PM', '5PM',\n",
    "                     '6PM', '7PM', '8PM', '9PM', '10PM', '11PM']\n",
    "    districts = pd.get_dummies(data.PdDistrict)\n",
    "    new_data = pd.concat([data, years, months, days, daysofweek, hours, districts], axis=1)\n",
    "    return new_data\n",
    "\n",
    "data = time_features(data_orig)\n",
    "test = time_features(test_orig)\n",
    "\n",
    "# Separate labels\n",
    "labels = data.Category\n",
    "\n",
    "# Drop Category, Descript and Resolution columns since we cannot use them to predict\n",
    "train_data = data.drop(['Category', 'Descript', 'Resolution'], axis=1)\n",
    "train_names = train_data.columns.values.tolist()\n",
    "test_names = test.columns.values.tolist()\n",
    "\n",
    "print data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Train Data: (10000, 110)\n",
      "Mini Train Labels: (10000L,)\n",
      "Mini Train Unique Labels: 38\n",
      "\n",
      "Regular Train Data: (428991, 110)\n",
      "Regular Train Labels: (428991L,)\n",
      "Regular Train Unique Labels: 39\n",
      "\n",
      "Dev Data: (438991, 110)\n",
      "Dev Labels: (438991L,)\n",
      "Dev Train Unique Labels: 39\n",
      "\n",
      "Test Data: (884262, 111)\n",
      "\n",
      "Columns in use: ['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'DateTime', 'Year', 'Month', 'Day', 'Hour', 'SecondsDelta', 'Weekend', 2003L, 2004L, 2005L, 2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L, 2014L, 2015L, 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to features.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data.shape[0]))\n",
    "train_data = train_data.reindex(shuffle)\n",
    "labels = labels.reindex(shuffle)\n",
    "\n",
    "# Remove records where Y == 90\n",
    "train_data_clean = train_data[train_data.Y != 90]\n",
    "labels_clean = labels[train_data.Y != 90]\n",
    "num_examples = train_data_clean.shape[0]\n",
    "\n",
    "# Split the feature and label sets into train and dev sets\n",
    "mini_train_data = train_data_clean[:10000]\n",
    "mini_train_labels = labels_clean[:10000]\n",
    "\n",
    "reg_train_data = train_data_clean[10000:num_examples/2]\n",
    "reg_train_labels = labels_clean[10000:num_examples/2]\n",
    "\n",
    "dev_data = train_data_clean[num_examples/2:]\n",
    "dev_labels = labels_clean[num_examples/2:]\n",
    "\n",
    "test_data = test.copy()\n",
    "\n",
    "print \"Mini Train Data:\", mini_train_data.shape\n",
    "print \"Mini Train Labels:\", mini_train_labels.shape\n",
    "print \"Mini Train Unique Labels:\", len(mini_train_labels.unique())\n",
    "print \"\\nRegular Train Data:\", reg_train_data.shape\n",
    "print \"Regular Train Labels:\", reg_train_labels.shape\n",
    "print \"Regular Train Unique Labels:\", len(reg_train_labels.unique())\n",
    "print \"\\nDev Data:\", dev_data.shape\n",
    "print \"Dev Labels:\", dev_labels.shape\n",
    "print \"Dev Train Unique Labels:\", len(dev_labels.unique())\n",
    "print \"\\nTest Data:\", test_data.shape\n",
    "print \"\\nColumns in use:\", train_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Some of the category labels have very few examples. When randomizing we need to make sure that dev and regular train data both contain 39 unique category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 99\n"
     ]
    }
   ],
   "source": [
    "features_to_use = [2003L, 2004L, 2005L, 2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L, 2014L, 2015L, \n",
    "                   'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', \n",
    "                   1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, \n",
    "                   16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, \n",
    "                   'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', \n",
    "                   '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', \n",
    "                   '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', \n",
    "                   'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', \n",
    "                   'SOUTHERN', 'TARAVAL', 'TENDERLOIN', 'X', 'Y'\n",
    "                   ]\n",
    "\n",
    "print \"Number of features:\", len(features_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bernoulli NB Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model used Bernoulli Naive Bayes, which scored about the same as logistic regression.\n",
    "\n",
    "We tried using several variations of the Bernoulli NB model. After optimizing alpha, we tried bagging to determine if we could get a more robust model. We also tried using PCA to reduce dimensions and using those new features in the Bernoulli NB model. However, neither of these made any improvements to the original Bernoulli NB model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter:\n",
      "{'alpha': 1.0}\n",
      "\n",
      "Best score:\n",
      "2.56622149659\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEPCAYAAACk43iMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGkxJREFUeJzt3Xm4HXV9x/H3BwIGCBAVG5AEARNqIcqeAmK91CImFVxY\nlSaIstqKdUndH+lTly5aBLSIuAUUV5AKBigFLq6kYBbAoOwmxDY8AolAiBDz7R8z15yce869c5Y5\nc+bM5/U898mdOb+Z+SYPzPfM7/eb708RgZmZVdMWRQdgZmbFcRIwM6swJwEzswpzEjAzqzAnATOz\nCnMSMDOrsNyTgKQtJS2RdHWDz4YkrU0/XyLpw3nHY2Zmm0zowTXeCSwHtm/y+S0RcUwP4jAzszq5\nPglImgrMAb4IqFmzPGMwM7Pm8u4OOg+YD2xs8nkAh0laJmmhpL1zjsfMzGrklgQkvRZ4JCKW0Pzb\n/mJgWkTsC1wIXJVXPGZmNpryqh0k6RPAXGADMBHYAbgiIuaNccyDwIER8Vjdfhc4MjNrQ0SM2eWe\nWxLY7CLSK4H3RsTRdfunkDwthKRZwLcjYvcGx8d4f5F+JunciDi36DjaVeb4yxw7OP6iDUD84947\nezE7aEQASDoTICIuBo4Dzpa0AVgHnNTDeMzMKq8nSSAibgFuSX+/uGb/54DP9SIGM7OqkGbOgWnn\nZGnbyyeBKhsuOoAODRcdQAeGiw6gQ8NFB9Ch4aID6NBw0QG0KkkAh54Pl0zPMgO/J2MCnSr7mICZ\nWa9Is6+Da49Kt8YdGHbtIDOzgbL9xFZaOwmYmQ2UJ9a30tpJwMxsoKy8AM64P2trJwEzswEScddC\nmPBpeN9TWdp7YNjMbMBIvAE4FXS0B4bNzKpnBnBvloZOAmZmg2cvnATMzCprBnBPloZOAmZmg8fd\nQWZmVSQxCZgMrMrS3knAzGywzADuj2i6ouNmnATMzAZL5vEAcBIwMxs0mccDwEnAzGzQZJ4eCk4C\nZmaDxt1BZmYV5u4gM7MqknguMBFYnfUYJwEzs8ExA7g3gsyVQZ0EzMwGR0vjAeAkYGY2SFoaDwAn\nATOzQdLS9FBwEjAzGyTuDjIzqyIJ4e4gM7PKegGwMYJHWznIScDMbDC0/BQATgJmZoOi5fEAcBIw\nMxsUfhIwM6uwlqeHgpOAmdmgcHeQmVkVpdNDp+MnATOzSnoh8FQEv2v1QCcBM7Pya6srCJwEzMwG\nQVszg8BJwMxsELQ1MwicBMzMBoGfBMzMKqztMQFFZF6FrDCSIiJUdBxmZv1GYkvgSeD5Eazb/LPx\n751+EjAzK7dpwKP1CSArJwEzs3JruysInATMzMqu7UFhcBIwMyu7tqeHgpOAmVnZ+UnAzKzCOhoT\n8BRRM7OSktiKZHroDhH8fvTnniJqZjbIdgd+0ygBZJV7EpC0paQlkq5u8vkFku6VtEzS/nnHY2Y2\nQDrqCoLePAm8E1gOjOp3kjQHmB4RM4AzgIt6EI+Z2aDoaFAYck4CkqYCc4AvAo36pY4BFgBExCJg\nsqQpecZkZjZAOpoeCvk/CZwHzAc2Nvl8V2BlzfbDwNScYzIzGxQddwdN6FIgo0h6LfBIRCyRNDRW\n07rthtOVJJ1bszkcEcMdBWhmVn6bdQel99qhVk6Q2xRRSZ8A5gIbgInADsAVETGvps3nSW7o30y3\nfwm8MiJW153LU0TNzGpITATWAJMi2NC4TYFTRCPigxExLSL2AE4CbqpNAKnvA/MAJB0CrKlPAGZm\n1tCewIpmCSCr3LqDGggASWcCRMTFEbFQ0hxJ9wFPAaf2MB4zszLreDwA/MawmVkpScwHXhjBu5q3\n8RvDZmaDquN3BMBJwMysrLrSHeQkYGZWTn4SMDOrIontgOez+cu2bXESMDMrn+nAAxFNqzFk5iRg\nZlY+XRkPACcBM7My6rhw3AgnATOz8unKoDA4CZiZlZG7g8zMKszdQWZmVSSxI7At8L/dOJ+TgJlZ\nucwA7otovPZKq5wEzMzKpWvjAeAkYGZWNl0bDwAnATOzsuna9FBwEjAzKxt3B5mZVZGEcHeQmVll\nPR8Q8NtundBJwMysPGYA93Rreig4CZiZlUlXB4XBScDMrEy6Oh4ATgJmZmXiJwEzswrr6vRQcBIw\nMyuFdHqonwTMzCpqZ2B9BGu6eVInATOzcuh6VxA4CZiZlUXXZwaBk4CZWVl0fTwAnATMzMrCScDM\nrMJyGRNQRNdKUORGUkSEio7DzKwIElsATwEviODJ7MeNf+/0k4CZWf+bCjzeSgLIatwkIOkESTuk\nv39E0vckHdDtQMzMrKlcuoIg25PARyLid5IOB14FfAm4KI9gzMysoVymh0K2JPCH9M/XApdExDXA\n1nkEY2ZmDeUyMwiyJYFVkr4AnAj8QNLEjMeZmVl3FJoETgCuB14dEWuA5wLz8wjGzMwa2oucxgTG\nnSIq6cXAqohYL+kI4GXAgjQh9ISniJpZVUlMAJ4EJkewvrVjuzNF9Epgg6TpwMUkU5UubyUQMzNr\n24uA1a0mgKyyJIGNEbEBeCNwYUTMB3bJIxgzMxslt+mhkC0JPCPpzcA84Jp031Z5BWRmZpvJbXoo\nZEsCbwUOBT4eEQ9K2hO4LK+AzMxsM7nNDIKMtYMkPYckGwXwq4h4Nq+AmlzfA8NmVinSzDkw7Rx4\n6Sz4zYOw9CMRdy1s7Rzj3zsnZDjJELAA+HW6azdJp0TELa0EY2Zm2SQJ4NDz4ZLp6a7nwunnSzNp\nNRGMe60MU0QXA2+KiF+l23sB34yIntUP8pOAmVWJNPs6uPao0Z/MuS5i4ezs5+nOFNEJIwkAICLu\nIcMThJmZtWv7iY33T9qm21fKcjP/uaQvAl8DBJwM3N7tQMzMbMQTTd4JePLpbl8py5PA2cDdwDnA\nO4BfpPvGJWmipEWSlkpaLumTDdoMSVoraUn68+FW/gJmZoNn5QVwxgOb7zvtflhxYbev1NbKYpJ+\nEhEvz9h224hYJ2kC8GPgvRHx45rPh4B3R8QxY5zDYwJmVinSeZ+CFfNg1fLkCWDFhYXMDmpit6wN\nI2Jd+uvWwJbAYw2a+QZvZraZd+0H/G0E38nzKrmXhJa0haSlwGrg5ohYXtckgMMkLZO0UNLeecdk\nZtbPJKYCBwBX532tpk8Cko4luUHXfksf2c48Qh0RG4H9JO0IXC9pKCKGa5osBqalXUazgatIXkyr\nj+fcms3hunOYmQ2SNwNXtFE1dAgYaumYZmMCkr5KctNvKCJObeVC6Tk/AjwdEZ8ao82DwIER8VjN\nPo8JmFklSAi4g6Qr6IednauDMYGIeEsnF08D2AnYEBFrJG0DHAn8Y12bKcAjERGSZpEkpkbjBmZm\nVbAvsD3JRJrctTQwLOmaiHhtC4fsAiyQtAXJ+MNlEXGjpDMBIuJi4DjgbEkbgHXASa3EZGY2YOYB\nl0WwsRcXa2mKqKQlEbF/jvE0u667g8xs4KWriK0EXhnR+RoC3SobUWtpB/GYmdnY/gpY0Y0EkFVb\nL4v1mp8EzKwKJL4O/CyCz3bnfOPfO7NUEb2T0VNF1wK3AR+LiEc7DXQ8TgJmNugktifpCpoewW+7\nc87uvDF8HbCBZHF5kQzcbkvy8tdXgaM7C9PMzIBjgVu6lQCyypIE/qpuMPiOkQHi9CnBzMw6Nxe4\nqNcXzTIwvKWkPx/ZSOfyjxy3IZeozMwqJC0TsR9wTa+vneVJ4G3AVyRNSrefAN4maTtgVGloMzNr\n2cm0USaiGzLPDkpr/xARa3ONqPG1PTBsZgMpLRNxF3BWBD/q7rm78J6ApMmSzgNuAm6S9OmRhGBm\nZh3bn2SyzU+KuHiWMYEvA78DjgdOIOkO+kqeQZmZVchcelgmol6W9wSWRcS+4+3Lk7uDzGwQpWUi\nHgZeEcG93T9/d8pGPC3pFTUnPZyk0JuZmXXmSOChPBJAVllmB50FXFozDvA4cEp+IZmZVcZc4LIi\nA2hrdpCkv4+Iz+Qa2ebXdneQmQ0UiR1IykS8OK+3hLtaRTQi1tZMD31PR5GZmdmxwM29LhNRL/eF\n5s3MrKHCu4LAScDMrOckppEsI9nzMhH1mg4MS3qS5gvNb5tPOGZmlXAy8N0Ifl90IGMtND+p2Wdm\nZtaetEzEXODMomMBdweZmfXaAcA2FFQmop6TgJlZb42UieiLtX29xrCZWY+kZSJWAYf34i3hrr4n\nYGZmHXs18ECRZSLqOQmYmfVOX7wbUMvdQWZmPSCxI7AC2DOCR3tzTXcHmZn1i2OBm3qVALJyEjAz\n642+6woCdweZmeVOYjdgMbBrL98SdneQmVl/6JsyEfWcBMzMcpSWiZgHXFp0LI04CZiZ5etAYCvg\nZ0UH0oiTgJlZvuYCX+uXMhH1PDBsZpYTia2Ah4GXR3Bf76/vgWEzsyK9Gri/iASQlZOAmVl++nZA\neIS7g8zMcpCWifg1SZmIx4qJwd1BZmZFOY6kTEQhCSArJwEzs3z0ZZmIeu4OMjPrMokXAT+nx2Ui\nRsfh7iAzsyL8DfDtfiwTUc9JwMysi9IyEaXoCgInATOzbjsI2BK4tehAsnASMDPrrr4uE1HPA8Nm\nZl2SlolYBRwawf3Fx+OBYTOzXjoKuKcfEkBWTgJmZt0zj5IMCI9wd5CZWRdITCYpE7F7BI8XHQ8U\n3B0kaaKkRZKWSlou6ZNN2l0g6V5JyyTtn1c8ZmY5Ow74735JAFnllgQiYj1wRETsB7wMOELS4bVt\nJM0BpkfEDOAM4KK84jEzy1lp3g2oleuYQESsS3/dmmTebH0hpWOABWnbRcBkSVPyjMnMrNskdgf2\nBhYWHErLck0CkraQtBRYDdwcEcvrmuwKrKzZfhiYmmdMZmY5GCkT8UzRgbRqQp4nj4iNwH6SdgSu\nlzQUEcN1zeoHLRqOVEs6t2ZzuMF5zMx6rqZMxCnFx6IhYKiVY3JNAiMiYq2kH5C8Tj1c89EqYFrN\n9tR0X6NznJtXfGZmHTiY5MvsoqIDSb8cD49sS/roeMfkOTtoJ0mT09+3AY4EltQ1+z7JvFokHQKs\niYjVecVkZpaDUpWJqJfnk8AuwAJJW5Akm8si4kZJZwJExMURsVDSHEn3AU8Bp+YYj5lZV0lsDZwI\nHFJ0LO3yy2JmZm2SOAaYH8Erio6lEdcOMjPLVynfDajlJwEzszb0Y5mIen4SMDPLz/HADf2aALJy\nEjAza0/pu4LA3UFmZi2T2AP4H2DXfn5L2N1BZmb5+BvgW/2cALJyEjAza0FNmYjSdwWBk4CZWatm\npX/+T6FRdImTgJlZa+YCl5W1TEQ9DwybmWWUlolYBcyK4MGi4xmPB4bNzLprNnB3GRJAVk4CZmbZ\nDcyA8Ah3B5mZZSDxXOAh4EURrCk4nEzcHWRm1j3HA/9VlgSQlZOAmVk284BLiw6i29wdZGY2Dok9\ngVtJykQ8W3Q8Wbk7yMysO0bKRJQmAWTVk4XmzczKqqZMxMlFx5IHPwmYmY3tz4GNwG1FB5IHJwEz\ns7HNBS4dlDIR9TwwbGbWRFom4jfAQRE8VHA4LfPAsJlZZ+YAvyhjAsjKA8NmZnWkmXNg2jkw80D4\n7WrptjkRdy0sOq48OAmYmdVIEsCh58Ml09NdO8Hp50szGcRE4O4gM7PNTDunJgGkLpkOu72jmHjy\n5ScBMzNA4jnA0fDSgxu3mLRNTwPqET8JmFllSUjiIInPkiwW83Z4ZFXj1k8+3cvYesVJwMwqR2IX\nifcCdwLfAlaTTAP9S7j9/XD6fZsfcdr9sOLC3keaP78nYGaVIDEROBp4C3AYcCXwVeDH9S+CJYPD\nu70j6QJ68mlYcWEZB4Wz3DudBMxsYKV1fw4iufGfCCwjufFfGcFTxUXWG1nunR4YNrOBI7ELSeXP\ntwATSW78B0bw6wLD6ktOAmY2EJp095xFg+4e28RJwMxKK+3uOZjkxn8CsJTkW/8JVeju6QYnATMr\nHYkXsqm7Z2vc3dM2JwEzK4W0u+cYkhv/ocAVwBnAT9zd0z4nATPrW3XdPScCS0i+9R/v7p7ucBIw\ns77TpLvnAHf3dJ+TgJn1hbS753UkN/5DgO8CpwM/dXdPfpwEzKwwaXfPLDbN7llM8q3/2AjWFRdZ\ndTgJmFnPSezKpu6eCSQ3/v0jWFFgWJVUmiQgzb4OVl5Qpvodm1Yn2n4iPLHe8fdOmWOHwYwf7rqJ\n0d09p+HunmJFRN//AAERcNq9sM+couPJFvM+c5J4Izb9OH7HXtX4370WbnwC4gaIkyG2LTrOKvwk\nt/ix25SmgBx//KLwtuXwpc+lG7XBj/d7K227cNwp82HByxjllGWw4F8anH+sa2f5rMvHn/xx+PqB\no+M/+Tb4+gcynqugP4//HHznsNGxH/9j+M7ZtPbv0c6/YYfHvP5yuGpodPxv/CFceRqwZc3PhLrt\nPvjsnFfBBbuMjv8NwxHfO2L0fsvLgBaQ22EyMBOo/YuN93srbbtxHLDTzjT0gl1IHonHu04rn+Vw\n/NQ9G0UPU2cAH8xwrgL/nPHixrHPOICkdjyb2rb0Z4+OeenzGse/9yzgWuAP6c+Gmt/rf8b6rJ1j\nnwXWZzvv7w8EGiSBrVwJuA+VMAn86o4I3l50FOORll8HHDX6k+WLIzip5wG1SLqjSfx33hrB7J4H\n1AJpSZPYl/6w32MHkG5vEv/i4XLEv+LNwJ+N/mQwV+Yqu5KtLFam1X1WXlDu1YnKHH+ZYwfHb72U\n65iApGnApcCfkPR5fiEiLqhrMwT8J/BAuuuKiPhYXZuA2deVbXWfsq9OVOb4yxw7OH7rjsJXFpO0\nM7BzRCyVNAn4OfD6iLi7ps0Q8O6IOGaM84z7F+lnkoYiYrjoONpV5vjLHDs4/qINQPzj3jtz7Q6K\niP+LiKXp708CdwMvbNC0tDf4jIaKDqBDQ0UH0IGhogPo0FDRAXRoqOgAOjRUdAB569mYgKTdgf2B\nRXUfBXCYpGWSFkrau1cxmZlVXU9mB6VdQd8F3pk+EdRaDEyLiHWSZgNXAXv1Ii4zs6rL/WUxSVsB\n1wDXRsRnMrR/EDgwIh6r2df/b7SZmfWhQl8WkyTgS8DyZglA0hTgkYgISbNIEtNjtW3KPChsZtbP\n8u4OejlJpcA7JC1J930Q2A0gIi4GjgPOlrQBWAf9/yKVmdmgKEXtIDMzy0fJ3hgGSe+RtFFSk/oq\n/UfSP6Wzn5ZKujF9ia40JP2bpLvTv8OVknYsOqZWSDpe0i8k/UHSAUXHk5Wk10j6paR7Jb2v6Hha\nIenLklZLurPoWNohaZqkm9P/bu6SdE7RMWUlaaKkRen9ZrmkT47VvlRJIL15HgmlW2f0XyNi34jY\nj2T200eLDqhF/wXsExH7AvcAHxinfb+5E3gD8MOiA8lK0pbAZ4HXAHsDb5LUoB5P3/oKSexl9Szw\nrojYh2Ttg78ty79/RKwHjkjvNy8DjpB0eLP2pUoCwL8D/1B0EK2KiCdqNicBvy0qlnZExA0RsTHd\nXARMLTKeVkXELyPinqLjaNEs4L6IeCgingW+yabqs30vIn4EPF50HO1q4UXXvhQRI0tzbk1S5vux\nZm1LkwQkvQ54OCLuKDqWdkj6uKQVwCnAPxcdTwfeCrgGTP52BVbWbD+c7rMeG+NF174laQtJS4HV\nwM0RsbxZ274qJS3pBqBRHf4PkXRBvLq2eU+CymiM2D8YEVdHxIeAD0l6P3AecGpPAxzHePGnbT4E\nPBMRl/c0uAyyxF8ynrHRB8Z50bVvpU/u+6Xjd9ePVQOpr5JARBzZaL+kmcAewLLk1QOmAj+XNCsi\nHulhiE01i72By+nDb9LjxS/pLcAc4FU9CahFLfz7l8UqoHYCwTSSpwHrkfRF1yuAr0XEVUXH046I\nWCvpB8BBwHCjNqXoDoqIuyJiSkTsERF7kPzPcEC/JIDxSJpRs/k6YEmztv1I0muA+cDr0kGnMuur\nJ8gx3A7MkLS7pK2BE4HvFxxTZWR50bVfSdpJ0uT0921IJtM0veeUIgk0ULZH5U9KujPtoxsC3lNw\nPK26kGRA+wZJSyT9R9EBtULSGyStJJnl8QNJ1xYd03giYgPwd8D1wHLgW7Ul2PudpG8APwX2krRS\nUl91f2Yw8qLrEel/80vSL0NlsAtwU3q/WQRcHRE3Nmvsl8XMzCqsrE8CZmbWBU4CZmYV5iRgZlZh\nTgJmZhXmJGBmVmFOAmZmFeYkYANJUlde8Zf0Uklfrtt3laSf1e07V9KY739kadPgmBslbd/KMWat\ncBKwQdWtF2DmAxeNbKRvYs4Etpa0R4vXayembwKnt3GcWSZOAlYZkvaTdGvN4jgjr9YfLOmO9K3Q\nfxtZCEXSc4BDIuK2mtO8Ebga+A6jl0KN9LhhSZ9Jz3enpINr2uydLlZyv6R31MT2PUm3pwuY1N70\nv9/gOmZd4yRgVXIpMD9dHOdONi3u8xXg9IjYH9jApm/s+wO/qjvHScC3gG8Db2pynQC2Sc/3dmCk\nO0nAS0iq4c4CPpouHgPw1og4CDgYOGdk5byIWA3sJGm79v7KZmNzErBKSEvq7pgudgKwAPiLdP+k\niBipFX85m4rMvQj435pzTAGmR8StEfEA8IykfZpc8hvwx8VVdkivE8A1EfFsRDwKPAJMSdu/M631\n8jOSKrm1RQdXs3lFUbOucRKwqmpWTbR+f+32CcDzJD0o6UFgd5o/DdQbebp4pmbfH4AJkoZISnQf\nki4JuBR4Tl0MLvJluXASsEqIiLXA4zVrrc4FhtP9T0iale6v7X9/iM0XqnkTcFRNSfODatqLTQlD\nJKWfSa+3JiJ+R+PEI2AH4PGIWC/pJSTVTmtNwWsJWE76alEZsy7aNi0fPeLTJEt7fl7StsD9bFrd\n7W3AJZI2ArcAa9P9y4A/hT8uMTitptuIiHhI0po0gQSbvq0HsF7SYpL/x95as7/+G30A1wFnSVpO\nMgbxx+mnknYGHo2Ip9r5RzAbj0tJW+VJ2m7kJpsu/zklIt6Vbn8VuKj25p/hfDcD74mIxV2I7Qxg\nu4g4r9NzmTXi7iAz+OuR6Zwki4l8rOazTwFnFRMWkHQrXVLg9W3A+UnAzKzC/CRgZlZhTgJmZhXm\nJGBmVmFOAmZmFeYkYGZWYU4CZmYV9v/J0bnBzQ8/PAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fd97f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the optimal alpha is for a Bernoulli NB model\n",
    "\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 500.0, 1000.0]}\n",
    "\n",
    "clf = GridSearchCV(BernoulliNB(), [params], scoring='log_loss')\n",
    "clf.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "\n",
    "print \"Best parameter:\\n\", clf.best_params_\n",
    "print \"\\nBest score:\\n\", -1 * clf.best_score_\n",
    "\n",
    "best_alpha = clf.best_params_['alpha']\n",
    "\n",
    "scores = [clf.grid_scores_[i][1] * -1 for i in range(len(clf.grid_scores_))]\n",
    "\n",
    "plt.plot(np.log10(params['alpha']), scores, marker='o')\n",
    "plt.xlabel(\"Log(Alpha)\")\n",
    "plt.ylabel(\"Log-Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we throw in 99 features (basically every feature we created), the log-loss is optimized when alpha = 1. However, the practical difference in log-loss up to alpha = 1 is very small. With alpha > 10, log-loss increases sharply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "\n",
      "model           alpha     accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "baseline        1.0000    0.2277    2.5656    \n",
      "optimized alpha 1.0000    0.2277    2.5656    \n",
      "bagged          1.0000    0.2258    2.5674    \n",
      "pca (70 comp)   1.0000    0.1920    2.7547    \n"
     ]
    }
   ],
   "source": [
    "print \"Bernoulli NB\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"model\", \"alpha\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Baseline Bernoulli NB model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "base_alpha = 1.0\n",
    "base_accuracy = bnb.score(dev_data[features_to_use], dev_labels)\n",
    "base_logloss = log_loss(dev_labels, bnb.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"baseline\", base_alpha, base_accuracy, base_logloss)\n",
    "\n",
    "# How well does the Bernoulli NB model do with the selected alpha?\n",
    "bnb.set_params(alpha=best_alpha)\n",
    "bnb.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "bnb_probs = bnb.predict_proba(test_data[features_to_use])\n",
    "accuracy = bnb.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, bnb.predict_proba(dev_data[features_to_use]))\n",
    "print result_form(\"optimized alpha\", best_alpha, accuracy, logloss)\n",
    "\n",
    "# Maybe bagging the Bernoulli NB model does us some good\n",
    "bnb_bag = BaggingClassifier(BernoulliNB(alpha=best_alpha), max_features=0.8, max_samples = 0.8)\n",
    "bnb_bag.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "bnb_bag_probs = bnb_bag.predict_proba(test_data[features_to_use])\n",
    "accuracy = bnb_bag.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, bnb_bag.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"bagged\", best_alpha, accuracy, logloss)\n",
    "\n",
    "# Perhaps doing some dimensionality reduction will help us\n",
    "pca = PCA(n_components = 70)\n",
    "pca_train_data = pca.fit_transform(reg_train_data[features_to_use])\n",
    "pca_dev_data = pca.transform(dev_data[features_to_use])\n",
    "pca_test_data = pca.transform(test_data[features_to_use])\n",
    "\n",
    "bnb2 = BernoulliNB(best_alpha)\n",
    "bnb2.fit(pca_train_data, reg_train_labels)\n",
    "bnb2_probs = bnb2.predict_proba(pca_test_data)\n",
    "accuracy = bnb2.score(pca_dev_data, dev_labels)\n",
    "logloss = log_loss(dev_labels, bnb2.predict_proba(pca_dev_data)) \n",
    "print result_form(\"pca (70 comp)\", best_alpha, accuracy, logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct class                  actual  predicted  correct\n",
      "ARSON                             732          0        0\n",
      "ASSAULT                         38487       7496     1281\n",
      "BAD CHECKS                        213          0        0\n",
      "BRIBERY                           145          0        0\n",
      "BURGLARY                        18563        251       41\n",
      "DISORDERLY CONDUCT               2152          0        0\n",
      "DRIVING UNDER THE INFLUENCE      1142          0        0\n",
      "DRUG/NARCOTIC                   26983      34607     8227\n",
      "DRUNKENNESS                      2161          0        0\n",
      "EMBEZZLEMENT                      577          0        0\n",
      "EXTORTION                         121          0        0\n",
      "FAMILY OFFENSES                   247          0        0\n",
      "FORGERY/COUNTERFEITING           5324         80       12\n",
      "FRAUD                            8406          0        0\n",
      "GAMBLING                           58          0        0\n",
      "KIDNAPPING                       1152          0        0\n",
      "LARCENY/THEFT                   87298     251864    63604\n",
      "LIQUOR LAWS                       926          0        0\n",
      "LOITERING                         610          1        0\n",
      "MISSING PERSON                  12971        628      105\n",
      "NON-CRIMINAL                    46174       3604      690\n",
      "OTHER OFFENSES                  63169     108852    20135\n",
      "PORNOGRAPHY/OBSCENE MAT            12          0        0\n",
      "PROSTITUTION                     3783       1848      161\n",
      "RECOVERED VEHICLE                1548          0        0\n",
      "ROBBERY                         11388          0        0\n",
      "RUNAWAY                           984          0        0\n",
      "SECONDARY CODES                  4998          0        0\n",
      "SEX OFFENSES FORCIBLE            2191          0        0\n",
      "SEX OFFENSES NON FORCIBLE          76          0        0\n",
      "STOLEN PROPERTY                  2310          0        0\n",
      "SUICIDE                           248          0        0\n",
      "SUSPICIOUS OCC                  15711          0        0\n",
      "TREA                                3          0        0\n",
      "TRESPASS                         3641          0        0\n",
      "VANDALISM                       22385        369       34\n",
      "VEHICLE THEFT                   26671      29391     5619\n",
      "WARRANTS                        21188          0        0\n",
      "WEAPON LAWS                      4243          0        0\n",
      "Confusion Matrix\n",
      "[[    0    28     0     0     0     0     0    16     0     0     0     0\n",
      "      0     0     0     0   336     0     0     0     5   271     0     2\n",
      "      0     0     0     0     0     0     0     0     0     0     0     2\n",
      "     72     0     0]\n",
      " [    0  1281     0     0    16     0     0  2958     0     0     0     0\n",
      "      6     0     0     0 20166     0     0    38   232 10887     0   165\n",
      "      0     0     0     0     0     0     0     0     0     0     0    58\n",
      "   2680     0     0]\n",
      " [    0     0     0     0     0     0     0    11     0     0     0     0\n",
      "      0     0     0     0   135     0     0     0     1    54     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     12     0     0]\n",
      " [    0     2     0     0     0     0     0     6     0     0     0     0\n",
      "      0     0     0     0    75     0     0     0     1    52     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      9     0     0]\n",
      " [    0   257     0     0    41     0     0   634     0     0     0     0\n",
      "      1     0     0     0 11381     0     0    61   218  4511     0    66\n",
      "      0     0     0     0     0     0     0     0     0     0     0    18\n",
      "   1375     0     0]\n",
      " [    0    45     0     0     2     0     0   350     0     0     0     0\n",
      "      0     0     0     0   959     0     0     1     7   652     0    32\n",
      "      0     0     0     0     0     0     0     0     0     0     0     3\n",
      "    101     0     0]\n",
      " [    0    74     0     0     0     0     0    38     0     0     0     0\n",
      "      0     0     0     0   643     0     0     0     5   300     0     9\n",
      "      0     0     0     0     0     0     0     0     0     0     0    10\n",
      "     63     0     0]\n",
      " [    0   226     0     0     2     0     0  8227     0     0     0     0\n",
      "      0     0     0     0 10881     0     1    25   126  6323     0    95\n",
      "      0     0     0     0     0     0     0     0     0     0     0     6\n",
      "   1071     0     0]\n",
      " [    0    93     0     0     1     0     0   164     0     0     0     0\n",
      "      0     0     0     0  1328     0     0     0    10   416     0    16\n",
      "      0     0     0     0     0     0     0     0     0     0     0     9\n",
      "    124     0     0]\n",
      " [    0     1     0     0     0     0     0    33     0     0     0     0\n",
      "      2     0     0     0   336     0     0     2     4   173     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     26     0     0]\n",
      " [    0     3     0     0     0     0     0     4     0     0     0     0\n",
      "      0     0     0     0    79     0     0     0     2    29     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      4     0     0]\n",
      " [    0     0     0     0     0     0     0    22     0     0     0     0\n",
      "      0     0     0     0   102     0     0     2     1    90     0     1\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     29     0     0]\n",
      " [    0    16     0     0     1     0     0   260     0     0     0     0\n",
      "     12     0     0     0  3161     0     0     1    30  1512     0     7\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    324     0     0]\n",
      " [    0    75     0     0     3     0     0   459     0     0     0     0\n",
      "      7     0     0     0  5419     0     0    14    97  1927     0    13\n",
      "      0     0     0     0     0     0     0     0     0     0     0     1\n",
      "    391     0     0]\n",
      " [    0     0     0     0     0     0     0     3     0     0     0     0\n",
      "      0     0     0     0    25     0     0     0     0    24     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      6     0     0]\n",
      " [    0    47     0     0     0     0     0    74     0     0     0     0\n",
      "      0     0     0     0   583     0     0     1    10   359     0     3\n",
      "      0     0     0     0     0     0     0     0     0     0     0     2\n",
      "     73     0     0]\n",
      " [    0   891     0     0    39     0     0  4061     0     0     0     0\n",
      "      6     0     0     0 63604     0     0    64   552 13681     0   260\n",
      "      0     0     0     0     0     0     0     0     0     0     0    46\n",
      "   4094     0     0]\n",
      " [    0    11     0     0     0     0     0    74     0     0     0     0\n",
      "      0     0     0     0   480     0     0     1     0   276     0     5\n",
      "      0     0     0     0     0     0     0     0     0     0     0     1\n",
      "     78     0     0]\n",
      " [    0     5     0     0     1     0     0    73     0     0     0     0\n",
      "      0     0     0     0   367     0     0     0     1   119     0     9\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     35     0     0]\n",
      " [    0   140     0     0    13     0     0   330     0     0     0     0\n",
      "      1     0     0     0  6407     0     0   105   210  4643     0    22\n",
      "      0     0     0     0     0     0     0     0     0     0     0     5\n",
      "   1095     0     0]\n",
      " [    0   596     0     0    26     0     0  3014     0     0     0     0\n",
      "     15     0     0     0 29516     0     0    70   690  9647     0   173\n",
      "      0     0     0     0     0     0     0     0     0     0     0    37\n",
      "   2390     0     0]\n",
      " [    0   993     0     0    34     0     0  6050     0     0     0     0\n",
      "      7     0     0     0 31418     0     0    77   495 20135     0   271\n",
      "      0     0     0     0     0     0     0     0     0     0     0    53\n",
      "   3636     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     7     0     0     0     0     5     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0   105     0     0    10     0     0   433     0     0     0     0\n",
      "      0     0     0     0  1805     0     0     0     4  1142     0   161\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    123     0     0]\n",
      " [    0    31     0     0     0     0     0    58     0     0     0     0\n",
      "      0     0     0     0   683     0     0     0     5   751     0     7\n",
      "      0     0     0     0     0     0     0     0     0     0     0     3\n",
      "     10     0     0]\n",
      " [    0   476     0     0     7     0     0   784     0     0     0     0\n",
      "      1     0     0     0  6028     0     0     4    43  3156     0    70\n",
      "      0     0     0     0     0     0     0     0     0     0     0    12\n",
      "    807     0     0]\n",
      " [    0     7     0     0     2     0     0     1     0     0     0     0\n",
      "      0     0     0     0   456     0     0    15    24   304     0     4\n",
      "      0     0     0     0     0     0     0     0     0     0     0     1\n",
      "    170     0     0]\n",
      " [    0   146     0     0     4     0     0   245     0     0     0     0\n",
      "      1     0     0     0  2532     0     0     9    51  1686     0    17\n",
      "      0     0     0     0     0     0     0     0     0     0     0    15\n",
      "    292     0     0]\n",
      " [    0    82     0     0     1     0     0   106     0     0     0     0\n",
      "      2     0     0     0  1181     0     0     6    22   647     0     9\n",
      "      0     0     0     0     0     0     0     0     0     0     0     6\n",
      "    129     0     0]\n",
      " [    0     0     0     0     0     0     0     1     0     0     0     0\n",
      "      1     0     0     0    36     0     0     0     2    32     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      4     0     0]\n",
      " [    0    40     0     0     2     0     0   134     0     0     0     0\n",
      "      0     0     0     0  1493     0     0     1    18   515     0    11\n",
      "      0     0     0     0     0     0     0     0     0     0     0     1\n",
      "     95     0     0]\n",
      " [    0     6     0     0     1     0     0    15     0     0     0     0\n",
      "      0     0     0     0   138     0     0     0     3    70     0     1\n",
      "      0     0     0     0     0     0     0     0     0     0     0     1\n",
      "     13     0     0]\n",
      " [    0   275     0     0     8     0     0   970     0     0     0     0\n",
      "     10     0     0     0  8704     0     0    29   171  4491     0    56\n",
      "      0     0     0     0     0     0     0     0     0     0     0    12\n",
      "    985     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     3     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0]\n",
      " [    0    44     0     0     4     0     0   504     0     0     0     0\n",
      "      0     0     0     0  1959     0     0     5    30   924     0    18\n",
      "      0     0     0     0     0     0     0     0     0     0     0     2\n",
      "    151     0     0]\n",
      " [    0   605     0     0    12     0     0   632     0     0     0     0\n",
      "      4     0     0     0 13100     0     0    32   180  5819     0   101\n",
      "      0     0     0     0     0     0     0     0     0     0     0    34\n",
      "   1866     0     0]\n",
      " [    0   473     0     0    10     0     0   423     0     0     0     0\n",
      "      3     0     0     0 13221     0     0    34   155  6595     0   121\n",
      "      0     0     0     0     0     0     0     0     0     0     0    17\n",
      "   5619     0     0]\n",
      " [    0   289     0     0    11     0     0  3113     0     0     0     0\n",
      "      1     0     0     0 11186     0     0    27   169  5160     0   109\n",
      "      0     0     0     0     0     0     0     0     0     0     0    11\n",
      "   1112     0     0]\n",
      " [    0   133     0     0     0     0     0   327     0     0     0     0\n",
      "      0     0     0     0  1934     0     0     4    30  1471     0    14\n",
      "      0     0     0     0     0     0     0     0     0     0     0     3\n",
      "    327     0     0]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's go back to the baseline model and see what errors we are making\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "predicted = bnb.predict(dev_data[features_to_use])\n",
    "\n",
    "confusion = confusion_matrix(dev_labels, predicted)\n",
    "np.set_printoptions(threshold='nan')\n",
    "\n",
    "classes = reg_train_labels.unique()\n",
    "classes.sort()\n",
    "\n",
    "row_form = \"      {:28s}{:>7d}{:>9.4f}\".format\n",
    "index = 0\n",
    "\n",
    "print \"{:<28s}{:>9s}{:>11s}{:>9s}\".format(\"Correct class\", \"actual\", \"predicted\", \"correct\")\n",
    "for row in confusion:\n",
    "    count = dev_labels.value_counts()[classes[index]]\n",
    "    print \"{:<28s}{:>9d}{:>11d}{:>9d}\".format(classes[index], \n",
    "                                              count, \n",
    "                                              sum(predicted == classes[index]),\n",
    "                                              confusion[index,index])\n",
    "    index += 1\n",
    "\n",
    "print \"Confusion Matrix\"\n",
    "print confusion, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the table above that the Naive Bayes model is mostly predicting \"LARCENY/THEFT\" or \"OTHER OFFENSES\". Because the distribution of crimes is very skewed, the model seems to most frequently choose the class where there are a plethora of examples. There are a large number of crimes that the model never predicts, particularly the ones where there are not very many examples.\n",
    "\n",
    "It is suprising that the model does not frequently predict \"NON-CRIMINAL\" crimes, even though there are many examples of these crimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tried gradient boosting, which improved our score to 2.49. Given the computational intensity of gradient boosting (initial model took 6 hours to run), we were unable to do significant parameter optimization. Increasing the leraning parameter to 0.2 seemed to improve the accuracy slightly, even with only 30 estimators. However, when the learning rate was increased further to 0.5, the accuracy decreased significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "\n",
      "model           param     accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "baseline                  0.2547    2.5018    \n"
     ]
    }
   ],
   "source": [
    "print \"Gradient Boosting\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"model\", \"param\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Try Gradient Boosting\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "grad_boost.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "accuracy = grad_boost.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, grad_boost.predict_proba(dev_data[features_to_use])) \n",
    "print \"{:<16s}{:<10s}{:<10.4f}{:<10.4f}\".format(\"baseline\", \"\", accuracy, logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "\n",
      "iteration       param     accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "n_estimators    30                            \n",
      "learning_rate   0.2       0.2537    2.4943    \n",
      "---------------------------------------------\n",
      "n_estimators    100                           \n",
      "learning_rate   0.5       0.0478    32.8866   \n"
     ]
    }
   ],
   "source": [
    "print \"Gradient Boosting\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"iteration\", \"param\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Let's try tweaking some parameters on the smaller data set\n",
    "grad_boost = GradientBoostingClassifier(n_estimators = 30, learning_rate = 0.2)\n",
    "grad_boost.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "new_grad_boost_probs = grad_boost.predict_proba(test_data[features_to_use])\n",
    "create_submission(new_grad_boost_probs)\n",
    "new_accuracy = grad_boost.score(dev_data[features_to_use], dev_labels)\n",
    "# new_logloss = log_loss(dev_labels, grad_boost.predict_proba(dev_data[features_to_use])) \n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"n_estimators\", \"30\", \"\", \"\")\n",
    "print \"{:<16s}{:<10s}{:<10.4f}{:<10.4f}\".format(\"learning_rate\", \"0.2\", new_accuracy, new_logloss)\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Let's try increasing the learning rate again\n",
    "grad_boost2 = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.5)\n",
    "grad_boost2.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "new_accuracy2 = grad_boost2.score(dev_data[features_to_use], dev_labels)\n",
    "new_logloss2 = log_loss(dev_labels, grad_boost2.predict_proba(dev_data[features_to_use])) \n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"n_estimators\", \"100\", \"\", \"\")\n",
    "print \"{:<16s}{:<10s}{:<10.4f}{:<10.4f}\".format(\"learning_rate\", \"0.5\", new_accuracy2, new_logloss2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we used a neural net, which got our best score of 2.43. This was enough to enter the top 50 in the SF Learns competition (as of August 17th).\n",
    "\n",
    "We ran the Neural Network using a combination of NoLearn, Lasange, and Theano. NoLearn and Lasange are both under development, so to replicate this code please follow the installation instructions on NoLearn, below. NoLearn is built on top of Lasange and Theano, and thus its requirements are the ones to follow\n",
    "https://github.com/dnouri/nolearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried two differnet types of Neural Networks - a 1-hidden layer network, then a 2-hidden layer network with a dropout layer between. The 2-hidden layer network resulted in only minor improvements. While the NoLearn/Lasange/Theano pipeline allows Neural Nets to be trained on certain types of GPU, we were not using such a machine for our development. As such, each model run took roughly an hour and limited the amount of runs possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried many different features in the Neural Net model. Interestingly enough, the \"year\" and \"month\" dummy variables caused the model to diverge, leading to poor prediction. These were dropped from the final model in place of a few simple booleans flagging if a crime occured before 2008, before 2010, and before 2012.\n",
    "\n",
    "We also parsed the X-Y coorinates into a series of dummy variables by rounding each coordinate to the 3rd decimal place. An attempt was made to round to the 4th decimal place and generate more features, however the computers available to us were unable to do this task. If work continues, this could be attempted via the cloud such as on AWS.\n",
    "\n",
    "Note that NoLearn automatically splits the data into training and development sets (default of 80/20 split) each iteration, so we did not need to clean a development set for this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 147)\n",
      "(878049, 107)\n",
      "\n",
      "(884262, 147)\n",
      "(884262, 107)\n"
     ]
    }
   ],
   "source": [
    "# Generate location-based dummies\n",
    "XR3 = data['X'].round(decimals=3).apply(str)\n",
    "YR3 = data['Y'].round(decimals=3).apply(str)\n",
    "data_XR3s = pd.get_dummies(XR3)\n",
    "data_YR3s = pd.get_dummies(YR3)    \n",
    "XR3 = test['X'].round(decimals=3).apply(str)\n",
    "YR3 = test['Y'].round(decimals=3).apply(str)\n",
    "test_XR3s = pd.get_dummies(XR3)\n",
    "test_YR3s = pd.get_dummies(YR3)    \n",
    "\n",
    "#Subset the test to only include features that exist in training set\n",
    "test_XR3s = test_XR3s[list(data_XR3s)]\n",
    "test_YR3s = test_YR3s[list(data_YR3s)]\n",
    "\n",
    "print data_XR3s.shape\n",
    "print data_YR3s.shape\n",
    "print\n",
    "print test_XR3s.shape\n",
    "print test_YR3s.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate labels\n",
    "train_labels = data.Category\n",
    "\n",
    "# Create integer labels\n",
    "panda_labels = pd.Categorical(data.Category).codes\n",
    "train_labels_int = np.array(panda_labels).astype(np.int32)\n",
    "\n",
    "# Drop Category, Descript and Resolution columns since they are not in the test set.\n",
    "# Drop non-numerics too - they are accounted for as dummy variables.\n",
    "train_data = data.drop(['Category', 'Descript', 'Resolution', 'DateTime', 'Dates', 'PdDistrict', 'Address', 'DayOfWeek'], axis=1)\n",
    "train_data.Weekend = train_data.Weekend * 1\n",
    "train_names = train_data.columns.values.tolist()\n",
    "\n",
    "test_data = test.drop(['DateTime', 'Dates', 'PdDistrict', 'Address', 'DayOfWeek'], axis=1)\n",
    "test_data.Weekend = test_data.Weekend * 1\n",
    "test_names = test_data.columns.values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted many different combinations of features. Some worked better than others, and a few unexpectedly diverged. The features below were used in our final set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['Jan','Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "np_train_data = np.array(pd.concat([train_data[features], (data.Year < 2006) * 1, (data.Year < 2008) * 1, (data.Year < 2010) * 1, data_XR3s, data_YR3s], axis=1))\n",
    "np_test_data = np.array(pd.concat([test_data[features], (test.Year < 2006) * 1, (test.Year < 2008) * 1, (test.Year < 2010) * 1, test_XR3s[list(data_XR3s)], test_YR3s[list(data_YR3s)]], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net with 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoLearn/Lasagne makes it easy to define a net in terms of layers. We tried a number of hidden units and learning rates before settling on these parameters. We stopped after 65 iterations (epochs) through network updating because we observed overfitting around this point (that is, our accuracy on the training dataset continued to improve but accuracy on the development dataset began to fall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 299)\n",
      "(878049,)\n",
      "# Neural Network with 169539 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      299\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.63157\u001b[0m       \u001b[32m2.63910\u001b[0m      0.99715      0.22339  29.22s\n",
      "      2       \u001b[36m2.52321\u001b[0m       \u001b[32m2.60649\u001b[0m      0.96805      0.22637  29.03s\n",
      "      3       \u001b[36m2.49771\u001b[0m       \u001b[32m2.58853\u001b[0m      0.96492      0.22857  29.44s\n",
      "      4       \u001b[36m2.48268\u001b[0m       \u001b[32m2.57886\u001b[0m      0.96270      0.23122  29.51s\n",
      "      5       \u001b[36m2.47280\u001b[0m       \u001b[32m2.57330\u001b[0m      0.96095      0.23314  29.57s\n",
      "      6       \u001b[36m2.46526\u001b[0m       \u001b[32m2.56965\u001b[0m      0.95938      0.23488  29.32s\n",
      "      7       \u001b[36m2.45889\u001b[0m       \u001b[32m2.56691\u001b[0m      0.95792      0.23767  28.03s\n",
      "      8       \u001b[36m2.45323\u001b[0m       \u001b[32m2.56476\u001b[0m      0.95651      0.23948  27.25s\n",
      "      9       \u001b[36m2.44806\u001b[0m       \u001b[32m2.56277\u001b[0m      0.95524      0.24086  27.55s\n",
      "     10       \u001b[36m2.44328\u001b[0m       \u001b[32m2.56095\u001b[0m      0.95405      0.24257  28.48s\n",
      "     11       \u001b[36m2.43880\u001b[0m       \u001b[32m2.55926\u001b[0m      0.95293      0.24412  28.76s\n",
      "     12       \u001b[36m2.43459\u001b[0m       \u001b[32m2.55748\u001b[0m      0.95195      0.24557  27.50s\n",
      "     13       \u001b[36m2.43062\u001b[0m       \u001b[32m2.55580\u001b[0m      0.95102      0.24673  27.49s\n",
      "     14       \u001b[36m2.42683\u001b[0m       \u001b[32m2.55410\u001b[0m      0.95017      0.24769  26.81s\n",
      "     15       \u001b[36m2.42320\u001b[0m       \u001b[32m2.55243\u001b[0m      0.94937      0.24912  32.95s\n",
      "     16       \u001b[36m2.41973\u001b[0m       \u001b[32m2.55074\u001b[0m      0.94864      0.24991  27.45s\n",
      "     17       \u001b[36m2.41640\u001b[0m       \u001b[32m2.54907\u001b[0m      0.94795      0.25073  27.19s\n",
      "     18       \u001b[36m2.41320\u001b[0m       \u001b[32m2.54746\u001b[0m      0.94729      0.25131  27.64s\n",
      "     19       \u001b[36m2.41011\u001b[0m       \u001b[32m2.54573\u001b[0m      0.94673      0.25212  27.59s\n",
      "     20       \u001b[36m2.40713\u001b[0m       \u001b[32m2.54417\u001b[0m      0.94613      0.25237  27.74s\n",
      "     21       \u001b[36m2.40423\u001b[0m       \u001b[32m2.54263\u001b[0m      0.94557      0.25312  27.98s\n",
      "     22       \u001b[36m2.40142\u001b[0m       \u001b[32m2.54115\u001b[0m      0.94502      0.25364  28.32s\n",
      "     23       \u001b[36m2.39870\u001b[0m       \u001b[32m2.53982\u001b[0m      0.94444      0.25414  27.67s\n",
      "     24       \u001b[36m2.39605\u001b[0m       \u001b[32m2.53845\u001b[0m      0.94390      0.25473  27.55s\n",
      "     25       \u001b[36m2.39348\u001b[0m       \u001b[32m2.53703\u001b[0m      0.94342      0.25527  27.16s\n",
      "     26       \u001b[36m2.39097\u001b[0m       \u001b[32m2.53557\u001b[0m      0.94297      0.25562  27.96s\n",
      "     27       \u001b[36m2.38854\u001b[0m       \u001b[32m2.53436\u001b[0m      0.94246      0.25572  27.31s\n",
      "     28       \u001b[36m2.38616\u001b[0m       \u001b[32m2.53310\u001b[0m      0.94199      0.25599  27.40s\n",
      "     29       \u001b[36m2.38385\u001b[0m       \u001b[32m2.53179\u001b[0m      0.94157      0.25613  27.95s\n",
      "     30       \u001b[36m2.38158\u001b[0m       \u001b[32m2.53061\u001b[0m      0.94111      0.25643  27.77s\n",
      "     31       \u001b[36m2.37936\u001b[0m       \u001b[32m2.52959\u001b[0m      0.94061      0.25691  27.46s\n",
      "     32       \u001b[36m2.37719\u001b[0m       \u001b[32m2.52853\u001b[0m      0.94015      0.25729  26.98s\n",
      "     33       \u001b[36m2.37508\u001b[0m       \u001b[32m2.52752\u001b[0m      0.93969      0.25735  27.32s\n",
      "     34       \u001b[36m2.37302\u001b[0m       \u001b[32m2.52649\u001b[0m      0.93925      0.25758  28.74s\n",
      "     35       \u001b[36m2.37100\u001b[0m       \u001b[32m2.52558\u001b[0m      0.93879      0.25814  28.95s\n",
      "     36       \u001b[36m2.36901\u001b[0m       \u001b[32m2.52459\u001b[0m      0.93837      0.25844  27.47s\n",
      "     37       \u001b[36m2.36707\u001b[0m       \u001b[32m2.52390\u001b[0m      0.93786      0.25843  26.49s\n",
      "     38       \u001b[36m2.36516\u001b[0m       \u001b[32m2.52310\u001b[0m      0.93740      0.25868  31.21s\n",
      "     39       \u001b[36m2.36329\u001b[0m       \u001b[32m2.52237\u001b[0m      0.93693      0.25899  30.32s\n",
      "     40       \u001b[36m2.36144\u001b[0m       \u001b[32m2.52165\u001b[0m      0.93647      0.25932  30.02s\n",
      "     41       \u001b[36m2.35962\u001b[0m       \u001b[32m2.52111\u001b[0m      0.93595      0.25952  28.41s\n",
      "     42       \u001b[36m2.35785\u001b[0m       \u001b[32m2.52053\u001b[0m      0.93545      0.25951  28.58s\n",
      "     43       \u001b[36m2.35609\u001b[0m       \u001b[32m2.51998\u001b[0m      0.93496      0.25988  28.93s\n",
      "     44       \u001b[36m2.35436\u001b[0m       \u001b[32m2.51950\u001b[0m      0.93446      0.26017  29.41s\n",
      "     45       \u001b[36m2.35267\u001b[0m       \u001b[32m2.51907\u001b[0m      0.93395      0.26038  28.50s\n",
      "     46       \u001b[36m2.35101\u001b[0m       \u001b[32m2.51854\u001b[0m      0.93348      0.26049  28.09s\n",
      "     47       \u001b[36m2.34937\u001b[0m       \u001b[32m2.51802\u001b[0m      0.93302      0.26053  29.95s\n",
      "     48       \u001b[36m2.34775\u001b[0m       \u001b[32m2.51764\u001b[0m      0.93252      0.26083  29.70s\n",
      "     49       \u001b[36m2.34616\u001b[0m       \u001b[32m2.51718\u001b[0m      0.93206      0.26089  28.02s\n",
      "     50       \u001b[36m2.34460\u001b[0m       \u001b[32m2.51684\u001b[0m      0.93156      0.26099  28.76s\n",
      "     51       \u001b[36m2.34306\u001b[0m       \u001b[32m2.51644\u001b[0m      0.93110      0.26102  28.55s\n",
      "     52       \u001b[36m2.34154\u001b[0m       \u001b[32m2.51603\u001b[0m      0.93065      0.26124  28.60s\n",
      "     53       \u001b[36m2.34004\u001b[0m       \u001b[32m2.51559\u001b[0m      0.93021      0.26120  31.44s\n",
      "     54       \u001b[36m2.33857\u001b[0m       \u001b[32m2.51535\u001b[0m      0.92972      0.26123  27.90s\n",
      "     55       \u001b[36m2.33711\u001b[0m       \u001b[32m2.51519\u001b[0m      0.92920      0.26128  28.31s\n",
      "     56       \u001b[36m2.33567\u001b[0m       \u001b[32m2.51498\u001b[0m      0.92870      0.26125  31.21s\n",
      "     57       \u001b[36m2.33424\u001b[0m       \u001b[32m2.51477\u001b[0m      0.92821      0.26127  29.63s\n",
      "     58       \u001b[36m2.33284\u001b[0m       \u001b[32m2.51456\u001b[0m      0.92773      0.26138  28.14s\n",
      "     59       \u001b[36m2.33145\u001b[0m       \u001b[32m2.51428\u001b[0m      0.92728      0.26139  27.94s\n",
      "     60       \u001b[36m2.33008\u001b[0m       \u001b[32m2.51412\u001b[0m      0.92680      0.26135  27.61s\n",
      "     61       \u001b[36m2.32873\u001b[0m       \u001b[32m2.51404\u001b[0m      0.92629      0.26127  30.21s\n",
      "     62       \u001b[36m2.32740\u001b[0m       \u001b[32m2.51395\u001b[0m      0.92580      0.26122  29.23s\n",
      "     63       \u001b[36m2.32609\u001b[0m       \u001b[32m2.51379\u001b[0m      0.92533      0.26118  29.51s\n",
      "     64       \u001b[36m2.32479\u001b[0m       \u001b[32m2.51377\u001b[0m      0.92482      0.26098  29.66s\n",
      "     65       \u001b[36m2.32350\u001b[0m       \u001b[32m2.51370\u001b[0m      0.92433      0.26108  31.46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c8b9a10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c8b9990>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 299),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=65, more_params={},\n",
       "     objective=<function objective at 0x10c8c4140>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c6b4140>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x10448b758>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x1148486c8>],\n",
       "     output_nonlinearity=<function softmax at 0x10c5607d0>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c8b9a50>,\n",
       "     update=<function nesterov_momentum at 0x10c6b4de8>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The accuracy was 0.293386815542 (train + dev) before stopping potential overfitting\n",
    "# However, we stopped early @ 65 epochs to prevent overfitting\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # automatically calculate the number of featrues\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=65,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAESCAYAAADnvkIDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137b8650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net1.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net1.train_history_])\n",
    "plt.plot(train_loss, linewidth=3, label=\"train\")\n",
    "plt.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(1e-0, 1e1)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1513\n",
      "          1       0.20      0.14      0.16     76876\n",
      "          2       0.00      0.00      0.00       406\n",
      "          3       0.00      0.00      0.00       289\n",
      "          4       0.22      0.02      0.04     36755\n",
      "          5       0.21      0.00      0.01      4320\n",
      "          6       0.00      0.00      0.00      2268\n",
      "          7       0.29      0.50      0.37     53971\n",
      "          8       0.00      0.00      0.00      4280\n",
      "          9       0.00      0.00      0.00      1166\n",
      "         10       0.00      0.00      0.00       256\n",
      "         11       0.00      0.00      0.00       491\n",
      "         12       0.19      0.01      0.01     10609\n",
      "         13       0.25      0.00      0.00     16679\n",
      "         14       0.00      0.00      0.00       146\n",
      "         15       0.00      0.00      0.00      2341\n",
      "         16       0.32      0.72      0.45    174900\n",
      "         17       0.00      0.00      0.00      1903\n",
      "         18       0.56      0.03      0.06      1225\n",
      "         19       0.51      0.33      0.40     25989\n",
      "         20       0.24      0.15      0.18     92304\n",
      "         21       0.23      0.41      0.29    126182\n",
      "         22       0.00      0.00      0.00        22\n",
      "         23       0.48      0.61      0.53      7484\n",
      "         24       0.00      0.00      0.00      3138\n",
      "         25       0.14      0.00      0.00     23000\n",
      "         26       0.49      0.17      0.25      1946\n",
      "         27       0.00      0.00      0.00      9985\n",
      "         28       0.00      0.00      0.00      4388\n",
      "         29       0.00      0.00      0.00       148\n",
      "         30       0.00      0.00      0.00      4540\n",
      "         31       0.00      0.00      0.00       508\n",
      "         32       0.00      0.00      0.00     31414\n",
      "         33       0.00      0.00      0.00         6\n",
      "         34       0.24      0.02      0.04      7326\n",
      "         35       0.18      0.02      0.03     44725\n",
      "         36       0.33      0.14      0.19     53781\n",
      "         37       0.16      0.01      0.01     42214\n",
      "         38       0.00      0.00      0.00      8555\n",
      "\n",
      "avg / total       0.24      0.29      0.22    878049\n",
      "\n",
      "The accuracy is: 0.285459011969\n"
     ]
    }
   ],
   "source": [
    "# View accuracy for each classification type. Note that this accuracy is measured on all training data, and thus\n",
    "# is higher than what we would expect on test data. This metric still was useful when comparing feature sets and model\n",
    "# hyperparamaters.\n",
    "\n",
    "train_pred = net1.predict(np_train_data)\n",
    "print(classification_report(train_labels_int, train_pred))\n",
    "print 'The accuracy is:', accuracy_score(train_labels_int, train_pred)\n",
    "\n",
    "# Get probabilities for submission to Kaggle\n",
    "test_proba = net1.predict_proba(np_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Net with 2 Hidden Layers + 1 Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next tried adding an additional hidden layer, including a \"dropout layer\" to help prevent overfitting. The dropout layer randomly severs a certain percentage of the connections between the nodes.\n",
    "\n",
    "We iterated through many possible values and settled on the ones below. However, further iteration with a faster processing speed (via a GPU) would likely lead to improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 420039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      500\n",
      "  2  dropout     500\n",
      "  3  dense1      500\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.49777\u001b[0m       \u001b[32m2.64343\u001b[0m      0.94490      0.22286  80.71s\n",
      "      2       \u001b[36m2.43491\u001b[0m       \u001b[32m2.59972\u001b[0m      0.93660      0.23403  80.35s\n",
      "      3       \u001b[36m2.41387\u001b[0m       \u001b[32m2.57477\u001b[0m      0.93751      0.24013  81.41s\n",
      "      4       \u001b[36m2.40082\u001b[0m       \u001b[32m2.55420\u001b[0m      0.93995      0.24428  82.20s\n",
      "      5       \u001b[36m2.39132\u001b[0m       \u001b[32m2.54141\u001b[0m      0.94094      0.24588  83.52s\n",
      "      6       \u001b[36m2.38321\u001b[0m       \u001b[32m2.53455\u001b[0m      0.94029      0.24774  82.93s\n",
      "      7       \u001b[36m2.37690\u001b[0m       \u001b[32m2.52936\u001b[0m      0.93972      0.24789  82.61s\n",
      "      8       \u001b[36m2.37140\u001b[0m       \u001b[32m2.51653\u001b[0m      0.94233      0.25034  82.85s\n",
      "      9       \u001b[36m2.36682\u001b[0m       \u001b[32m2.51053\u001b[0m      0.94276      0.25273  82.66s\n",
      "     10       \u001b[36m2.36227\u001b[0m       \u001b[32m2.50447\u001b[0m      0.94322      0.25404  82.91s\n",
      "     11       \u001b[36m2.35771\u001b[0m       2.50597      0.94084      0.25417  82.88s\n",
      "     12       \u001b[36m2.35414\u001b[0m       \u001b[32m2.50088\u001b[0m      0.94132      0.25493  82.83s\n",
      "     13       \u001b[36m2.35088\u001b[0m       \u001b[32m2.49870\u001b[0m      0.94084      0.25513  82.91s\n",
      "     14       \u001b[36m2.34852\u001b[0m       \u001b[32m2.49291\u001b[0m      0.94208      0.25671  82.91s\n",
      "     15       \u001b[36m2.34561\u001b[0m       \u001b[32m2.49078\u001b[0m      0.94172      0.25819  83.04s\n",
      "     16       \u001b[36m2.34285\u001b[0m       \u001b[32m2.48882\u001b[0m      0.94135      0.25744  83.02s\n",
      "     17       \u001b[36m2.33990\u001b[0m       \u001b[32m2.48666\u001b[0m      0.94098      0.25939  82.93s\n",
      "     18       \u001b[36m2.33734\u001b[0m       \u001b[32m2.48640\u001b[0m      0.94005      0.25912  82.86s\n",
      "     19       \u001b[36m2.33535\u001b[0m       \u001b[32m2.48623\u001b[0m      0.93931      0.25864  83.17s\n",
      "     20       \u001b[36m2.33354\u001b[0m       \u001b[32m2.48333\u001b[0m      0.93968      0.25974  82.75s\n",
      "     21       \u001b[36m2.33089\u001b[0m       \u001b[32m2.48254\u001b[0m      0.93891      0.25916  82.97s\n",
      "     22       \u001b[36m2.32917\u001b[0m       \u001b[32m2.47983\u001b[0m      0.93925      0.26040  85.38s\n",
      "     23       \u001b[36m2.32727\u001b[0m       2.48016      0.93835      0.26162  86.05s\n",
      "     24       \u001b[36m2.32594\u001b[0m       \u001b[32m2.47887\u001b[0m      0.93831      0.26149  84.90s\n",
      "     25       \u001b[36m2.32317\u001b[0m       2.47925      0.93704      0.26205  86.40s\n",
      "     26       \u001b[36m2.32193\u001b[0m       \u001b[32m2.47855\u001b[0m      0.93681      0.26148  84.95s\n",
      "     27       \u001b[36m2.32002\u001b[0m       \u001b[32m2.47825\u001b[0m      0.93615      0.26147  85.35s\n",
      "     28       \u001b[36m2.31859\u001b[0m       \u001b[32m2.47635\u001b[0m      0.93629      0.26187  85.21s\n",
      "     29       \u001b[36m2.31681\u001b[0m       \u001b[32m2.47531\u001b[0m      0.93597      0.26293  85.71s\n",
      "     30       \u001b[36m2.31531\u001b[0m       2.47668      0.93484      0.26159  85.68s\n",
      "     31       \u001b[36m2.31367\u001b[0m       2.47534      0.93469      0.26392  85.52s\n",
      "     32       \u001b[36m2.31300\u001b[0m       2.47536      0.93441      0.26268  84.69s\n",
      "     33       \u001b[36m2.31076\u001b[0m       2.47710      0.93285      0.26222  85.35s\n",
      "     34       \u001b[36m2.30927\u001b[0m       2.47605      0.93264      0.26238  86.49s\n",
      "     35       \u001b[36m2.30826\u001b[0m       2.47607      0.93223      0.26185  88.26s\n",
      "     36       \u001b[36m2.30710\u001b[0m       \u001b[32m2.47463\u001b[0m      0.93230      0.26294  86.80s\n",
      "     37       \u001b[36m2.30558\u001b[0m       \u001b[32m2.47318\u001b[0m      0.93223      0.26316  87.60s\n",
      "     38       \u001b[36m2.30439\u001b[0m       2.47323      0.93173      0.26298  88.59s\n",
      "     39       \u001b[36m2.30360\u001b[0m       2.47650      0.93019      0.26086  86.68s\n",
      "     40       \u001b[36m2.30285\u001b[0m       2.47457      0.93061      0.26186  87.29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c8b9a10>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c8b9990>,\n",
       "     custom_score=None, dense0_num_units=500, dense1_num_units=500,\n",
       "     dropout_p=0.5, input_shape=(None, 299),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=40, more_params={},\n",
       "     objective=<function objective at 0x10c8c4140>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c6b4140>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x114d990e0>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x114d99320>],\n",
       "     output_nonlinearity=<function softmax at 0x10c5607d0>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c8b9a50>,\n",
       "     update=<function nesterov_momentum at 0x10c6b4de8>,\n",
       "     update_learning_rate=0.02, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net2 = NeuralNet(\n",
    "    layers=[  # more layers\n",
    "        ('input', layers.InputLayer),\n",
    "        ('dense0', layers.DenseLayer),\n",
    "        ('dropout', layers.DropoutLayer),\n",
    "        ('dense1', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer)\n",
    "    ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    dense0_num_units=500,  # number of units in hidden layer\n",
    "    dropout_p=0.5,  # randomly disconect a number of notes to help preven overfitting\n",
    "    dense1_num_units=500,  # number of units in hidden layer\n",
    "    output_num_units=39,  # 39 target values\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.02,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=40,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "net2.fit(np_train_data, train_labels_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAESCAYAAADnvkIDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngZAktbJASJJaWSAkSa0sEJKkVv8fZCm/SG/2mWMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11306fc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net2.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net2.train_history_])\n",
    "plt.plot(train_loss, linewidth=3, label=\"train\")\n",
    "plt.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(1e-0, 1e1)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1513\n",
      "          1       0.20      0.20      0.20     76876\n",
      "          2       0.00      0.00      0.00       406\n",
      "          3       0.00      0.00      0.00       289\n",
      "          4       0.24      0.06      0.09     36755\n",
      "          5       0.29      0.04      0.06      4320\n",
      "          6       0.00      0.00      0.00      2268\n",
      "          7       0.31      0.48      0.38     53971\n",
      "          8       0.00      0.00      0.00      4280\n",
      "          9       0.00      0.00      0.00      1166\n",
      "         10       0.00      0.00      0.00       256\n",
      "         11       0.00      0.00      0.00       491\n",
      "         12       0.18      0.03      0.06     10609\n",
      "         13       0.17      0.00      0.00     16679\n",
      "         14       0.00      0.00      0.00       146\n",
      "         15       0.00      0.00      0.00      2341\n",
      "         16       0.33      0.67      0.45    174900\n",
      "         17       0.35      0.02      0.04      1903\n",
      "         18       0.49      0.09      0.16      1225\n",
      "         19       0.60      0.34      0.43     25989\n",
      "         20       0.25      0.17      0.20     92304\n",
      "         21       0.24      0.40      0.30    126182\n",
      "         22       0.00      0.00      0.00        22\n",
      "         23       0.53      0.57      0.55      7484\n",
      "         24       0.00      0.00      0.00      3138\n",
      "         25       0.20      0.00      0.00     23000\n",
      "         26       0.48      0.22      0.30      1946\n",
      "         27       0.39      0.00      0.01      9985\n",
      "         28       0.00      0.00      0.00      4388\n",
      "         29       0.00      0.00      0.00       148\n",
      "         30       0.00      0.00      0.00      4540\n",
      "         31       0.00      0.00      0.00       508\n",
      "         32       0.00      0.00      0.00     31414\n",
      "         33       0.00      0.00      0.00         6\n",
      "         34       0.30      0.01      0.02      7326\n",
      "         35       0.42      0.01      0.02     44725\n",
      "         36       0.25      0.27      0.26     53781\n",
      "         37       0.20      0.01      0.03     42214\n",
      "         38       0.00      0.00      0.00      8555\n",
      "\n",
      "avg / total       0.27      0.29      0.24    878049\n",
      "\n",
      "The accuracy is: 0.292563399081\n"
     ]
    }
   ],
   "source": [
    "train_pred = net2.predict(np_train_data)\n",
    "print(classification_report(train_labels_int, train_pred))\n",
    "print 'The accuracy is:', accuracy_score(train_labels_int, train_pred)\n",
    "\n",
    "# Get probabilities for submission to Kaggle\n",
    "test_proba = net2.predict_proba(np_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Summary Table\n",
    "\n",
    "Overall, we improved our score on Kaggle from 31.0522 to 2.4282 using a variety of different methods. This landed us in the top 50. We believe that the neural net and gradient boosting algorithms were the most promising, and given more time and computing power, we could further improve these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Submission                                           Score\n",
      "------------------------------------------------------------------\n",
      "Very first baseline model                                 31.0522\n",
      "KNN = 1, Normalized, top 4 crimes, category avg for 0s     2.9256\n",
      "Logistic regression, C = 0.001                             2.6973\n",
      "BernoulliNB, alpha = 1.0                                   2.5669\n",
      "Gradient boosting                                          2.4956\n",
      "Neural Net                                                 2.4282\n"
     ]
    }
   ],
   "source": [
    "table_form = \"{:55s}{:>10.4f}\".format\n",
    "print \"{:55s}{:>10s}\".format(\"Kaggle Submission\", \"Score\")\n",
    "print \"------------------------------------------------------------------\"\n",
    "print table_form(\"Very first baseline model\", 31.0522)\n",
    "print table_form(\"KNN = 1, Normalized, top 4 crimes, category avg for 0s\", 2.9256)\n",
    "print table_form(\"Logistic regression, C = 0.001\", 2.6973)\n",
    "print table_form(\"BernoulliNB, alpha = 1.0\", 2.5669)\n",
    "print table_form(\"Gradient boosting\", 2.4956)\n",
    "print table_form(\"Neural Net\", 2.4282)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Appendix - Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted a decision tree and random forest for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model           accuracy  log-loss  \n",
      "-----------------------------------\n",
      "decision tree   0.2253    26.2909   \n",
      "random forest   0.2454    14.4204   \n"
     ]
    }
   ],
   "source": [
    "result_form = \"{:<16s}{:<10.4f}{:<10.4f}\".format\n",
    "\n",
    "print \"{:<16s}{:<10s}{:<10s}\".format(\"model\", \"accuracy\", \"log-loss\")\n",
    "print \"-----------------------------------\"\n",
    "\n",
    "# Let's try a decision tree\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "dt_probs = dt.predict_proba(test_data[features_to_use])\n",
    "accuracy = dt.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, dt.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"decision tree\", accuracy, logloss)\n",
    "\n",
    "# Let's try a random forest\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "rf_probs = rf.predict_proba(test_data[features_to_use])\n",
    "accuracy = rf.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, rf.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"random forest\", accuracy, logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
