{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UC Berkeley W207 - Machine Learning\n",
    "Miki Seltzer, Eric Freeman, Christopher Llop\n",
    "\n",
    "This Notebook contains our solution to the Kaggle SF Crime problem available at:\n",
    "https://www.kaggle.com/c/sf-crime\n",
    "\n",
    "The notebook covers the following methods and includes discussion of key decisions along the way (as of 2015-06-17). If you are not part of our W207 class, you are welcome to look through our methods and leverage our code to develop new solutions. We may also continue work after creating this report - just go by the \"last modified\" date on GitHub.\n",
    "\n",
    "Concepts Covered:\n",
    "1. Basic data exploration\n",
    "2. K-Nearest Neighbors\n",
    "3. Logistic Regression\n",
    "4. Bernoulli Naive Bayes\n",
    "5. Gradient Boosting\n",
    "6. Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Libraries for Neural Networks. The versions currently available via \"pip\" are not up to date, and do\n",
    "# not sync with each other. To use these packages, install directly from github using the instructions\n",
    "# here:\n",
    "# https://github.com/dnouri/nolearn\n",
    "from lasagne import layers\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started with a basic exploration of the data. For the purposes of this report only some of those checks are\n",
    "shown below. You can examine these searches further in the \"Load and Examine Data\" iPython notebook. \n",
    "\n",
    "Some tasks:    \n",
    "- Count unique values for each variable\n",
    "- View most common values\n",
    "- XY plot of lat/long w. circles to indicate number of crimes\n",
    "- Time series plots to see how category use changes over time\n",
    "\n",
    "Interesting Points:\n",
    "- Most crime on Friday, then Wednesday. Least on Sunday.\n",
    "- X and Y latitude have same number of distinct values. Seem to be somehow linked to locations\n",
    "  since, despite there being a lots of sig fig, they still can be frequency counted\n",
    "- 800 Block of BRYANT ST has 4x+ more data points than anyplace else. Seems to link w/ most freq X and Y\n",
    "- \"Other Offenses\" are common\n",
    "- The dates with the most crime are new years day. Also the first of months.\n",
    "- Note: Strange max value of Y = 90 for 67 values. These appear to be in Chicago, but the data has addresses in SF. We removed this data from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 878,049, values.\n",
      "There are a total of 389,257 Dates.\n",
      "There are a total of 39 Category.\n",
      "There are a total of 879 Descript.\n",
      "There are a total of 7 DayOfWeek.\n",
      "There are a total of 10 PdDistrict.\n",
      "There are a total of 17 Resolution.\n",
      "There are a total of 23,228 Address.\n",
      "There are a total of 34,243 X.\n",
      "There are a total of 34,243 Y.\n",
      "-------------------------------------------------------------------------\n",
      "There are a total of 39 distinct Category values, as follows: \n",
      "LARCENY/THEFT                  0.199192\n",
      "OTHER OFFENSES                 0.143707\n",
      "NON-CRIMINAL                   0.105124\n",
      "ASSAULT                        0.087553\n",
      "DRUG/NARCOTIC                  0.061467\n",
      "VEHICLE THEFT                  0.061251\n",
      "VANDALISM                      0.050937\n",
      "WARRANTS                       0.048077\n",
      "BURGLARY                       0.041860\n",
      "SUSPICIOUS OCC                 0.035777\n",
      "MISSING PERSON                 0.029599\n",
      "ROBBERY                        0.026194\n",
      "FRAUD                          0.018996\n",
      "FORGERY/COUNTERFEITING         0.012082\n",
      "SECONDARY CODES                0.011372\n",
      "WEAPON LAWS                    0.009743\n",
      "PROSTITUTION                   0.008523\n",
      "TRESPASS                       0.008343\n",
      "STOLEN PROPERTY                0.005171\n",
      "SEX OFFENSES FORCIBLE          0.004997\n",
      "DISORDERLY CONDUCT             0.004920\n",
      "DRUNKENNESS                    0.004874\n",
      "RECOVERED VEHICLE              0.003574\n",
      "KIDNAPPING                     0.002666\n",
      "DRIVING UNDER THE INFLUENCE    0.002583\n",
      "RUNAWAY                        0.002216\n",
      "LIQUOR LAWS                    0.002167\n",
      "ARSON                          0.001723\n",
      "LOITERING                      0.001395\n",
      "EMBEZZLEMENT                   0.001328\n",
      "SUICIDE                        0.000579\n",
      "FAMILY OFFENSES                0.000559\n",
      "BAD CHECKS                     0.000462\n",
      "BRIBERY                        0.000329\n",
      "EXTORTION                      0.000292\n",
      "SEX OFFENSES NON FORCIBLE      0.000169\n",
      "GAMBLING                       0.000166\n",
      "PORNOGRAPHY/OBSCENE MAT        0.000025\n",
      "TREA                           0.000007\n",
      "dtype: float64\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "There are a total of 10 distinct PdDistrict values, as follows: \n",
      "SOUTHERN      0.179013\n",
      "MISSION       0.136562\n",
      "NORTHERN      0.119920\n",
      "BAYVIEW       0.101852\n",
      "CENTRAL       0.097329\n",
      "TENDERLOIN    0.093171\n",
      "INGLESIDE     0.089796\n",
      "TARAVAL       0.074707\n",
      "PARK          0.056162\n",
      "RICHMOND      0.051488\n",
      "dtype: float64\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "There are a total of 17 distinct Resolution values, as follows: \n",
      "NONE                                      0.599955\n",
      "ARREST, BOOKED                            0.235070\n",
      "ARREST, CITED                             0.087699\n",
      "LOCATED                                   0.019476\n",
      "PSYCHOPATHIC CASE                         0.016553\n",
      "UNFOUNDED                                 0.010916\n",
      "JUVENILE BOOKED                           0.006337\n",
      "COMPLAINANT REFUSES TO PROSECUTE          0.004528\n",
      "DISTRICT ATTORNEY REFUSES TO PROSECUTE    0.004480\n",
      "NOT PROSECUTED                            0.004230\n",
      "JUVENILE CITED                            0.003795\n",
      "PROSECUTED BY OUTSIDE AGENCY              0.002852\n",
      "EXCEPTIONAL CLEARANCE                     0.001742\n",
      "JUVENILE ADMONISHED                       0.001657\n",
      "JUVENILE DIVERTED                         0.000404\n",
      "CLEARED-CONTACT JUVENILE FOR MORE INFO    0.000247\n",
      "PROSECUTED FOR LESSER OFFENSE             0.000058\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_orig = pd.read_csv(\"train.csv\")\n",
    "test_orig = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Count distinct for each variable:\n",
    "print \"There are a total of {:,}, values.\".format(len(data_orig))\n",
    "\n",
    "for var, series in data_orig.iteritems():\n",
    "    print \"There are a total of {:,} {}.\".format(len(series.value_counts()), var)\n",
    "# View All of Categories, PdDistrict, Resolution, DayOfWeek\n",
    "variables = [\"Category\", \"PdDistrict\", \"Resolution\"]\n",
    "x = data_orig[\"Category\"].value_counts()/len(data_orig)\n",
    "for col in variables:\n",
    "    print \"-------------------------------------------------------------------------\"\n",
    "    print \"There are a total of {:,} distinct {} values, as follows: \".format(len(data_orig[col].value_counts()), col)\n",
    "    print data_orig[col].value_counts()/len(data_orig)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We added several new date related features to the dataset right away, which were used in data investigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_clean = data_orig[data_orig.Y != 90]\n",
    "\n",
    "def add_date_vars(data):\n",
    "    data['Dates'] = pd.to_datetime(data['Dates'])\n",
    "    data['Year'] = data.Dates.dt.year\n",
    "    data['Month'] = data.Dates.dt.month\n",
    "    data['Day'] = data.Dates.dt.day\n",
    "    data['Date'] = data.Dates.dt.date\n",
    "    data['Hour'] = data.Dates.dt.hour\n",
    "    data['DayOfYear'] = data.Dates.dt.dayofyear\n",
    "    data['WeekDay'] = data.Dates.dt.weekday\n",
    "    \n",
    "    datetime_vector = data['Dates']\n",
    "    date_vector = datetime_vector.dt.date\n",
    "    date_diff_vector = (date_vector - date_vector.min()) / np.timedelta64(1, 'D')\n",
    "    data['DateDiff'] = date_diff_vector\n",
    "\n",
    "\n",
    "add_date_vars(data_clean)\n",
    "add_date_vars(test_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also created a standard \"output\" function that we used across sensitivites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a function that will create a correctly formatted output file for submission to Kaggle.\n",
    "\n",
    "def create_submission(preds):\n",
    "    labels = [\"Id\",\"ARSON\",\"ASSAULT\",\"BAD CHECKS\",\"BRIBERY\",\"BURGLARY\",\"DISORDERLY CONDUCT\",\"DRIVING UNDER THE INFLUENCE\",\n",
    "              \"DRUG/NARCOTIC\",\"DRUNKENNESS\",\"EMBEZZLEMENT\",\"EXTORTION\",\"FAMILY OFFENSES\",\"FORGERY/COUNTERFEITING\",\n",
    "              \"FRAUD\",\"GAMBLING\",\"KIDNAPPING\",\"LARCENY/THEFT\",\"LIQUOR LAWS\",\"LOITERING\",\"MISSING PERSON\",\"NON-CRIMINAL\",\n",
    "              \"OTHER OFFENSES\",\"PORNOGRAPHY/OBSCENE MAT\",\"PROSTITUTION\",\"RECOVERED VEHICLE\",\"ROBBERY\",\"RUNAWAY\",\n",
    "              \"SECONDARY CODES\",\"SEX OFFENSES FORCIBLE\",\"SEX OFFENSES NON FORCIBLE\",\"STOLEN PROPERTY\",\"SUICIDE\",\n",
    "              \"SUSPICIOUS OCC\",\"TREA\",\"TRESPASS\",\"VANDALISM\",\"VEHICLE THEFT\",\"WARRANTS\",\"WEAPON LAWS\"]\n",
    "    head_str = ','.join(labels)\n",
    "\n",
    "    num_cats = len(labels)\n",
    "    \n",
    "    # Make a dummy row to append to\n",
    "    ids = np.arange(preds.shape[0])[np.newaxis].transpose()\n",
    "    \n",
    "    results = np.column_stack((ids, preds))\n",
    "\n",
    "    num_form = ['%6f'] * (num_cats - 1)\n",
    "    num_form.insert(0, '%d')\n",
    "    # Write results to csv\n",
    "    np.savetxt('sample.csv', results, fmt=num_form, delimiter=',', header=head_str, comments='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first model was a simple KNN model. We tried a variety of N's for neighbors and both normalized and regular data. We also looked at just including a subset of crimes for it to predict. In the end, we chose N=1, normalized data, and only attempted to predict the top 4 crimes. \n",
    "\n",
    "KNN makes classification predictions based on the nearest neighbors - the predicted category is set to 1 and all others to 0. This resulted in a low score (27) because the ranking algorythem expected us to submit the likelihood of each potential classification, instead of just an absolute guess.\n",
    "\n",
    "For the final KNN submission, we replaced the 0's with average probability of the crime as observed in the entire dataset. This simple change improved submission score from 27 to 2.92."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only look at top 4 crimes\n",
    "stk_list = ['LARCENY/THEFT','OTHER OFFENSES','NON-CRIMINAL','ASSAULT']\n",
    "data = data_clean[data_clean.Category.isin(stk_list)]\n",
    "\n",
    "# Create random dev sample so we can see how that accuracy compares to our Kaggle results\n",
    "np.random.seed(100)\n",
    "\n",
    "rows = np.random.choice(data.index, size = len(data) / 10, replace = False)\n",
    "\n",
    "dev = data.ix[rows]\n",
    "train = data.drop(rows)\n",
    "\n",
    "# Convert to Numpy Format with only DateDiff, X and Y features\n",
    "train_data = np.array(train[['DateDiff','X','Y']].values)\n",
    "train_labels = np.array(train[['Category']].values.ravel())\n",
    "\n",
    "dev_data = np.array(dev[['DateDiff','X','Y']].values)\n",
    "dev_labels = np.array(dev[['Category']].values.ravel())\n",
    "\n",
    "full_data = np.array(data[['DateDiff','X','Y']].values)\n",
    "full_labels = np.array(data[['Category']].values.ravel())\n",
    "\n",
    "test_data = np.array(test_orig[['DateDiff','X','Y']].values)\n",
    "\n",
    "# Normalize Data to Between 0-1\n",
    "#a + (x-A)*(b-a)/(B-A) \n",
    "\n",
    "def normalize(data):\n",
    "    return 0 + (np.abs(data) - np.abs(data).min(axis=0))*(1-0)/(np.abs(data).max(axis=0) - np.abs(data).min(axis=0))\n",
    "\n",
    "train_normed = normalize(train_data)\n",
    "dev_normed = normalize(dev_data)\n",
    "test_normed = normalize(test_data)\n",
    "full_normed = normalize(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scores for each k value was [mean: 0.22013, std: 0.03731, params: {'n_neighbors': 1}, mean: 0.23360, std: 0.04609, params: {'n_neighbors': 2}, mean: 0.22577, std: 0.04040, params: {'n_neighbors': 3}, mean: 0.23504, std: 0.04684, params: {'n_neighbors': 4}, mean: 0.22913, std: 0.04194, params: {'n_neighbors': 5}, mean: 0.23679, std: 0.04770, params: {'n_neighbors': 6}, mean: 0.23198, std: 0.04362, params: {'n_neighbors': 7}, mean: 0.23708, std: 0.04755, params: {'n_neighbors': 8}, mean: 0.23393, std: 0.04445, params: {'n_neighbors': 9}, mean: 0.23831, std: 0.04801, params: {'n_neighbors': 10}] \n",
      "\n",
      "The best k value was {'n_neighbors': 10} with accuracy 0.2383\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to find a good number of neighbors.\n",
    "\n",
    "ks = {'n_neighbors': [1,2,3,4,5,6,7,8,9,10]}\n",
    "KNNGridSearch = GridSearchCV(KNeighborsClassifier(), ks, scoring='accuracy')\n",
    "KNNGridSearch.fit(train_normed, train_labels)\n",
    "\n",
    "# Report out on the accuracies    \n",
    "print \"The scores for each k value was %s \" % (KNNGridSearch.grid_scores_)\n",
    "print \"\\nThe best k value was %s with accuracy %.4f\" % (KNNGridSearch.best_params_, KNNGridSearch.best_score_)\n",
    "best_n = KNNGridSearch.best_score_['n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors\n",
      "\n",
      "model           neighbors accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "baseline        5.0000    0.4179    7.4622    \n",
      "optimized n     10.0000   0.4310    3.6500    \n"
     ]
    }
   ],
   "source": [
    "result_form = \"{:<16s}{:<10.4f}{:<10.4f}{:<10.4f}\".format\n",
    "print \"K-Nearest Neighbors\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"model\", \"neighbors\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Now that we've done this, let's run the KNN on the full train, apply to the test, then format.\n",
    "KNNmodel = KNeighborsClassifier()\n",
    "KNNmodel.fit(train_normed, train_labels)\n",
    "base_n = 5\n",
    "base_accuracy = KNNmodel.score(dev_normed, dev_labels)\n",
    "base_logloss = log_loss(dev_labels, KNNmodel.predict_proba(dev_normed))\n",
    "print result_form(\"baseline\", base_n, base_accuracy, base_logloss)\n",
    "\n",
    "# Use optimized N\n",
    "KNNmodel = KNeighborsClassifier(n_neighbors = best_n)\n",
    "KNNmodel.fit(train_normed, train_labels)\n",
    "accuracy = KNNmodel.score(dev_normed, dev_labels)\n",
    "logloss = log_loss(dev_labels, KNNmodel.predict_proba(dev_normed))\n",
    "print result_form(\"optimized n\", best_n, accuracy, logloss)\n",
    "test_predict = KNNmodel.predict_proba(test_normed).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that entering the other 35 categories with their average probability was done by hand in Excel. We looked at doing it in python, but realized we wanted to pursue other models instead. The create_submission function needs all categories to be input, so in this case we did not use the create_submission function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model was logistic regression. A logistic regression method (and all others examined in this notebook) allows output of predicted probabilites, removing the need to use the simple occurance averages.\n",
    "\n",
    "We tried several values of C, which determines the regularization strength, before settling on C=0.001. This improved our score from 2.92 to 2.69.\n",
    "\n",
    "After submitting this model, we tried averaging the KNN and LR in various ways, but that did not improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "# Let's go back to using all crimes\n",
    "data = data_clean.copy()\n",
    "\n",
    "np.random.seed()\n",
    "rows = np.random.choice(data.index, size = len(data) / 10, replace = False)\n",
    "\n",
    "dev = data.ix[rows]\n",
    "train = data.drop(rows)\n",
    "\n",
    "# Convert to Numpy Format with only DateDiff, X and Y features\n",
    "train_data = np.array(train[['DateDiff','X','Y']].values)\n",
    "train_labels = np.array(train[['Category']].values.ravel())\n",
    "\n",
    "dev_data = np.array(dev[['DateDiff','X','Y']].values)\n",
    "dev_labels = np.array(dev[['Category']].values.ravel())\n",
    "\n",
    "full_data = np.array(data[['DateDiff','X','Y']].values)\n",
    "full_labels = np.array(data[['Category']].values.ravel())\n",
    "\n",
    "test_data = np.array(test_orig[['DateDiff','X','Y']].values)\n",
    "\n",
    "# We need to re-normalize the data sets containing all crimes, not just the top 4 crimes\n",
    "train_normed = normalize(train_data)\n",
    "dev_normed = normalize(dev_data)\n",
    "test_normed = normalize(test_data)\n",
    "full_normed = normalize(full_data)\n",
    "\n",
    "print len(dev['Category'].unique())\n",
    "print len(train['Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
    "clf.fit(train_data, train_labels)\n",
    "print clf.best_params_\n",
    "best_c = clf.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAFHCAYAAADz1HtzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERE1ECStoiIiIga+P+3YHYTbmzJYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e9966a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\n",
      "model           C         accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "baseline        1.0000    0.2156    2.6448    \n",
      "optimized C     0.0010    0.2148    2.6977    \n"
     ]
    }
   ],
   "source": [
    "res = zip(*[(f1m, f1s.std(), p['C']) \n",
    "            for p, f1m, f1s in clf.grid_scores_])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.log10(res[2]),res[0],'-o')\n",
    "plt.xlabel(\"Log(C)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.log10(res[2]),res[1],'-o')\n",
    "plt.xlabel(\"Log(C)\")\n",
    "plt.ylabel(\"StDev of Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "result_form = \"{:<16s}{:<10.4f}{:<10.4f}{:<10.4f}\".format\n",
    "print \"Logistic Regression\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"model\", \"C\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Logistic Regression: baseline\n",
    "model = LogisticRegression()\n",
    "model.fit(train_normed, train_labels)\n",
    "base_c = 1.0\n",
    "base_accuracy = model.score(dev_normed, dev_labels)\n",
    "base_logloss = log_loss(dev_labels, model.predict_proba(dev_normed)) \n",
    "print result_form(\"baseline\", base_c, base_accuracy, base_logloss)\n",
    "\n",
    "# Logistic Regression: optimized C\n",
    "model.set_params(C=best_c)\n",
    "model.fit(train_normed, train_labels)\n",
    "test_predict = model.predict_proba(test_normed)\n",
    "accuracy = model.score(dev_normed, dev_labels)\n",
    "logloss = log_loss(dev_labels, model.predict_proba(dev_normed)) \n",
    "print result_form(\"optimized C\", best_c, accuracy, logloss)\n",
    "\n",
    "#results = create_submission(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Bernoulli Naive Bayes\n",
    "##Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BernoulliNB requires binary data, so we created dummy variables for each variable in the data set (all date and district variables). We also left all variables in the data set so that we could include and exclude features in our models easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates' 'Category' 'Descript' 'DayOfWeek' 'PdDistrict' 'Resolution'\n",
      " 'Address' 'X' 'Y' 'DateTime' 'Year' 'Month' 'Day' 'Hour' 'SecondsDelta'\n",
      " 'Weekend' 2003L 2004L 2005L 2006L 2007L 2008L 2009L 2010L 2011L 2012L\n",
      " 2013L 2014L 2015L 'Jan' 'Feb' 'Mar' 'Apr' 'May' 'Jun' 'Jul' 'Aug' 'Sep'\n",
      " 'Oct' 'Nov' 'Dec' 1L 2L 3L 4L 5L 6L 7L 8L 9L 10L 11L 12L 13L 14L 15L 16L\n",
      " 17L 18L 19L 20L 21L 22L 23L 24L 25L 26L 27L 28L 29L 30L 31L 'Friday'\n",
      " 'Monday' 'Saturday' 'Sunday' 'Thursday' 'Tuesday' 'Wednesday' '12AM' '1AM'\n",
      " '2AM' '3AM' '4AM' '5AM' '6AM' '7AM' '8AM' '9AM' '10AM' '11AM' '12PM' '1PM'\n",
      " '2PM' '3PM' '4PM' '5PM' '6PM' '7PM' '8PM' '9PM' '10PM' '11PM' 'BAYVIEW'\n",
      " 'CENTRAL' 'INGLESIDE' 'MISSION' 'NORTHERN' 'PARK' 'RICHMOND' 'SOUTHERN'\n",
      " 'TARAVAL' 'TENDERLOIN']\n"
     ]
    }
   ],
   "source": [
    "# Extract new features in Pandas\n",
    "def time_features(data):\n",
    "    data['DateTime'] = pd.to_datetime(data['Dates'])\n",
    "    data['Year'] = pd.DatetimeIndex(data['DateTime']).year\n",
    "    data['Month'] = pd.DatetimeIndex(data['DateTime']).month\n",
    "    data['Day'] = pd.DatetimeIndex(data['DateTime']).day\n",
    "    data['Hour'] = pd.DatetimeIndex(data['DateTime']).hour\n",
    "    data['SecondsDelta'] = (data.DateTime - pd.Timestamp('2013-01-01')) / np.timedelta64(1,'s')\n",
    "    data['Weekend'] = (data.DayOfWeek == \"Saturday\") | (data.DayOfWeek == \"Sunday\")\n",
    "    years = pd.get_dummies(data.Year)\n",
    "    months = pd.get_dummies(data.Month)\n",
    "    months.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    days = pd.get_dummies(data.Day)\n",
    "    daysofweek = pd.get_dummies(data.DayOfWeek)\n",
    "    hours = pd.get_dummies(data.Hour)\n",
    "    hours.columns = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM',\n",
    "                     '6AM', '7AM', '8AM', '9AM', '10AM', '11AM',\n",
    "                     '12PM', '1PM', '2PM', '3PM', '4PM', '5PM',\n",
    "                     '6PM', '7PM', '8PM', '9PM', '10PM', '11PM']\n",
    "    districts = pd.get_dummies(data.PdDistrict)\n",
    "    new_data = pd.concat([data, years, months, days, daysofweek, hours, districts], axis=1)\n",
    "    return new_data\n",
    "\n",
    "data = time_features(data_orig)\n",
    "test = time_features(test_orig)\n",
    "\n",
    "# Separate labels\n",
    "labels = data.Category\n",
    "\n",
    "# Drop Category, Descript and Resolution columns since we cannot use them to predict\n",
    "train_data = data.drop(['Category', 'Descript', 'Resolution'], axis=1)\n",
    "train_names = train_data.columns.values.tolist()\n",
    "test_names = test.columns.values.tolist()\n",
    "\n",
    "print data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mini Train Data: (5000, 110)\n",
      "Mini Train Labels: (5000L,)\n",
      "Mini Train Unique Labels: 36\n",
      "\n",
      "Regular Train Data: (433991, 110)\n",
      "Regular Train Labels: (433991L,)\n",
      "Regular Train Unique Labels: 39\n",
      "\n",
      "Dev Data: (438991, 110)\n",
      "Dev Labels: (438991L,)\n",
      "Dev Train Unique Labels: 39\n",
      "\n",
      "Test Data: (884262, 111)\n",
      "\n",
      "Columns in use: ['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'DateTime', 'Year', 'Month', 'Day', 'Hour', 'SecondsDelta', 'Weekend', 2003L, 2004L, 2005L, 2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L, 2014L, 2015L, 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, 'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and apply this\n",
    "# permutation to features.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "\n",
    "shuffle = np.random.permutation(np.arange(train_data.shape[0]))\n",
    "train_data = train_data.reindex(shuffle)\n",
    "labels = labels.reindex(shuffle)\n",
    "\n",
    "# Remove records where Y == 90\n",
    "train_data_clean = train_data[train_data.Y != 90]\n",
    "labels_clean = labels[train_data.Y != 90]\n",
    "num_examples = train_data_clean.shape[0]\n",
    "\n",
    "# Split the feature and label sets into train and dev sets\n",
    "mini_train_data = train_data_clean[:5000]\n",
    "mini_train_labels = labels_clean[:5000]\n",
    "\n",
    "reg_train_data = train_data_clean[5000:num_examples/2]\n",
    "reg_train_labels = labels_clean[5000:num_examples/2]\n",
    "\n",
    "dev_data = train_data_clean[num_examples/2:]\n",
    "dev_labels = labels_clean[num_examples/2:]\n",
    "\n",
    "test_data = test.copy()\n",
    "\n",
    "print \"Mini Train Data:\", mini_train_data.shape\n",
    "print \"Mini Train Labels:\", mini_train_labels.shape\n",
    "print \"Mini Train Unique Labels:\", len(mini_train_labels.unique())\n",
    "print \"\\nRegular Train Data:\", reg_train_data.shape\n",
    "print \"Regular Train Labels:\", reg_train_labels.shape\n",
    "print \"Regular Train Unique Labels:\", len(reg_train_labels.unique())\n",
    "print \"\\nDev Data:\", dev_data.shape\n",
    "print \"Dev Labels:\", dev_labels.shape\n",
    "print \"Dev Train Unique Labels:\", len(dev_labels.unique())\n",
    "print \"\\nTest Data:\", test_data.shape\n",
    "print \"\\nColumns in use:\", train_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Some of the category labels have very few examples. When randomizing we need to make sure that dev and regular train data both contain 39 unique category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 99\n"
     ]
    }
   ],
   "source": [
    "features_to_use = [2003L, 2004L, 2005L, 2006L, 2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L, 2014L, 2015L, \n",
    "                   'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', \n",
    "                   1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, \n",
    "                   16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 30L, 31L, \n",
    "                   'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', \n",
    "                   '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', \n",
    "                   '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', \n",
    "                   'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', \n",
    "                   'SOUTHERN', 'TARAVAL', 'TENDERLOIN', 'X', 'Y'\n",
    "                   ]\n",
    "\n",
    "print \"Number of features:\", len(features_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Bernoulli NB Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model used Bernoulli Naive Bayes, which scored about the same as logistic regression.\n",
    "\n",
    "We tried using several variations of the Bernoulli NB model. After optimizing alpha, we tried bagging to determine if we could get a more robust model. We also tried using PCA to reduce dimensions and using those new features in the Bernoulli NB model. However, neither of these made any improvements to the original Bernoulli NB model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter:\n",
      "{'alpha': 1.0}\n",
      "\n",
      "Best score:\n",
      "2.56622149659\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEPCAYAAACk43iMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJGBmVmFOAmZmFeYkYGZWYU4CZmYV9v/J0bnBzQ8/PAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fd97f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the optimal alpha is for a Bernoulli NB model\n",
    "\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 500.0, 1000.0]}\n",
    "\n",
    "clf = GridSearchCV(BernoulliNB(), [params], scoring='log_loss')\n",
    "clf.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "\n",
    "print \"Best parameter:\\n\", clf.best_params_\n",
    "print \"\\nBest score:\\n\", -1 * clf.best_score_\n",
    "\n",
    "best_alpha = clf.best_params_['alpha']\n",
    "\n",
    "scores = [clf.grid_scores_[i][1] * -1 for i in range(len(clf.grid_scores_))]\n",
    "\n",
    "plt.plot(np.log10(params['alpha']), scores, marker='o')\n",
    "plt.xlabel(\"Log(Alpha)\")\n",
    "plt.ylabel(\"Log-Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we throw in 99 features (basically every feature we created), the log-loss is optimized when alpha = 1. However, the practical difference in log-loss up to alpha = 1 is very small. With alpha > 10, log-loss increases sharply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "\n",
      "model           alpha     accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "baseline        1.0000    0.2277    2.5656    \n",
      "optimized alpha 1.0000    0.2277    2.5656    \n",
      "bagged          1.0000    0.2258    2.5674    \n",
      "pca (70 comp)   1.0000    0.1920    2.7547    \n"
     ]
    }
   ],
   "source": [
    "print \"Bernoulli NB\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"model\", \"alpha\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Baseline Bernoulli NB model\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "base_alpha = 1.0\n",
    "base_accuracy = bnb.score(dev_data[features_to_use], dev_labels)\n",
    "base_logloss = log_loss(dev_labels, bnb.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"baseline\", base_alpha, base_accuracy, base_logloss)\n",
    "\n",
    "# How well does the Bernoulli NB model do with the selected alpha?\n",
    "bnb.set_params(alpha=best_alpha)\n",
    "bnb.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "bnb_probs = bnb.predict_proba(test_data[features_to_use])\n",
    "accuracy = bnb.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, bnb.predict_proba(dev_data[features_to_use]))\n",
    "print result_form(\"optimized alpha\", best_alpha, accuracy, logloss)\n",
    "\n",
    "# Maybe bagging the Bernoulli NB model does us some good\n",
    "bnb_bag = BaggingClassifier(BernoulliNB(alpha=best_alpha), max_features=0.8, max_samples = 0.8)\n",
    "bnb_bag.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "bnb_bag_probs = bnb_bag.predict_proba(test_data[features_to_use])\n",
    "accuracy = bnb_bag.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, bnb_bag.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"bagged\", best_alpha, accuracy, logloss)\n",
    "\n",
    "# Perhaps doing some dimensionality reduction will help us\n",
    "pca = PCA(n_components = 70)\n",
    "pca_train_data = pca.fit_transform(reg_train_data[features_to_use])\n",
    "pca_dev_data = pca.transform(dev_data[features_to_use])\n",
    "pca_test_data = pca.transform(test_data[features_to_use])\n",
    "\n",
    "bnb2 = BernoulliNB(best_alpha)\n",
    "bnb2.fit(pca_train_data, reg_train_labels)\n",
    "bnb2_probs = bnb2.predict_proba(pca_test_data)\n",
    "accuracy = bnb2.score(pca_dev_data, dev_labels)\n",
    "logloss = log_loss(dev_labels, bnb2.predict_proba(pca_dev_data)) \n",
    "print result_form(\"pca (70 comp)\", best_alpha, accuracy, logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tried gradient boosting, which improved our score to 2.49. Given the computational intensity of gradient boosting (initial model took 6 hours to run), we were unable to do further parameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "\n",
      "model           param     accuracy  log-loss  \n",
      "---------------------------------------------\n",
      "baseline                  0.2547    2.5018    \n"
     ]
    }
   ],
   "source": [
    "print \"Gradient Boosting\\n\"\n",
    "print \"{:<16s}{:<10s}{:<10s}{:<10s}\".format(\"model\", \"param\", \"accuracy\", \"log-loss\")\n",
    "print \"---------------------------------------------\"\n",
    "\n",
    "# Try Gradient Boosting\n",
    "grad_boost = GradientBoostingClassifier()\n",
    "grad_boost.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "accuracy = grad_boost.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, grad_boost.predict_proba(dev_data[features_to_use])) \n",
    "print \"{:<16s}{:<10s}{:<10.4f}{:<10.4f}\".format(\"baseline\", \"\", accuracy, logloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we used a neural net, which got our best score of 2.43. This was enough to enter the top 50 in the SF Learns competition (as of August 17th).\n",
    "\n",
    "We ran the Neural Network using a combination of NoLearn, Lasange, and Theano. NoLearn and Lasange are both under development, so to replicate this code please follow the installation instructions on NoLearn, below. NoLearn is built on top of Lasange and Theano, and thus its requirements are the ones to follow\n",
    "https://github.com/dnouri/nolearn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried two differnet types of Neural Networks - a 1-hidden layer network, then a 2-hidden layer network with a dropout layer between. The 2-hidden layer network resulted in only minor improvements. While the NoLearn/Lasange/Theano pipeline allows Neural Nets to be trained on certain types of GPU, we were not using such a machine for our development. As such, each model run took roughly an hour and limited the amount of runs possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried many different features in the Neural Net model. Interestingly enough, the \"year\" and \"month\" dummy variables caused the model to diverge, leading to poor prediction. These were dropped from the final model in place of a few simple booleans flagging if a crime occured before 2008, before 2010, and before 2012.\n",
    "\n",
    "We also parsed the X-Y coorinates into a series of dummy variables by rounding each coordinate to the 3rd decimal place. An attempt was made to round to the 4th decimal place and generate more features, however the computers available to us were unable to do this task. If work continues, this could be attempted via the cloud such as on AWS.\n",
    "\n",
    "Note that NoLearn automatically splits the data into training and development sets (default of 80/20 split) each iteration, so we did not need to clean a development set for this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract new features here because it's easier in Pandas than NumPy\n",
    "data_orig = pd.read_csv(\"train.csv\")\n",
    "test_orig = pd.read_csv(\"test.csv\")\n",
    "\n",
    "def build_features(data):\n",
    "    data['DateTime'] = pd.to_datetime(data['Dates'])\n",
    "    date_vector = data['DateTime'].dt.date\n",
    "    data['DateDiff'] = (date_vector - date_vector.min()) / np.timedelta64(1, 'D')\n",
    "    data['Year'] = pd.DatetimeIndex(data['DateTime']).year\n",
    "    data['Month'] = pd.DatetimeIndex(data['DateTime']).month\n",
    "    data['Day'] = pd.DatetimeIndex(data['DateTime']).day\n",
    "    data['Hour'] = pd.DatetimeIndex(data['DateTime']).hour\n",
    "    data['SecondsDelta'] = (data.DateTime - pd.Timestamp('2013-01-01')) / np.timedelta64(1,'s')\n",
    "    data['Weekend'] = (data.DayOfWeek == \"Saturday\") | (data.DayOfWeek == \"Sunday\")\n",
    "    years = pd.get_dummies(data.Year)\n",
    "    years.columns = ['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "    months = pd.get_dummies(data.Month)\n",
    "    months.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    days = pd.get_dummies(data.Day)\n",
    "    days.columns = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "    daysofweek = pd.get_dummies(data.DayOfWeek)\n",
    "    hours = pd.get_dummies(data.Hour)\n",
    "    hours.columns = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM',\n",
    "                     '6AM', '7AM', '8AM', '9AM', '10AM', '11AM',\n",
    "                     '12PM', '1PM', '2PM', '3PM', '4PM', '5PM',\n",
    "                     '6PM', '7PM', '8PM', '9PM', '10PM', '11PM']\n",
    "    districts = pd.get_dummies(data.PdDistrict)\n",
    "    new_data = pd.concat([data, years, months, days, daysofweek, hours, districts], axis=1)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "data = build_features(data_orig)\n",
    "test = build_features(test_orig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 147)\n",
      "(878049, 107)\n",
      "\n",
      "(884262, 147)\n",
      "(884262, 107)\n"
     ]
    }
   ],
   "source": [
    "# Generate location-based dummies\n",
    "XR3 = data['X'].round(decimals=3).apply(str)\n",
    "YR3 = data['Y'].round(decimals=3).apply(str)\n",
    "data_XR3s = pd.get_dummies(XR3)\n",
    "data_YR3s = pd.get_dummies(YR3)    \n",
    "XR3 = test['X'].round(decimals=3).apply(str)\n",
    "YR3 = test['Y'].round(decimals=3).apply(str)\n",
    "test_XR3s = pd.get_dummies(XR3)\n",
    "test_YR3s = pd.get_dummies(YR3)    \n",
    "\n",
    "#Subset the test to only include features that exist in training set\n",
    "test_XR3s = test_XR3s[list(data_XR3s)]\n",
    "test_YR3s = test_YR3s[list(data_YR3s)]\n",
    "\n",
    "print data_XR3s.shape\n",
    "print data_YR3s.shape\n",
    "print\n",
    "print test_XR3s.shape\n",
    "print test_YR3s.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate labels\n",
    "train_labels = data.Category\n",
    "\n",
    "# Create integer labels\n",
    "panda_labels = pd.Categorical(data.Category).codes\n",
    "train_labels_int = np.array(panda_labels).astype(np.int32)\n",
    "\n",
    "# Drop Category, Descript and Resolution columns since they are not in the test set.\n",
    "# Drop non-numerics too - they are accounted for as dummy variables.\n",
    "train_data = data.drop(['Category', 'Descript', 'Resolution', 'DateTime', 'Dates', 'PdDistrict', 'Address', 'DayOfWeek'], axis=1)\n",
    "train_data.Weekend = train_data.Weekend * 1\n",
    "train_names = train_data.columns.values.tolist()\n",
    "\n",
    "test_data = test.drop(['DateTime', 'Dates', 'PdDistrict', 'Address', 'DayOfWeek'], axis=1)\n",
    "test_data.Weekend = test_data.Weekend * 1\n",
    "test_names = test_data.columns.values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We attempted many different combinations of features. Some worked better than others, and a few unexpectedly diverged. The features below were used in our final set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Jan','Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "np_train_data = np.array(pd.concat([train_data[features], (data.Year < 2006) * 1, (data.Year < 2008) * 1, (data.Year < 2010) * 1, data_XR3s, data_YR3s], axis=1))\n",
    "np_test_data = np.array(pd.concat([test_data[features], (test.Year < 2006) * 1, (test.Year < 2008) * 1, (test.Year < 2010) * 1, test_XR3s[list(data_XR3s)], test_YR3s[list(data_YR3s)]], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net with 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoLearn/Lasagne makes it easy to define a net in terms of layers. We tried a number of hidden units and learning rates before settling on these parameters. We stopped after 65 iterations (epochs) through network updating because we observed overfitting around this point (that is, our accuracy on the training dataset continued to improve but accuracy on the development dataset began to fall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The accuracy was: 0.293386815542\n",
    "# Stopped early @ 65 to prevent overfitting\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # automatically calculate the number of featrues\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=65,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net1.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net1.train_history_])\n",
    "plt.plot(train_loss, linewidth=3, label=\"train\")\n",
    "plt.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(1e-0, 1e1)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View accuracy for each classification type. Note that this accuracy is measured on all training data, and thus\n",
    "# is higher than what we would expect on test data. This metric still was useful when comparing feature sets and model\n",
    "# hyperparamaters.\n",
    "\n",
    "train_pred = net1.predict(np_train_data)\n",
    "print(classification_report(train_labels_int, train_pred))\n",
    "print 'The accuracy is:', accuracy_score(train_labels_int, train_pred)\n",
    "\n",
    "# Get probabilities for submission to Kaggle\n",
    "test_proba = net1.predict_proba(np_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Net with 2 Hidden Layers + 1 Dropout Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next tried adding an additional hidden layer, including a \"dropout layer\" to help prevent overfitting. The dropout layer randomly severs a certain percentage of the connections between the nodes.\n",
    "\n",
    "We iterated through many possible values and settled on the ones below. However, further iteration with a faster processing speed (via a GPU) would likely lead to improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net2 = NeuralNet(\n",
    "    layers=[  # more layers\n",
    "        ('input', layers.InputLayer),\n",
    "        ('dense0', layers.DenseLayer),\n",
    "        ('dropout', layers.DropoutLayer),\n",
    "        ('dense1', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer)\n",
    "    ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    dense0_num_units=500,  # number of units in hidden layer\n",
    "    dropout_p=0.5,  # randomly disconect a number of notes to help preven overfitting\n",
    "    dense1_num_units=500,  # number of units in hidden layer\n",
    "    output_num_units=39,  # 39 target values\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.02,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=40,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "net2.fit(np_train_data, train_labels_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net2.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net2.train_history_])\n",
    "plt.plot(train_loss, linewidth=3, label=\"train\")\n",
    "plt.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(1e-0, 1e1)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred = net1.predict(np_train_data)\n",
    "print(classification_report(train_labels_int, train_pred))\n",
    "print 'The accuracy is:', accuracy_score(train_labels_int, train_pred)\n",
    "\n",
    "# Get probabilities for submission to Kaggle\n",
    "test_proba = net1.predict_proba(np_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Appendix - Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted a decision tree and random forest for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model           accuracy  log-loss  \n",
      "-----------------------------------\n",
      "decision tree   0.2253    26.2909   \n",
      "random forest   0.2454    14.4204   \n"
     ]
    }
   ],
   "source": [
    "result_form = \"{:<16s}{:<10.4f}{:<10.4f}\".format\n",
    "\n",
    "print \"{:<16s}{:<10s}{:<10s}\".format(\"model\", \"accuracy\", \"log-loss\")\n",
    "print \"-----------------------------------\"\n",
    "\n",
    "# Let's try a decision tree\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "dt_probs = dt.predict_proba(test_data[features_to_use])\n",
    "accuracy = dt.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, dt.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"decision tree\", accuracy, logloss)\n",
    "\n",
    "# Let's try a random forest\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(reg_train_data[features_to_use], reg_train_labels)\n",
    "rf_probs = rf.predict_proba(test_data[features_to_use])\n",
    "accuracy = rf.score(dev_data[features_to_use], dev_labels)\n",
    "logloss = log_loss(dev_labels, rf.predict_proba(dev_data[features_to_use])) \n",
    "print result_form(\"random forest\", accuracy, logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
