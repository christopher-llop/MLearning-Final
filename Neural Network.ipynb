{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Neural Net libraries. \n",
    "# I installed the live version of lasagne, theano, and nolearn directly from git. Follow this instructions here:\n",
    "# https://github.com/dnouri/nolearn\n",
    "from lasagne import layers\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "# Function to print all pandas rows\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    pd.set_option('display.max_columns', len(x.columns))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "    pd.reset_option('display.max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/cjllop/Code/MIDS/MLearning/Final/Data/train.csv\")\n",
    "test = pd.read_csv(\"/Users/cjllop/Code/MIDS/MLearning/Final/Data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "(878049, 9)\n",
      "['Dates' 'Category' 'Descript' 'DayOfWeek' 'PdDistrict' 'Resolution'\n",
      " 'Address' 'X' 'Y']\n",
      "Test Data:\n",
      "(884262, 7)\n",
      "['Id' 'Dates' 'DayOfWeek' 'PdDistrict' 'Address' 'X' 'Y']\n"
     ]
    }
   ],
   "source": [
    "# Big Picture of Data\n",
    "print \"Train Data:\"\n",
    "print data.shape\n",
    "print data.columns.values\n",
    "print \"Test Data:\"\n",
    "print test.shape\n",
    "print test.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract new features here because it's easier in Pandas than NumPy\n",
    "def build_features(data):\n",
    "    data['DateTime'] = pd.to_datetime(data['Dates'])\n",
    "    date_vector = data['DateTime'].dt.date\n",
    "    data['DateDiff'] = (date_vector - date_vector.min()) / np.timedelta64(1, 'D')\n",
    "    data['Year'] = pd.DatetimeIndex(data['DateTime']).year\n",
    "    data['Month'] = pd.DatetimeIndex(data['DateTime']).month\n",
    "    data['Day'] = pd.DatetimeIndex(data['DateTime']).day\n",
    "    data['Hour'] = pd.DatetimeIndex(data['DateTime']).hour\n",
    "    data['SecondsDelta'] = (data.DateTime - pd.Timestamp('2013-01-01')) / np.timedelta64(1,'s')\n",
    "    data['Weekend'] = (data.DayOfWeek == \"Saturday\") | (data.DayOfWeek == \"Sunday\")\n",
    "    years = pd.get_dummies(data.Year)\n",
    "    years.columns = ['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']\n",
    "    months = pd.get_dummies(data.Month)\n",
    "    months.columns = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    days = pd.get_dummies(data.Day)\n",
    "    days.columns = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31']\n",
    "    daysofweek = pd.get_dummies(data.DayOfWeek)\n",
    "    hours = pd.get_dummies(data.Hour)\n",
    "    hours.columns = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM',\n",
    "                     '6AM', '7AM', '8AM', '9AM', '10AM', '11AM',\n",
    "                     '12PM', '1PM', '2PM', '3PM', '4PM', '5PM',\n",
    "                     '6PM', '7PM', '8PM', '9PM', '10PM', '11PM']\n",
    "    districts = pd.get_dummies(data.PdDistrict)\n",
    "    new_data = pd.concat([data, years, months, days, daysofweek, hours, districts], axis=1)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "data = build_features(data)\n",
    "test = build_features(test)\n",
    "\n",
    "#print data.columns.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 147)\n",
      "(878049, 107)\n",
      "\n",
      "(884262, 147)\n",
      "(884262, 107)\n"
     ]
    }
   ],
   "source": [
    "#Generate location dummies (try 3 and 4 decimals)\n",
    "XR3 = data['X'].round(decimals=3).apply(str)\n",
    "YR3 = data['Y'].round(decimals=3).apply(str)\n",
    "#XR4 = data['X'].round(decimals=4).apply(str)\n",
    "#YR4 = data['Y'].round(decimals=4).apply(str)\n",
    "data_XR3s = pd.get_dummies(XR3)\n",
    "data_YR3s = pd.get_dummies(YR3)    \n",
    "#data_XR4s = pd.get_dummies(XR4)\n",
    "#data_YR4s = pd.get_dummies(YR4)\n",
    "XR3 = test['X'].round(decimals=3).apply(str)\n",
    "YR3 = test['Y'].round(decimals=3).apply(str)\n",
    "#XR4 = test['X'].round(decimals=4).apply(str)\n",
    "#YR4 = test['Y'].round(decimals=4).apply(str)\n",
    "test_XR3s = pd.get_dummies(XR3)\n",
    "test_YR3s = pd.get_dummies(YR3)    \n",
    "#test_XR4s = pd.get_dummies(XR4)\n",
    "#test_YR4s = pd.get_dummies(YR4)    \n",
    "\n",
    "#Subset the test ot only include ones in train\n",
    "#overlapX = list(set(list(test_XR4s)) & set(list(data_XR4s)))\n",
    "#overlapY = list(set(list(test_YR4s)) & set(list(data_YR4s)))\n",
    "\n",
    "test_XR3s = test_XR3s[list(data_XR3s)]\n",
    "test_YR3s = test_YR3s[list(data_YR3s)]\n",
    "#test_XR4s = test_XR4s[overlapX]\n",
    "#test_YR4s = test_YR4s[overlapY]\n",
    "\n",
    "print data_XR3s.shape\n",
    "print data_YR3s.shape\n",
    "#print data_XR4s.shape\n",
    "#print data_YR4s.shape\n",
    "print\n",
    "print test_XR3s.shape\n",
    "print test_YR3s.shape\n",
    "#print test_XR4s.shape\n",
    "#print test_YR4s.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8b4c0ac11916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Separate labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create integer labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpanda_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Separate labels\n",
    "train_labels = data.Category\n",
    "\n",
    "# Create integer labels\n",
    "panda_labels = pd.Categorical(data.Category).codes\n",
    "train_labels_int = np.array(panda_labels).astype(np.int32)\n",
    "\n",
    "# Drop Category, Descript and Resolution columns since they are not in the test set.\n",
    "# Drop non-numerics too - they are accounted for as dummy variables.\n",
    "train_data = data.drop(['Category', 'Descript', 'Resolution', 'DateTime', 'Dates', 'PdDistrict', 'Address', 'DayOfWeek'], axis=1)\n",
    "train_data.Weekend = train_data.Weekend * 1\n",
    "train_names = train_data.columns.values.tolist()\n",
    "\n",
    "test_data = test.drop(['DateTime', 'Dates', 'PdDistrict', 'Address', 'DayOfWeek'], axis=1)\n",
    "test_data.Weekend = test_data.Weekend * 1\n",
    "test_names = test_data.columns.values.tolist()\n",
    "\n",
    "#print_full(train_data.head(5))\n",
    "#print_full(test_data.head(5))\n",
    "\n",
    "#print_full(pd.DataFrame(train_data.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO \n",
    "#data_address_dummies = pd.get_dummies(data.Address,sparse=True)\n",
    "#test_address_dummies = pd.get_dummies(test.Address,sparse=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO \n",
    "#print pd.concat([data, pd.get_dummies(data.PdDistrict, prefix=\"PdDistrict\")], axis=1)\n",
    "# Let's create dummy variables for any address with over 100 events.\n",
    "#print len(address_counts)\n",
    "#print len(address_counts[address_counts > 100])\n",
    "#address_list.columns.values = [\"Address\"]\n",
    "#print pd.DataFrame(address_list).columns.values\n",
    "#print data.columns.values\n",
    "#print address_list\n",
    "#top_address['temp'] = 1\n",
    "#bottom_address = pd.merge(data, top_address, how='left', on=['Address'])\n",
    "\n",
    "\n",
    "#address_counts = data[\"Address\"].value_counts()\n",
    "#address_list = pd.Series(address_counts[address_counts > 100].index).to_frame(\"Address\")\n",
    "#top_address = pd.merge(data, address_list, how='inner', on=['Address'])\n",
    "#print data.shape\n",
    "#print result.shape\n",
    "#print bottom_address.shape\n",
    "\n",
    "#address_counts[address_counts > 100].index.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lasagne works off of Theano instead of Numpy\n",
    "#print data.Category\n",
    "#train_data = np.array(data[['X','Y','Year','Month','Day','Hour','DayOfYear']].values)\n",
    "#train_data = np.array(data_features)\n",
    "\n",
    "#panda_labels = pd.Categorical(data.Category).labels\n",
    "#train_labels = np.array(panda_labels).astype(np.int32)\n",
    "#print train_labels\n",
    "#print train_data.groupby(level=0).first()\n",
    "#print train_data.index.get_duplicates()\n",
    "#print train_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create random dev sample so we can see how that accuracy compares to our Kaggle results\n",
    "# np.random.seed(100)\n",
    "\n",
    "# Pick 10% of rows for dev\n",
    "# rows = np.random.choice(data.index, size = len(data) / 10, replace = False)\n",
    "\n",
    "# dev = data.ix[rows]\n",
    "# train = data.drop(rows)\n",
    "\n",
    "# print train.shape\n",
    "# print dev.shape\n",
    "# print test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features = [\n",
    " 'X', 'Y', 'DateDiff', 'Year', 'Month', 'Day', 'Hour',\n",
    " 'SecondsDelta', 'Weekend', '2003', '2004', '2005', '2006', '2007', '2008', '2009',\n",
    " '2010', '2011', '2012', '2013', '2014', '2015', 'Jan', 'Feb', 'Mar', 'Apr', 'May',\n",
    " 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', '1', '2', '3', '4', '5', '6', '7', '8',\n",
    " '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',\n",
    " '24', '25', '26', '27', '28', '29', '30', '31', 'Friday', 'Monday', 'Saturday',\n",
    " 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM',\n",
    " '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM',\n",
    " '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL',\n",
    " 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL',\n",
    " 'TENDERLOIN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 299)\n",
      "(884262, 299)\n"
     ]
    }
   ],
   "source": [
    "# Convert to numpy\n",
    "#features = ['X', 'Y'] #???? | ???? | 2.69680 | 0.1871\n",
    "#features = ['X', 'Y', 'DateDiff'] #2.73877 | 2.69913 | 2.69680 | 0.199\n",
    "#features = ['X', 'Y', 'DateDiff', 'Year', 'Month', 'Day', 'Hour', 'Weekend'] #2.73866 | 2.69914 || 0.199\n",
    "#features = ['X', 'Y', 'Year', 'Month', 'Day', 'Hour', 'Weekend'] #2.73870\n",
    "#features = ['Year', 'Month', 'Day', 'Hour', 'Weekend'] #2.73867\n",
    "#features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday',] # 2.70496\n",
    "#2.64560 | 2.62009 | 0.174%\n",
    "#features = ['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "#2.71693|||0.20055\n",
    "#[(200,10) = 0.231704608741]\n",
    "#[(500,20) = 0.240513912094]\n",
    "#features = ['2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', 'Jan', 'Feb', 'Mar', 'Apr', 'May',  'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23',  '24', '25', '26', '27', '28', '29', '30', '31', 'Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM',  '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "#features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "features = ['Jan','Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "\n",
    "\n",
    "#np_train_data = np.array(train_data[features])\n",
    "#np_train_data = np.array(pd.concat([data_XRs, data_YRs], axis=1))\n",
    "#np_train_data = np.array(pd.concat([train_data[features], data_XRs, data_YRs], axis=1))\n",
    "# Need to drop any \"test\" coordinate features that are not in \"train\". We accept this as a limitation.\n",
    "\n",
    "np_train_data = np.array(pd.concat([train_data[features], (data.Year < 2006) * 1, (data.Year < 2008) * 1, (data.Year < 2010) * 1, data_XR3s, data_YR3s], axis=1))\n",
    "np_test_data = np.array(pd.concat([test_data[features], (test.Year < 2006) * 1, (test.Year < 2008) * 1, (test.Year < 2010) * 1, test_XR3s[list(data_XR3s)], test_YR3s[list(data_YR3s)]], axis=1))\n",
    "#np_train_data4 = np.array(pd.concat([train_data[features], (data.Year < 2006) * 1, (data.Year < 2008) * 1, (data.Year < 2010) * 1, data_XR4s, data_YR4s], axis=1))\n",
    "\n",
    "\n",
    "print np_train_data.shape\n",
    "print np_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 299)\n",
      "(878049,)\n",
      "# Neural Network with 169539 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      299\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.62687\u001b[0m       \u001b[32m2.62683\u001b[0m      1.00001      0.22451  32.22s\n",
      "      2       \u001b[36m2.52016\u001b[0m       \u001b[32m2.59829\u001b[0m      0.96993      0.22837  29.78s\n",
      "      3       \u001b[36m2.49604\u001b[0m       \u001b[32m2.58270\u001b[0m      0.96645      0.23156  30.13s\n",
      "      4       \u001b[36m2.48211\u001b[0m       \u001b[32m2.57427\u001b[0m      0.96420      0.23356  29.14s\n",
      "      5       \u001b[36m2.47273\u001b[0m       \u001b[32m2.56963\u001b[0m      0.96229      0.23478  27.44s\n",
      "      6       \u001b[36m2.46537\u001b[0m       \u001b[32m2.56693\u001b[0m      0.96044      0.23686  26.94s\n",
      "      7       \u001b[36m2.45904\u001b[0m       \u001b[32m2.56508\u001b[0m      0.95866      0.23838  27.37s\n",
      "      8       \u001b[36m2.45332\u001b[0m       \u001b[32m2.56359\u001b[0m      0.95699      0.24026  27.99s\n",
      "      9       \u001b[36m2.44803\u001b[0m       \u001b[32m2.56212\u001b[0m      0.95547      0.24148  27.87s\n",
      "     10       \u001b[36m2.44309\u001b[0m       \u001b[32m2.56051\u001b[0m      0.95414      0.24299  27.88s\n",
      "     11       \u001b[36m2.43848\u001b[0m       \u001b[32m2.55898\u001b[0m      0.95291      0.24453  27.48s\n",
      "     12       \u001b[36m2.43415\u001b[0m       \u001b[32m2.55714\u001b[0m      0.95190      0.24572  27.90s\n",
      "     13       \u001b[36m2.43007\u001b[0m       \u001b[32m2.55522\u001b[0m      0.95102      0.24708  30.29s\n",
      "     14       \u001b[36m2.42623\u001b[0m       \u001b[32m2.55341\u001b[0m      0.95019      0.24798  30.42s\n",
      "     15       \u001b[36m2.42257\u001b[0m       \u001b[32m2.55166\u001b[0m      0.94941      0.24942  27.72s\n",
      "     16       \u001b[36m2.41907\u001b[0m       \u001b[32m2.54986\u001b[0m      0.94871      0.25057  27.27s\n",
      "     17       \u001b[36m2.41573\u001b[0m       \u001b[32m2.54797\u001b[0m      0.94810      0.25143  28.56s\n",
      "     18       \u001b[36m2.41251\u001b[0m       \u001b[32m2.54615\u001b[0m      0.94752      0.25184  27.90s\n",
      "     19       \u001b[36m2.40941\u001b[0m       \u001b[32m2.54441\u001b[0m      0.94694      0.25259  27.91s\n",
      "     20       \u001b[36m2.40642\u001b[0m       \u001b[32m2.54279\u001b[0m      0.94637      0.25299  26.99s\n",
      "     21       \u001b[36m2.40353\u001b[0m       \u001b[32m2.54113\u001b[0m      0.94585      0.25355  27.61s\n",
      "     22       \u001b[36m2.40074\u001b[0m       \u001b[32m2.53975\u001b[0m      0.94527      0.25398  27.94s\n",
      "     23       \u001b[36m2.39803\u001b[0m       \u001b[32m2.53836\u001b[0m      0.94472      0.25458  27.20s\n",
      "     24       \u001b[36m2.39542\u001b[0m       \u001b[32m2.53698\u001b[0m      0.94420      0.25491  27.31s\n",
      "     25       \u001b[36m2.39288\u001b[0m       \u001b[32m2.53565\u001b[0m      0.94369      0.25513  27.68s\n",
      "     26       \u001b[36m2.39041\u001b[0m       \u001b[32m2.53436\u001b[0m      0.94320      0.25568  27.37s\n",
      "     27       \u001b[36m2.38800\u001b[0m       \u001b[32m2.53309\u001b[0m      0.94272      0.25621  27.49s\n",
      "     28       \u001b[36m2.38566\u001b[0m       \u001b[32m2.53200\u001b[0m      0.94221      0.25636  27.88s\n",
      "     29       \u001b[36m2.38338\u001b[0m       \u001b[32m2.53093\u001b[0m      0.94170      0.25669  27.03s\n",
      "     30       \u001b[36m2.38115\u001b[0m       \u001b[32m2.52998\u001b[0m      0.94117      0.25732  31.91s\n",
      "     31       \u001b[36m2.37898\u001b[0m       \u001b[32m2.52891\u001b[0m      0.94071      0.25782  26.90s\n",
      "     32       \u001b[36m2.37686\u001b[0m       \u001b[32m2.52808\u001b[0m      0.94018      0.25788  27.25s\n",
      "     33       \u001b[36m2.37477\u001b[0m       \u001b[32m2.52713\u001b[0m      0.93971      0.25798  27.37s\n",
      "     34       \u001b[36m2.37274\u001b[0m       \u001b[32m2.52636\u001b[0m      0.93919      0.25814  27.27s\n",
      "     35       \u001b[36m2.37073\u001b[0m       \u001b[32m2.52565\u001b[0m      0.93866      0.25859  27.27s\n",
      "     36       \u001b[36m2.36877\u001b[0m       \u001b[32m2.52502\u001b[0m      0.93812      0.25880  27.16s\n",
      "     37       \u001b[36m2.36685\u001b[0m       \u001b[32m2.52431\u001b[0m      0.93762      0.25903  27.12s\n",
      "     38       \u001b[36m2.36497\u001b[0m       \u001b[32m2.52354\u001b[0m      0.93716      0.25914  27.34s\n",
      "     39       \u001b[36m2.36312\u001b[0m       \u001b[32m2.52296\u001b[0m      0.93664      0.25934  28.13s\n",
      "     40       \u001b[36m2.36129\u001b[0m       \u001b[32m2.52219\u001b[0m      0.93621      0.25961  27.12s\n",
      "     41       \u001b[36m2.35950\u001b[0m       \u001b[32m2.52148\u001b[0m      0.93576      0.25979  27.15s\n",
      "     42       \u001b[36m2.35775\u001b[0m       \u001b[32m2.52087\u001b[0m      0.93529      0.25991  27.24s\n",
      "     43       \u001b[36m2.35602\u001b[0m       \u001b[32m2.52036\u001b[0m      0.93479      0.25996  27.82s\n",
      "     44       \u001b[36m2.35431\u001b[0m       \u001b[32m2.51975\u001b[0m      0.93434      0.26017  27.57s\n",
      "     45       \u001b[36m2.35264\u001b[0m       \u001b[32m2.51911\u001b[0m      0.93392      0.26036  27.14s\n",
      "     46       \u001b[36m2.35100\u001b[0m       \u001b[32m2.51861\u001b[0m      0.93345      0.26041  27.46s\n",
      "     47       \u001b[36m2.34937\u001b[0m       \u001b[32m2.51812\u001b[0m      0.93299      0.26051  27.36s\n",
      "     48       \u001b[36m2.34777\u001b[0m       \u001b[32m2.51776\u001b[0m      0.93248      0.26063  27.21s\n",
      "     49       \u001b[36m2.34621\u001b[0m       \u001b[32m2.51737\u001b[0m      0.93201      0.26080  27.22s\n",
      "     50       \u001b[36m2.34467\u001b[0m       \u001b[32m2.51704\u001b[0m      0.93152      0.26110  27.39s\n",
      "     51       \u001b[36m2.34315\u001b[0m       \u001b[32m2.51672\u001b[0m      0.93103      0.26127  27.28s\n",
      "     52       \u001b[36m2.34165\u001b[0m       \u001b[32m2.51648\u001b[0m      0.93053      0.26140  27.23s\n",
      "     53       \u001b[36m2.34017\u001b[0m       \u001b[32m2.51616\u001b[0m      0.93006      0.26143  27.48s\n",
      "     54       \u001b[36m2.33872\u001b[0m       \u001b[32m2.51602\u001b[0m      0.92953      0.26153  27.44s\n",
      "     55       \u001b[36m2.33729\u001b[0m       \u001b[32m2.51585\u001b[0m      0.92902      0.26173  28.13s\n",
      "     56       \u001b[36m2.33587\u001b[0m       \u001b[32m2.51570\u001b[0m      0.92851      0.26191  27.22s\n",
      "     57       \u001b[36m2.33447\u001b[0m       \u001b[32m2.51552\u001b[0m      0.92803      0.26190  27.31s\n",
      "     58       \u001b[36m2.33309\u001b[0m       \u001b[32m2.51533\u001b[0m      0.92755      0.26225  27.71s\n",
      "     59       \u001b[36m2.33173\u001b[0m       \u001b[32m2.51525\u001b[0m      0.92704      0.26246  27.14s\n",
      "     60       \u001b[36m2.33039\u001b[0m       \u001b[32m2.51516\u001b[0m      0.92654      0.26232  27.73s\n",
      "     61       \u001b[36m2.32906\u001b[0m       \u001b[32m2.51515\u001b[0m      0.92601      0.26252  27.53s\n",
      "     62       \u001b[36m2.32775\u001b[0m       \u001b[32m2.51513\u001b[0m      0.92550      0.26253  27.13s\n",
      "     63       \u001b[36m2.32644\u001b[0m       2.51514      0.92497      0.26262  27.80s\n",
      "     64       \u001b[36m2.32515\u001b[0m       \u001b[32m2.51509\u001b[0m      0.92448      0.26254  27.21s\n",
      "     65       \u001b[36m2.32387\u001b[0m       \u001b[32m2.51507\u001b[0m      0.92398      0.26251  27.33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc210>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc190>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 299),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=65, more_params={},\n",
       "     objective=<function objective at 0x10c1c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c0e5c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x11993ee18>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x11ac681b8>],\n",
       "     output_nonlinearity=<function softmax at 0x10b79f398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c1cc250>,\n",
       "     update=<function nesterov_momentum at 0x10c0f89b0>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "# The accuracy is: 0.293386815542\n",
    "# Should run again stopping early @ 65 for better results\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # automatically calculate the number of featrues\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=65,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAESCAYAAADnvkIDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118201d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = np.array([i[\"train_loss\"] for i in net1.train_history_])\n",
    "valid_loss = np.array([i[\"valid_loss\"] for i in net1.train_history_])\n",
    "plt.plot(train_loss, linewidth=3, label=\"train\")\n",
    "plt.plot(valid_loss, linewidth=3, label=\"valid\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.ylim(1e-0, 1e1)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1513\n",
      "          1       0.21      0.17      0.19     76876\n",
      "          2       0.00      0.00      0.00       406\n",
      "          3       0.00      0.00      0.00       289\n",
      "          4       0.24      0.06      0.10     36755\n",
      "          5       0.42      0.01      0.01      4320\n",
      "          6       0.00      0.00      0.00      2268\n",
      "          7       0.29      0.51      0.37     53971\n",
      "          8       0.00      0.00      0.00      4280\n",
      "          9       0.00      0.00      0.00      1166\n",
      "         10       0.00      0.00      0.00       256\n",
      "         11       0.00      0.00      0.00       491\n",
      "         12       0.18      0.04      0.06     10609\n",
      "         13       0.21      0.00      0.01     16679\n",
      "         14       0.00      0.00      0.00       146\n",
      "         15       0.00      0.00      0.00      2341\n",
      "         16       0.33      0.67      0.45    174900\n",
      "         17       0.34      0.02      0.03      1903\n",
      "         18       0.49      0.10      0.17      1225\n",
      "         19       0.60      0.34      0.43     25989\n",
      "         20       0.26      0.15      0.19     92304\n",
      "         21       0.24      0.41      0.30    126182\n",
      "         22       0.00      0.00      0.00        22\n",
      "         23       0.54      0.55      0.54      7484\n",
      "         24       0.00      0.00      0.00      3138\n",
      "         25       0.36      0.00      0.00     23000\n",
      "         26       0.49      0.19      0.28      1946\n",
      "         27       0.41      0.00      0.00      9985\n",
      "         28       0.00      0.00      0.00      4388\n",
      "         29       0.00      0.00      0.00       148\n",
      "         30       0.00      0.00      0.00      4540\n",
      "         31       0.00      0.00      0.00       508\n",
      "         32       0.00      0.00      0.00     31414\n",
      "         33       0.00      0.00      0.00         6\n",
      "         34       0.35      0.01      0.01      7326\n",
      "         35       0.39      0.01      0.02     44725\n",
      "         36       0.25      0.26      0.25     53781\n",
      "         37       0.20      0.01      0.03     42214\n",
      "         38       0.00      0.00      0.00      8555\n",
      "\n",
      "avg / total       0.27      0.29      0.23    878049\n",
      "\n",
      "The accuracy is: 0.290555538472\n"
     ]
    }
   ],
   "source": [
    "train_pred = net3.predict(np_train_data)\n",
    "print(classification_report(train_labels_int, train_pred))\n",
    "print 'The accuracy is:', accuracy_score(train_labels_int, train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_submission(preds):\n",
    "    labels = [\"Id\",\n",
    "                \"ARSON\",\n",
    "                \"ASSAULT\",\n",
    "                \"BAD CHECKS\",\n",
    "                \"BRIBERY\",\n",
    "                \"BURGLARY\",\n",
    "                \"DISORDERLY CONDUCT\",\n",
    "                \"DRIVING UNDER THE INFLUENCE\",\n",
    "                \"DRUG/NARCOTIC\",\n",
    "                \"DRUNKENNESS\",\n",
    "                \"EMBEZZLEMENT\",\n",
    "                \"EXTORTION\",\n",
    "                \"FAMILY OFFENSES\",\n",
    "                \"FORGERY/COUNTERFEITING\",\n",
    "                \"FRAUD\",\n",
    "                \"GAMBLING\",\n",
    "                \"KIDNAPPING\",\n",
    "                \"LARCENY/THEFT\",\n",
    "                \"LIQUOR LAWS\",\n",
    "                \"LOITERING\",\n",
    "                \"MISSING PERSON\",\n",
    "                \"NON-CRIMINAL\",\n",
    "                \"OTHER OFFENSES\",\n",
    "                \"PORNOGRAPHY/OBSCENE MAT\",\n",
    "                \"PROSTITUTION\",\n",
    "                \"RECOVERED VEHICLE\",\n",
    "                \"ROBBERY\",\n",
    "                \"RUNAWAY\",\n",
    "                \"SECONDARY CODES\",\n",
    "                \"SEX OFFENSES FORCIBLE\",\n",
    "                \"SEX OFFENSES NON FORCIBLE\",\n",
    "                \"STOLEN PROPERTY\",\n",
    "                \"SUICIDE\",\n",
    "                \"SUSPICIOUS OCC\",\n",
    "                \"TREA\",\n",
    "                \"TRESPASS\",\n",
    "                \"VANDALISM\",\n",
    "                \"VEHICLE THEFT\",\n",
    "                \"WARRANTS\",\n",
    "                \"WEAPON LAWS\"\n",
    "              ]\n",
    "    head_str = ','.join(labels)\n",
    "\n",
    "    num_cats = len(labels)\n",
    "    \n",
    "    # Make a dummy row to append to\n",
    "    ids = np.arange(preds.shape[0])[np.newaxis].transpose()\n",
    "    \n",
    "    results = np.column_stack((ids, preds))\n",
    "\n",
    "    # Write results to csv\n",
    "    str_fmt = \"%d\"\n",
    "    for i in range(0,39):\n",
    "        str_fmt += \",%f\"\n",
    "    np.savetxt('Sample_NN_Deep2.csv', results, fmt=str_fmt, delimiter=',', header=head_str, comments='')\n",
    "\n",
    "    #return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 299)\n",
      "(884262, 299)\n"
     ]
    }
   ],
   "source": [
    "# Now that we've done this, let's run the KNN on the full train, apply to the test, then format.\n",
    "#KNNmodel = KNeighborsClassifier(n_neighbors=1)\n",
    "#KNNmodel.fit(full_data, full_labels)\n",
    "#dev_predict = KNNmodel.predict_proba(test_data).astype(int)\n",
    "\n",
    "print np_train_data.shape\n",
    "print np_test_data.shape\n",
    "test_proba = net1.predict_proba(np_test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = create_submission(test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262, 40)\n",
      "%d %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f %f\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print results.shape\n",
    "#print results[0:2]\n",
    "print str_fmt\n",
    "print len(str_fmt)/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 420039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      500\n",
      "  2  dropout     500\n",
      "  3  dense1      500\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.49865\u001b[0m       \u001b[32m2.65078\u001b[0m      0.94261      0.21992  84.06s\n",
      "      2       \u001b[36m2.43557\u001b[0m       \u001b[32m2.60976\u001b[0m      0.93326      0.23235  82.84s\n",
      "      3       \u001b[36m2.41412\u001b[0m       \u001b[32m2.58342\u001b[0m      0.93447      0.23864  84.63s\n",
      "      4       \u001b[36m2.40060\u001b[0m       \u001b[32m2.56654\u001b[0m      0.93535      0.24160  87.63s\n",
      "      5       \u001b[36m2.39060\u001b[0m       \u001b[32m2.55583\u001b[0m      0.93535      0.24474  88.61s\n",
      "      6       \u001b[36m2.38290\u001b[0m       \u001b[32m2.53963\u001b[0m      0.93829      0.24722  86.20s\n",
      "      7       \u001b[36m2.37709\u001b[0m       \u001b[32m2.52743\u001b[0m      0.94051      0.24763  89.13s\n",
      "      8       \u001b[36m2.37068\u001b[0m       \u001b[32m2.52606\u001b[0m      0.93849      0.24882  94.16s\n",
      "      9       \u001b[36m2.36634\u001b[0m       \u001b[32m2.52168\u001b[0m      0.93840      0.24982  86.55s\n",
      "     10       \u001b[36m2.36211\u001b[0m       \u001b[32m2.51125\u001b[0m      0.94061      0.25159  91.08s\n",
      "     11       \u001b[36m2.35743\u001b[0m       \u001b[32m2.51017\u001b[0m      0.93915      0.25271  92.57s\n",
      "     12       \u001b[36m2.35406\u001b[0m       \u001b[32m2.50272\u001b[0m      0.94060      0.25327  84.62s\n",
      "     13       \u001b[36m2.35110\u001b[0m       \u001b[32m2.50259\u001b[0m      0.93947      0.25422  89.64s\n",
      "     14       \u001b[36m2.34759\u001b[0m       \u001b[32m2.49873\u001b[0m      0.93951      0.25564  83.83s\n",
      "     15       \u001b[36m2.34539\u001b[0m       \u001b[32m2.49592\u001b[0m      0.93969      0.25550  83.79s\n",
      "     16       \u001b[36m2.34219\u001b[0m       \u001b[32m2.48980\u001b[0m      0.94072      0.25840  83.70s\n",
      "     17       \u001b[36m2.34009\u001b[0m       \u001b[32m2.48825\u001b[0m      0.94046      0.25740  83.96s\n",
      "     18       \u001b[36m2.33816\u001b[0m       \u001b[32m2.48686\u001b[0m      0.94021      0.25754  83.88s\n",
      "     19       \u001b[36m2.33539\u001b[0m       2.48735      0.93891      0.25856  84.09s\n",
      "     20       \u001b[36m2.33305\u001b[0m       \u001b[32m2.48337\u001b[0m      0.93947      0.25931  83.94s\n",
      "     21       \u001b[36m2.33141\u001b[0m       2.48512      0.93815      0.25794  84.09s\n",
      "     22       \u001b[36m2.32927\u001b[0m       \u001b[32m2.48330\u001b[0m      0.93797      0.25910  83.91s\n",
      "     23       \u001b[36m2.32715\u001b[0m       2.48415      0.93680      0.26042  84.00s\n",
      "     24       \u001b[36m2.32575\u001b[0m       \u001b[32m2.48213\u001b[0m      0.93700      0.26098  84.06s\n",
      "     25       \u001b[36m2.32425\u001b[0m       \u001b[32m2.48123\u001b[0m      0.93673      0.25980  84.13s\n",
      "     26       \u001b[36m2.32198\u001b[0m       \u001b[32m2.48076\u001b[0m      0.93600      0.26050  84.01s\n",
      "     27       \u001b[36m2.31980\u001b[0m       \u001b[32m2.47763\u001b[0m      0.93630      0.26115  84.02s\n",
      "     28       \u001b[36m2.31848\u001b[0m       \u001b[32m2.47733\u001b[0m      0.93588      0.26070  83.99s\n",
      "     29       \u001b[36m2.31693\u001b[0m       \u001b[32m2.47547\u001b[0m      0.93596      0.26174  83.84s\n",
      "     30       \u001b[36m2.31509\u001b[0m       2.47667      0.93476      0.26096  84.75s\n",
      "     31       \u001b[36m2.31387\u001b[0m       2.47580      0.93460      0.26106  84.33s\n",
      "     32       \u001b[36m2.31260\u001b[0m       2.47696      0.93365      0.26084  84.15s\n",
      "     33       \u001b[36m2.31065\u001b[0m       \u001b[32m2.47453\u001b[0m      0.93377      0.26214  84.20s\n",
      "     34       \u001b[36m2.31015\u001b[0m       \u001b[32m2.47439\u001b[0m      0.93363      0.26238  84.28s\n",
      "     35       \u001b[36m2.30806\u001b[0m       \u001b[32m2.47425\u001b[0m      0.93283      0.26244  84.28s\n",
      "     36       \u001b[36m2.30687\u001b[0m       2.47545      0.93190      0.26261  84.36s\n",
      "     37       \u001b[36m2.30577\u001b[0m       2.47474      0.93172      0.26346  84.36s\n",
      "     38       \u001b[36m2.30481\u001b[0m       2.47552      0.93104      0.26217  84.30s\n",
      "     39       \u001b[36m2.30432\u001b[0m       2.47515      0.93098      0.26244  84.09s\n",
      "     40       \u001b[36m2.30276\u001b[0m       \u001b[32m2.47151\u001b[0m      0.93172      0.26342  84.06s\n",
      "     41       \u001b[36m2.30081\u001b[0m       2.47581      0.92932      0.26109  84.01s\n",
      "     42       \u001b[36m2.29992\u001b[0m       2.47430      0.92953      0.26239  83.95s\n",
      "     43       \u001b[36m2.29922\u001b[0m       2.47376      0.92944      0.26280  84.24s\n",
      "     44       \u001b[36m2.29802\u001b[0m       2.47463      0.92863      0.26177  84.12s\n",
      "     45       \u001b[36m2.29680\u001b[0m       2.47456      0.92817      0.26383  84.17s\n",
      "     46       \u001b[36m2.29587\u001b[0m       2.47381      0.92807      0.26367  84.22s\n",
      "     47       \u001b[36m2.29504\u001b[0m       2.47528      0.92719      0.26274  84.55s\n",
      "     48       \u001b[36m2.29478\u001b[0m       2.47336      0.92780      0.26239  83.09s\n",
      "     49       \u001b[36m2.29275\u001b[0m       2.47405      0.92672      0.26175  83.06s\n",
      "     50       \u001b[36m2.29184\u001b[0m       2.47358      0.92653      0.26280  83.00s\n",
      "     51       \u001b[36m2.29157\u001b[0m       2.47539      0.92574      0.26202  83.00s\n",
      "     52       \u001b[36m2.29057\u001b[0m       2.47630      0.92500      0.26087  82.99s\n",
      "     53       \u001b[36m2.28855\u001b[0m       2.47526      0.92457      0.26263  83.05s\n",
      "     54       \u001b[36m2.28848\u001b[0m       2.47446      0.92484      0.26260  82.98s\n",
      "     55       \u001b[36m2.28813\u001b[0m       2.47787      0.92343      0.26070  83.20s\n",
      "     56       \u001b[36m2.28720\u001b[0m       2.47429      0.92438      0.26115  83.20s\n",
      "     57       \u001b[36m2.28662\u001b[0m       2.47651      0.92332      0.26103  83.12s\n",
      "     58       \u001b[36m2.28527\u001b[0m       2.47774      0.92232      0.26052  83.06s\n",
      "     59       2.28533       2.47618      0.92292      0.26077  83.08s\n",
      "     60       \u001b[36m2.28371\u001b[0m       2.47673      0.92206      0.26218  83.10s\n",
      "     61       \u001b[36m2.28248\u001b[0m       2.47541      0.92206      0.26287  83.00s\n",
      "     62       2.28275       2.47905      0.92081      0.26181  83.00s\n",
      "     63       \u001b[36m2.28210\u001b[0m       2.47642      0.92153      0.26222  82.94s\n",
      "     64       \u001b[36m2.28059\u001b[0m       2.47918      0.91990      0.26102  82.90s\n",
      "     65       2.28079       2.47775      0.92051      0.26084  82.93s\n",
      "     66       \u001b[36m2.28001\u001b[0m       2.47723      0.92039      0.26210  82.91s\n",
      "     67       \u001b[36m2.27905\u001b[0m       2.47769      0.91983      0.26258  83.08s\n",
      "     68       \u001b[36m2.27831\u001b[0m       2.47864      0.91917      0.26190  83.03s\n",
      "     69       \u001b[36m2.27782\u001b[0m       2.47883      0.91891      0.26182  83.11s\n",
      "     70       \u001b[36m2.27778\u001b[0m       2.47961      0.91860      0.26069  83.14s\n",
      "     71       \u001b[36m2.27672\u001b[0m       2.47724      0.91905      0.26205  83.18s\n",
      "     72       \u001b[36m2.27602\u001b[0m       2.47644      0.91907      0.26177  83.74s\n",
      "     73       \u001b[36m2.27594\u001b[0m       2.47643      0.91904      0.26166  83.10s\n",
      "     74       \u001b[36m2.27529\u001b[0m       2.47822      0.91811      0.26182  83.06s\n",
      "     75       \u001b[36m2.27402\u001b[0m       2.47979      0.91702      0.26179  83.03s\n",
      "     76       2.27414       2.47692      0.91813      0.26202  83.02s\n",
      "     77       \u001b[36m2.27288\u001b[0m       2.47713      0.91754      0.26136  83.04s\n",
      "     78       \u001b[36m2.27208\u001b[0m       2.47787      0.91695      0.26174  83.09s\n",
      "     79       \u001b[36m2.27074\u001b[0m       2.47878      0.91607      0.26078  83.16s\n",
      "     80       2.27145       2.48054      0.91571      0.26156  83.21s\n",
      "     81       \u001b[36m2.27072\u001b[0m       2.48019      0.91554      0.26146  83.54s\n",
      "     82       2.27094       2.47938      0.91593      0.26134  83.17s\n",
      "     83       \u001b[36m2.27006\u001b[0m       2.47749      0.91627      0.26222  83.80s\n",
      "     84       \u001b[36m2.26974\u001b[0m       2.47926      0.91549      0.26155  83.37s\n",
      "     85       \u001b[36m2.26902\u001b[0m       2.48046      0.91476      0.26137  83.14s\n",
      "     86       2.26902       2.48009      0.91490      0.26091  83.13s\n",
      "     87       \u001b[36m2.26846\u001b[0m       2.48107      0.91431      0.26137  83.17s\n",
      "     88       \u001b[36m2.26745\u001b[0m       2.47955      0.91446      0.26238  83.15s\n",
      "     89       \u001b[36m2.26646\u001b[0m       2.47984      0.91396      0.26179  83.30s\n",
      "     90       2.26694       2.48057      0.91388      0.26103  83.28s\n",
      "     91       \u001b[36m2.26589\u001b[0m       2.48090      0.91333      0.26265  83.31s\n",
      "     92       \u001b[36m2.26510\u001b[0m       2.48197      0.91262      0.26197  83.22s\n",
      "     93       2.26518       2.48053      0.91318      0.26160  84.06s\n",
      "     94       \u001b[36m2.26477\u001b[0m       2.48417      0.91168      0.26096  83.08s\n",
      "     95       \u001b[36m2.26462\u001b[0m       2.48257      0.91221      0.26122  83.10s\n",
      "     96       \u001b[36m2.26363\u001b[0m       2.48008      0.91272      0.26180  83.10s\n",
      "     97       \u001b[36m2.26306\u001b[0m       2.48122      0.91208      0.26130  83.27s\n",
      "     98       \u001b[36m2.26289\u001b[0m       2.48246      0.91155      0.26059  83.41s\n",
      "     99       \u001b[36m2.26177\u001b[0m       2.48239      0.91113      0.26025  83.37s\n",
      "    100       2.26263       2.48421      0.91081      0.26026  83.34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc210>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc190>,\n",
       "     custom_score=None, dense0_num_units=500, dense1_num_units=500,\n",
       "     dropout_p=0.5, input_shape=(None, 299),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x10c1c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c0e5c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x11ac25200>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x11abf8830>],\n",
       "     output_nonlinearity=<function softmax at 0x10b79f398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c1cc250>,\n",
       "     update=<function nesterov_momentum at 0x10c0f89b0>,\n",
       "     update_learning_rate=0.02, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tried a more complicated net with layers... still doing the same thing.\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net2 = NeuralNet(\n",
    "    layers=[  # more layers\n",
    "        ('input', layers.InputLayer),\n",
    "        ('dense0', layers.DenseLayer),\n",
    "        ('dropout', layers.DropoutLayer),\n",
    "        ('dense1', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer)\n",
    "    ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    dense0_num_units=500,  # number of units in hidden layer\n",
    "    dropout_p=0.5,  # randomly disconect a number of notes to help preven overfitting\n",
    "    dense1_num_units=500,  # number of units in hidden layer\n",
    "    output_num_units=39,  # 39 target values\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.02,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "net2.fit(np_train_data, train_labels_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 420039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      500\n",
      "  2  dropout     500\n",
      "  3  dense1      500\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.49755\u001b[0m       \u001b[32m2.65711\u001b[0m      0.93995      0.22395  83.58s\n",
      "      2       \u001b[36m2.43409\u001b[0m       \u001b[32m2.60991\u001b[0m      0.93263      0.23222  83.18s\n",
      "      3       \u001b[36m2.41343\u001b[0m       \u001b[32m2.58174\u001b[0m      0.93480      0.23777  83.77s\n",
      "      4       \u001b[36m2.40078\u001b[0m       \u001b[32m2.56049\u001b[0m      0.93763      0.24299  84.11s\n",
      "      5       \u001b[36m2.39081\u001b[0m       \u001b[32m2.55195\u001b[0m      0.93686      0.24226  84.18s\n",
      "      6       \u001b[36m2.38352\u001b[0m       \u001b[32m2.53489\u001b[0m      0.94028      0.24644  84.45s\n",
      "      7       \u001b[36m2.37662\u001b[0m       \u001b[32m2.52921\u001b[0m      0.93967      0.24769  84.78s\n",
      "      8       \u001b[36m2.37154\u001b[0m       \u001b[32m2.52130\u001b[0m      0.94060      0.24904  84.80s\n",
      "      9       \u001b[36m2.36625\u001b[0m       \u001b[32m2.51491\u001b[0m      0.94089      0.24977  84.96s\n",
      "     10       \u001b[36m2.36218\u001b[0m       2.51760      0.93827      0.24953  85.10s\n",
      "     11       \u001b[36m2.35903\u001b[0m       \u001b[32m2.50526\u001b[0m      0.94163      0.25402  85.26s\n",
      "     12       \u001b[36m2.35540\u001b[0m       2.50639      0.93976      0.25271  85.26s\n",
      "     13       \u001b[36m2.35153\u001b[0m       \u001b[32m2.50234\u001b[0m      0.93973      0.25272  85.45s\n",
      "     14       \u001b[36m2.34896\u001b[0m       \u001b[32m2.49516\u001b[0m      0.94141      0.25530  85.17s\n",
      "     15       \u001b[36m2.34556\u001b[0m       \u001b[32m2.49359\u001b[0m      0.94063      0.25526  85.40s\n",
      "     16       \u001b[36m2.34273\u001b[0m       \u001b[32m2.49184\u001b[0m      0.94016      0.25578  85.29s\n",
      "     17       \u001b[36m2.34053\u001b[0m       \u001b[32m2.48799\u001b[0m      0.94073      0.25727  85.46s\n",
      "     18       \u001b[36m2.33788\u001b[0m       2.48954      0.93908      0.25723  85.46s\n",
      "     19       \u001b[36m2.33612\u001b[0m       \u001b[32m2.48769\u001b[0m      0.93907      0.25739  85.60s\n",
      "     20       \u001b[36m2.33401\u001b[0m       2.49151      0.93679      0.25671  85.30s\n",
      "     21       \u001b[36m2.33159\u001b[0m       2.48903      0.93674      0.25740  85.40s\n",
      "     22       \u001b[36m2.32979\u001b[0m       \u001b[32m2.48081\u001b[0m      0.93912      0.25959  85.74s\n",
      "     23       \u001b[36m2.32760\u001b[0m       2.48467      0.93678      0.25688  85.57s\n",
      "     24       \u001b[36m2.32614\u001b[0m       2.48479      0.93615      0.25782  85.61s\n",
      "     25       \u001b[36m2.32411\u001b[0m       \u001b[32m2.47965\u001b[0m      0.93727      0.26000  85.70s\n",
      "     26       \u001b[36m2.32245\u001b[0m       2.48067      0.93622      0.25795  85.56s\n",
      "     27       \u001b[36m2.32076\u001b[0m       \u001b[32m2.47941\u001b[0m      0.93601      0.25844  85.52s\n",
      "     28       \u001b[36m2.31878\u001b[0m       2.47963      0.93513      0.25945  85.58s\n",
      "     29       \u001b[36m2.31785\u001b[0m       2.47998      0.93462      0.25902  85.40s\n",
      "     30       \u001b[36m2.31554\u001b[0m       \u001b[32m2.47909\u001b[0m      0.93402      0.25970  85.40s\n",
      "     31       \u001b[36m2.31363\u001b[0m       \u001b[32m2.47713\u001b[0m      0.93399      0.25990  85.80s\n",
      "     32       \u001b[36m2.31265\u001b[0m       2.47912      0.93285      0.25879  85.75s\n",
      "     33       \u001b[36m2.31113\u001b[0m       2.47963      0.93204      0.25824  85.82s\n",
      "     34       \u001b[36m2.31021\u001b[0m       \u001b[32m2.47660\u001b[0m      0.93281      0.25965  85.55s\n",
      "     35       \u001b[36m2.30887\u001b[0m       2.47669      0.93224      0.25883  85.40s\n",
      "     36       \u001b[36m2.30821\u001b[0m       2.47765      0.93161      0.25856  85.54s\n",
      "     37       \u001b[36m2.30660\u001b[0m       2.47681      0.93128      0.25964  85.72s\n",
      "     38       \u001b[36m2.30500\u001b[0m       \u001b[32m2.47539\u001b[0m      0.93117      0.25900  85.67s\n",
      "     39       \u001b[36m2.30364\u001b[0m       \u001b[32m2.47521\u001b[0m      0.93069      0.25958  85.98s\n",
      "     40       \u001b[36m2.30330\u001b[0m       2.47591      0.93028      0.26023  85.65s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc210>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc190>,\n",
       "     custom_score=None, dense0_num_units=500, dense1_num_units=500,\n",
       "     dropout_p=0.5, input_shape=(None, 299),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=40, more_params={},\n",
       "     objective=<function objective at 0x10c1c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c0e5c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x127685fc8>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x12795d200>],\n",
       "     output_nonlinearity=<function softmax at 0x10b79f398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c1cc250>,\n",
       "     update=<function nesterov_momentum at 0x10c0f89b0>,\n",
       "     update_learning_rate=0.02, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tried a more complicated net with layers... still doing the same thing.\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net3 = NeuralNet(\n",
    "    layers=[  # more layers\n",
    "        ('input', layers.InputLayer),\n",
    "        ('dense0', layers.DenseLayer),\n",
    "        ('dropout', layers.DropoutLayer),\n",
    "        ('dense1', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer)\n",
    "    ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    dense0_num_units=500,  # number of units in hidden layer\n",
    "    dropout_p=0.5,  # randomly disconect a number of notes to help preven overfitting\n",
    "    dense1_num_units=500,  # number of units in hidden layer\n",
    "    output_num_units=39,  # 39 target values\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.02,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=40,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "net3.fit(np_train_data, train_labels_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 670539 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name        size\n",
      "---  --------  ------\n",
      "  0  input        299\n",
      "  1  dense0       500\n",
      "  2  dropout      500\n",
      "  3  dense1       500\n",
      "  4  dropout2     500\n",
      "  5  dense2       500\n",
      "  6  output        39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.51606\u001b[0m       \u001b[32m2.62653\u001b[0m      0.95794      0.22004  133.23s\n",
      "      2       \u001b[36m2.44882\u001b[0m       \u001b[32m2.59233\u001b[0m      0.94464      0.23085  130.09s\n",
      "      3       \u001b[36m2.42730\u001b[0m       \u001b[32m2.57093\u001b[0m      0.94413      0.23736  130.94s\n",
      "      4       \u001b[36m2.41481\u001b[0m       \u001b[32m2.56277\u001b[0m      0.94226      0.23933  131.46s\n",
      "      5       \u001b[36m2.40572\u001b[0m       \u001b[32m2.55340\u001b[0m      0.94216      0.23926  136.97s\n",
      "      6       \u001b[36m2.39905\u001b[0m       \u001b[32m2.53952\u001b[0m      0.94469      0.24458  134.14s\n",
      "      7       \u001b[36m2.39298\u001b[0m       \u001b[32m2.53399\u001b[0m      0.94435      0.24412  132.86s\n",
      "      8       \u001b[36m2.38778\u001b[0m       \u001b[32m2.52930\u001b[0m      0.94405      0.24502  132.63s\n",
      "      9       \u001b[36m2.38282\u001b[0m       \u001b[32m2.52683\u001b[0m      0.94301      0.24771  132.61s\n",
      "     10       \u001b[36m2.37920\u001b[0m       \u001b[32m2.52351\u001b[0m      0.94281      0.24957  132.49s\n",
      "     11       \u001b[36m2.37594\u001b[0m       \u001b[32m2.52099\u001b[0m      0.94246      0.24874  132.76s\n",
      "     12       \u001b[36m2.37269\u001b[0m       \u001b[32m2.51545\u001b[0m      0.94325      0.25031  133.07s\n",
      "     13       \u001b[36m2.36972\u001b[0m       \u001b[32m2.51430\u001b[0m      0.94250      0.24995  132.91s\n",
      "     14       \u001b[36m2.36715\u001b[0m       \u001b[32m2.51301\u001b[0m      0.94196      0.24955  133.20s\n",
      "     15       \u001b[36m2.36474\u001b[0m       \u001b[32m2.50967\u001b[0m      0.94225      0.25086  133.15s\n",
      "     16       \u001b[36m2.36253\u001b[0m       \u001b[32m2.50607\u001b[0m      0.94272      0.25166  133.36s\n",
      "     17       \u001b[36m2.36010\u001b[0m       \u001b[32m2.50477\u001b[0m      0.94224      0.25121  133.26s\n",
      "     18       \u001b[36m2.35838\u001b[0m       \u001b[32m2.50388\u001b[0m      0.94189      0.25287  133.14s\n",
      "     19       \u001b[36m2.35679\u001b[0m       2.50449      0.94103      0.25246  133.59s\n",
      "     20       \u001b[36m2.35439\u001b[0m       \u001b[32m2.50041\u001b[0m      0.94160      0.25319  133.53s\n",
      "     21       \u001b[36m2.35315\u001b[0m       \u001b[32m2.49814\u001b[0m      0.94196      0.25546  133.55s\n",
      "     22       \u001b[36m2.35146\u001b[0m       2.50049      0.94040      0.25360  133.38s\n",
      "     23       \u001b[36m2.34984\u001b[0m       2.50088      0.93961      0.25330  133.35s\n",
      "     24       \u001b[36m2.34891\u001b[0m       \u001b[32m2.49721\u001b[0m      0.94061      0.25476  133.60s\n",
      "     25       \u001b[36m2.34719\u001b[0m       \u001b[32m2.49385\u001b[0m      0.94119      0.25504  133.31s\n",
      "     26       \u001b[36m2.34564\u001b[0m       \u001b[32m2.49363\u001b[0m      0.94066      0.25649  133.23s\n",
      "     27       \u001b[36m2.34477\u001b[0m       2.49763      0.93879      0.25564  133.34s\n",
      "     28       \u001b[36m2.34359\u001b[0m       2.49373      0.93979      0.25691  133.55s\n",
      "     29       \u001b[36m2.34246\u001b[0m       2.49545      0.93869      0.25568  133.40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc210>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc190>,\n",
       "     custom_score=None, dense0_num_units=500, dense1_num_units=500,\n",
       "     dense2_num_units=500, dropout2_p=0.5, dropout_p=0.5,\n",
       "     input_shape=(None, 299),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('dense0', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense1', <class 'lasagne.layers.dense.DenseLayer'>), ('dropout2', <class 'lasagne.layers.noise.DropoutLayer'>), ('dense2', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=40, more_params={},\n",
       "     objective=<function objective at 0x10c1c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c0e5c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x12760c3f8>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x120cd9440>],\n",
       "     output_nonlinearity=<function softmax at 0x10b79f398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c1cc250>,\n",
       "     update=<function nesterov_momentum at 0x10c0f89b0>,\n",
       "     update_learning_rate=0.02, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tried a more complicated net with layers... still doing the same thing.\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net4 = NeuralNet(\n",
    "    layers=[  # more layers\n",
    "        ('input', layers.InputLayer),\n",
    "        ('dense0', layers.DenseLayer),\n",
    "        ('dropout', layers.DropoutLayer),\n",
    "        ('dense1', layers.DenseLayer),\n",
    "        ('dropout2', layers.DropoutLayer),\n",
    "        ('dense2', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer)\n",
    "    ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    dense0_num_units=500,  # number of units in hidden layer\n",
    "    dropout_p=0.5,  # Not sure what this does.....\n",
    "    dense1_num_units=500,  # number of units in hidden layer\n",
    "    dropout2_p=0.5,  # Not sure what this does.....\n",
    "    dense2_num_units=500,  # number of units in hidden layer\n",
    "    output_num_units=39,  # 39 target values\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.02,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=40,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "net4.fit(np_train_data, train_labels_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: 0 Net: 100\n",
      "# Neural Network with 44039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      100\n",
      "  2  dropout     100\n",
      "  3  dense1      100\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.52787\u001b[0m       \u001b[32m2.63137\u001b[0m      0.96067      0.22331  15.39s\n",
      "      2       \u001b[36m2.46498\u001b[0m       \u001b[32m2.60272\u001b[0m      0.94708      0.22908  15.11s\n",
      "      3       \u001b[36m2.44846\u001b[0m       \u001b[32m2.57903\u001b[0m      0.94937      0.23337  15.64s\n",
      "      4       \u001b[36m2.43978\u001b[0m       \u001b[32m2.57073\u001b[0m      0.94906      0.23660  15.20s\n",
      "      5       \u001b[36m2.43432\u001b[0m       \u001b[32m2.55846\u001b[0m      0.95148      0.23976  14.53s\n",
      "Loop: 1 Net: 200\n",
      "# Neural Network with 108039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      200\n",
      "  2  dropout     200\n",
      "  3  dense1      200\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.51268\u001b[0m       \u001b[32m2.65478\u001b[0m      0.94647      0.21847  26.76s\n",
      "      2       \u001b[36m2.45102\u001b[0m       \u001b[32m2.61114\u001b[0m      0.93868      0.22983  27.40s\n",
      "      3       \u001b[36m2.43178\u001b[0m       \u001b[32m2.58804\u001b[0m      0.93962      0.23662  26.97s\n",
      "      4       \u001b[36m2.42131\u001b[0m       \u001b[32m2.57149\u001b[0m      0.94160      0.24009  27.69s\n",
      "      5       \u001b[36m2.41362\u001b[0m       \u001b[32m2.55805\u001b[0m      0.94354      0.24176  26.34s\n",
      "Loop: 2 Net: 300\n",
      "# Neural Network with 192039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      300\n",
      "  2  dropout     300\n",
      "  3  dense1      300\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.50746\u001b[0m       \u001b[32m2.63737\u001b[0m      0.95074      0.22111  40.35s\n",
      "      2       \u001b[36m2.44303\u001b[0m       \u001b[32m2.60059\u001b[0m      0.93941      0.23154  42.20s\n",
      "      3       \u001b[36m2.42370\u001b[0m       \u001b[32m2.57771\u001b[0m      0.94025      0.23688  42.32s\n",
      "      4       \u001b[36m2.41134\u001b[0m       \u001b[32m2.56438\u001b[0m      0.94032      0.23977  42.14s\n",
      "      5       \u001b[36m2.40380\u001b[0m       \u001b[32m2.55218\u001b[0m      0.94186      0.24189  41.41s\n",
      "Loop: 3 Net: 400\n",
      "# Neural Network with 296039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      400\n",
      "  2  dropout     400\n",
      "  3  dense1      400\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.50129\u001b[0m       \u001b[32m2.62795\u001b[0m      0.95180      0.22358  57.11s\n",
      "      2       \u001b[36m2.43853\u001b[0m       \u001b[32m2.59406\u001b[0m      0.94004      0.23544  56.61s\n",
      "      3       \u001b[36m2.41799\u001b[0m       \u001b[32m2.57342\u001b[0m      0.93960      0.23933  56.82s\n",
      "      4       \u001b[36m2.40517\u001b[0m       \u001b[32m2.55981\u001b[0m      0.93959      0.24188  57.15s\n",
      "      5       \u001b[36m2.39578\u001b[0m       \u001b[32m2.55120\u001b[0m      0.93908      0.24617  57.33s\n",
      "Loop: 4 Net: 500\n",
      "# Neural Network with 420039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      500\n",
      "  2  dropout     500\n",
      "  3  dense1      500\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.49878\u001b[0m       \u001b[32m2.63777\u001b[0m      0.94731      0.22477  81.79s\n",
      "      2       \u001b[36m2.43524\u001b[0m       \u001b[32m2.61486\u001b[0m      0.93131      0.23069  81.40s\n",
      "      3       \u001b[36m2.41403\u001b[0m       \u001b[32m2.57792\u001b[0m      0.93643      0.23686  81.37s\n",
      "      4       \u001b[36m2.40082\u001b[0m       \u001b[32m2.56080\u001b[0m      0.93753      0.24140  81.82s\n",
      "      5       \u001b[36m2.39133\u001b[0m       \u001b[32m2.55259\u001b[0m      0.93682      0.24173  82.11s\n",
      "Loop: 5 Net: 600\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.49492\u001b[0m       \u001b[32m2.64419\u001b[0m      0.94355      0.22442  100.56s\n",
      "      2       \u001b[36m2.43184\u001b[0m       \u001b[32m2.60856\u001b[0m      0.93225      0.23432  100.60s\n",
      "      3       \u001b[36m2.41033\u001b[0m       \u001b[32m2.57891\u001b[0m      0.93463      0.24028  101.39s\n",
      "      4       \u001b[36m2.39653\u001b[0m       \u001b[32m2.55897\u001b[0m      0.93652      0.24297  101.88s\n",
      "      5       \u001b[36m2.38687\u001b[0m       \u001b[32m2.54347\u001b[0m      0.93843      0.24646  101.99s\n",
      "Loop: 6 Net: 700\n",
      "# Neural Network with 728039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      700\n",
      "  2  dropout     700\n",
      "  3  dense1      700\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.49341\u001b[0m       \u001b[32m2.65852\u001b[0m      0.93789      0.22339  127.76s\n",
      "      2       \u001b[36m2.42996\u001b[0m       \u001b[32m2.61620\u001b[0m      0.92881      0.23276  125.29s\n",
      "      3       \u001b[36m2.40835\u001b[0m       \u001b[32m2.58124\u001b[0m      0.93302      0.23913  126.64s\n",
      "      4       \u001b[36m2.39491\u001b[0m       \u001b[32m2.56411\u001b[0m      0.93401      0.24153  127.48s\n",
      "      5       \u001b[36m2.38427\u001b[0m       \u001b[32m2.54183\u001b[0m      0.93801      0.24558  127.67s\n",
      "Loop: 7 Net: 800\n",
      "# Neural Network with 912039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      800\n",
      "  2  dropout     800\n",
      "  3  dense1      800\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.49244\u001b[0m       \u001b[32m2.63391\u001b[0m      0.94629      0.22487  156.00s\n",
      "      2       \u001b[36m2.42870\u001b[0m       \u001b[32m2.60529\u001b[0m      0.93222      0.23234  154.03s\n",
      "      3       \u001b[36m2.40658\u001b[0m       \u001b[32m2.57791\u001b[0m      0.93354      0.23686  154.74s\n",
      "      4       \u001b[36m2.39282\u001b[0m       \u001b[32m2.55911\u001b[0m      0.93502      0.24227  155.61s\n",
      "      5       \u001b[36m2.38169\u001b[0m       \u001b[32m2.54648\u001b[0m      0.93529      0.24335  156.10s\n",
      "Loop: 8 Net: 900\n",
      "# Neural Network with 1116039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      900\n",
      "  2  dropout     900\n",
      "  3  dense1      900\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.49019\u001b[0m       \u001b[32m2.65981\u001b[0m      0.93623      0.22194  195.97s\n",
      "      2       \u001b[36m2.42676\u001b[0m       \u001b[32m2.61426\u001b[0m      0.92828      0.23402  194.10s\n",
      "      3       \u001b[36m2.40478\u001b[0m       \u001b[32m2.57968\u001b[0m      0.93220      0.24004  195.84s\n",
      "      4       \u001b[36m2.39077\u001b[0m       \u001b[32m2.56256\u001b[0m      0.93296      0.24292  196.80s\n",
      "      5       \u001b[36m2.37948\u001b[0m       \u001b[32m2.54758\u001b[0m      0.93401      0.24644  197.28s\n",
      "Loop: 9 Net: 1000\n",
      "# Neural Network with 1340039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0     1000\n",
      "  2  dropout    1000\n",
      "  3  dense1     1000\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.49013\u001b[0m       \u001b[32m2.63889\u001b[0m      0.94363      0.22320  232.05s\n",
      "      2       \u001b[36m2.42577\u001b[0m       \u001b[32m2.60681\u001b[0m      0.93055      0.23239  233.11s\n",
      "      3       \u001b[36m2.40352\u001b[0m       \u001b[32m2.57773\u001b[0m      0.93242      0.23997  233.74s\n",
      "      4       \u001b[36m2.38875\u001b[0m       \u001b[32m2.55806\u001b[0m      0.93381      0.24314  234.62s\n",
      "      5       \u001b[36m2.37799\u001b[0m       \u001b[32m2.54439\u001b[0m      0.93460      0.24393  235.05s\n"
     ]
    }
   ],
   "source": [
    "#### TEST WITH DIFFERENT NODE NUMBERS\n",
    "#### >600 seems mildly better\n",
    "\n",
    "# Tried a more complicated net with layers... still doing the same thing.\n",
    "num_features = np_train_data.shape[1]\n",
    "netlist = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "\n",
    "nets = {}\n",
    "for x, y in enumerate(netlist):\n",
    "    print \"Loop: \" + str(x) + \" Net: \" + str(y)\n",
    "    this_net = NeuralNet(\n",
    "        layers=[  # more layers\n",
    "            ('input', layers.InputLayer),\n",
    "            ('dense0', layers.DenseLayer),\n",
    "            ('dropout', layers.DropoutLayer),\n",
    "            ('dense1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer)\n",
    "        ],\n",
    "        # layer parameters:\n",
    "        input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "        dense0_num_units= y,  # number of units in hidden layer\n",
    "        dropout_p=0.5,  # Not sure what this does.....\n",
    "        dense1_num_units= y,  # number of units in hidden layer\n",
    "        output_num_units=39,  # 39 target values\n",
    "        output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "        # optimization method:\n",
    "        update=nesterov_momentum,\n",
    "        update_learning_rate=0.02,\n",
    "        update_momentum=0.9,\n",
    "\n",
    "        regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "        max_epochs=5,  # we want to train this many epochs\n",
    "        verbose=1,\n",
    "        )\n",
    "\n",
    "    this_net.fit(np_train_data, train_labels_int)    \n",
    "#print nets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: 0 Net: 0.01\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.47409\u001b[0m       \u001b[32m2.62851\u001b[0m      0.94125      0.22789  108.57s\n",
      "      2       \u001b[36m2.40458\u001b[0m       \u001b[32m2.59963\u001b[0m      0.92497      0.23680  790.10s\n",
      "      3       \u001b[36m2.37736\u001b[0m       \u001b[32m2.57643\u001b[0m      0.92273      0.23843  103.52s\n",
      "      4       \u001b[36m2.35790\u001b[0m       \u001b[32m2.56026\u001b[0m      0.92096      0.24065  111.53s\n",
      "      5       \u001b[36m2.34173\u001b[0m       \u001b[32m2.54856\u001b[0m      0.91884      0.24322  106.94s\n",
      "Loop: 1 Net: 0.1\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.47847\u001b[0m       \u001b[32m2.64969\u001b[0m      0.93538      0.23124  104.45s\n",
      "      2       \u001b[36m2.40926\u001b[0m       \u001b[32m2.61218\u001b[0m      0.92232      0.23566  103.26s\n",
      "      3       \u001b[36m2.38279\u001b[0m       \u001b[32m2.57760\u001b[0m      0.92442      0.24077  104.84s\n",
      "      4       \u001b[36m2.36467\u001b[0m       \u001b[32m2.56109\u001b[0m      0.92331      0.24261  104.24s\n",
      "      5       \u001b[36m2.34990\u001b[0m       \u001b[32m2.55193\u001b[0m      0.92083      0.24516  103.71s\n",
      "Loop: 2 Net: 0.2\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.48043\u001b[0m       \u001b[32m2.63955\u001b[0m      0.93972      0.22602  103.71s\n",
      "      2       \u001b[36m2.41357\u001b[0m       \u001b[32m2.60439\u001b[0m      0.92673      0.23741  103.31s\n",
      "      3       \u001b[36m2.38856\u001b[0m       \u001b[32m2.57489\u001b[0m      0.92764      0.24354  103.20s\n",
      "      4       \u001b[36m2.37201\u001b[0m       \u001b[32m2.55513\u001b[0m      0.92833      0.24735  103.50s\n",
      "      5       \u001b[36m2.35892\u001b[0m       \u001b[32m2.54602\u001b[0m      0.92651      0.24722  103.82s\n",
      "Loop: 3 Net: 0.3\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.48532\u001b[0m       \u001b[32m2.64275\u001b[0m      0.94043      0.22646  104.95s\n",
      "      2       \u001b[36m2.41984\u001b[0m       \u001b[32m2.59819\u001b[0m      0.93136      0.23806  104.00s\n",
      "      3       \u001b[36m2.39577\u001b[0m       \u001b[32m2.57260\u001b[0m      0.93126      0.24199  103.75s\n",
      "      4       \u001b[36m2.37962\u001b[0m       \u001b[32m2.55001\u001b[0m      0.93318      0.24600  104.12s\n",
      "      5       \u001b[36m2.36769\u001b[0m       \u001b[32m2.53757\u001b[0m      0.93306      0.24836  104.06s\n",
      "Loop: 4 Net: 0.4\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.48967\u001b[0m       \u001b[32m2.63660\u001b[0m      0.94427      0.22541  103.25s\n",
      "      2       \u001b[36m2.42469\u001b[0m       \u001b[32m2.60416\u001b[0m      0.93108      0.23387  103.02s\n",
      "      3       \u001b[36m2.40245\u001b[0m       \u001b[32m2.57898\u001b[0m      0.93155      0.23854  103.74s\n",
      "      4       \u001b[36m2.38792\u001b[0m       \u001b[32m2.56323\u001b[0m      0.93161      0.24345  104.37s\n",
      "      5       \u001b[36m2.37707\u001b[0m       \u001b[32m2.54683\u001b[0m      0.93335      0.24557  104.60s\n",
      "Loop: 5 Net: 0.5\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.49457\u001b[0m       \u001b[32m2.65188\u001b[0m      0.94068      0.22373  101.45s\n",
      "      2       \u001b[36m2.43258\u001b[0m       \u001b[32m2.60906\u001b[0m      0.93236      0.23461  102.24s\n",
      "      3       \u001b[36m2.41100\u001b[0m       \u001b[32m2.57738\u001b[0m      0.93544      0.23976  103.09s\n",
      "      4       \u001b[36m2.39734\u001b[0m       \u001b[32m2.55698\u001b[0m      0.93757      0.24459  104.25s\n",
      "      5       \u001b[36m2.38749\u001b[0m       \u001b[32m2.54716\u001b[0m      0.93731      0.24719  104.71s\n",
      "Loop: 6 Net: 0.6\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.50198\u001b[0m       \u001b[32m2.64672\u001b[0m      0.94531      0.22368  103.71s\n",
      "      2       \u001b[36m2.44052\u001b[0m       \u001b[32m2.60534\u001b[0m      0.93674      0.23184  103.80s\n",
      "      3       \u001b[36m2.42085\u001b[0m       \u001b[32m2.57697\u001b[0m      0.93941      0.23747  104.49s\n",
      "      4       \u001b[36m2.40865\u001b[0m       \u001b[32m2.55357\u001b[0m      0.94325      0.24288  105.21s\n",
      "      5       \u001b[36m2.39936\u001b[0m       \u001b[32m2.54283\u001b[0m      0.94358      0.24616  105.85s\n",
      "Loop: 7 Net: 0.7\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.51291\u001b[0m       \u001b[32m2.64608\u001b[0m      0.94967      0.21794  102.60s\n",
      "      2       \u001b[36m2.45232\u001b[0m       \u001b[32m2.61359\u001b[0m      0.93830      0.22721  103.08s\n",
      "      3       \u001b[36m2.43389\u001b[0m       \u001b[32m2.58251\u001b[0m      0.94245      0.23551  104.05s\n",
      "      4       \u001b[36m2.42232\u001b[0m       \u001b[32m2.56551\u001b[0m      0.94419      0.23869  105.04s\n",
      "      5       \u001b[36m2.41457\u001b[0m       \u001b[32m2.54919\u001b[0m      0.94719      0.24375  112.11s\n",
      "Loop: 8 Net: 0.8\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.52879\u001b[0m       \u001b[32m2.64874\u001b[0m      0.95471      0.21606  103.40s\n",
      "      2       \u001b[36m2.46977\u001b[0m       \u001b[32m2.62253\u001b[0m      0.94175      0.22377  104.15s\n",
      "      3       \u001b[36m2.45244\u001b[0m       \u001b[32m2.59062\u001b[0m      0.94666      0.23270  108.11s\n",
      "      4       \u001b[36m2.44205\u001b[0m       \u001b[32m2.57766\u001b[0m      0.94739      0.23546  106.94s\n",
      "      5       \u001b[36m2.43563\u001b[0m       \u001b[32m2.56363\u001b[0m      0.95007      0.23867  107.48s\n",
      "Loop: 9 Net: 0.9\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.55913\u001b[0m       \u001b[32m2.65352\u001b[0m      0.96443      0.21550  103.19s\n",
      "      2       \u001b[36m2.50547\u001b[0m       \u001b[32m2.62093\u001b[0m      0.95595      0.22590  104.50s\n",
      "      3       \u001b[36m2.48852\u001b[0m       \u001b[32m2.60254\u001b[0m      0.95619      0.23458  107.85s\n",
      "      4       \u001b[36m2.47852\u001b[0m       \u001b[32m2.58682\u001b[0m      0.95813      0.23674  110.29s\n",
      "      5       \u001b[36m2.47265\u001b[0m       \u001b[32m2.57924\u001b[0m      0.95868      0.23814  111.95s\n",
      "Loop: 10 Net: 0.99\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.64267\u001b[0m       \u001b[32m2.72466\u001b[0m      0.96991      0.14360  102.05s\n",
      "      2       \u001b[36m2.63210\u001b[0m       \u001b[32m2.71397\u001b[0m      0.96984      0.14360  134.59s\n",
      "      3       \u001b[36m2.63191\u001b[0m       \u001b[32m2.71232\u001b[0m      0.97035      0.14360  161.88s\n",
      "      4       \u001b[36m2.63118\u001b[0m       \u001b[32m2.71144\u001b[0m      0.97040      0.14360  167.97s\n",
      "      5       \u001b[36m2.62979\u001b[0m       \u001b[32m2.70985\u001b[0m      0.97046      0.14360  171.68s\n"
     ]
    }
   ],
   "source": [
    "#### TEST WITH DIFFERENT DROPOUT PERCENTAGES\n",
    "#### Use 600 which did pretty good above.\n",
    "#### 0.3 did the best\n",
    "\n",
    "# Tried a more complicated net with layers... still doing the same thing.\n",
    "num_features = np_train_data.shape[1]\n",
    "netlist = [.01, .1, .2, .3, .4, .5, .6, .7, .8, .9, .99]\n",
    "\n",
    "nets = {}\n",
    "for x, y in enumerate(netlist):\n",
    "    print \"Loop: \" + str(x) + \" Net: \" + str(y)\n",
    "    this_net = NeuralNet(\n",
    "        layers=[  # more layers\n",
    "            ('input', layers.InputLayer),\n",
    "            ('dense0', layers.DenseLayer),\n",
    "            ('dropout', layers.DropoutLayer),\n",
    "            ('dense1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer)\n",
    "        ],\n",
    "        # layer parameters:\n",
    "        input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "        dense0_num_units= 600,  # number of units in hidden layer\n",
    "        dropout_p=y,  # Not sure what this does.....\n",
    "        dense1_num_units= 600,  # number of units in hidden layer\n",
    "        output_num_units=39,  # 39 target values\n",
    "        output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "        # optimization method:\n",
    "        update=nesterov_momentum,\n",
    "        update_learning_rate=0.02,\n",
    "        update_momentum=0.9,\n",
    "\n",
    "        regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "        max_epochs=5,  # we want to train this many epochs\n",
    "        verbose=1,\n",
    "        )\n",
    "\n",
    "    this_net.fit(np_train_data, train_labels_int)    \n",
    "#print nets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: 0 Net: 10\n",
      "# Neural Network with 186439 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1       10\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.51448\u001b[0m       \u001b[32m2.68829\u001b[0m      0.93535      0.21206  45.85s\n",
      "      2       \u001b[36m2.44747\u001b[0m       \u001b[32m2.64082\u001b[0m      0.92678      0.22623  46.33s\n",
      "      3       \u001b[36m2.42075\u001b[0m       \u001b[32m2.59907\u001b[0m      0.93139      0.23785  47.36s\n",
      "      4       \u001b[36m2.40447\u001b[0m       \u001b[32m2.57744\u001b[0m      0.93289      0.24148  47.61s\n",
      "      5       \u001b[36m2.39243\u001b[0m       \u001b[32m2.56273\u001b[0m      0.93355      0.24474  48.04s\n",
      "Loop: 1 Net: 20\n",
      "# Neural Network with 192839 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1       20\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.49997\u001b[0m       \u001b[32m2.67263\u001b[0m      0.93540      0.22417  47.56s\n",
      "      2       \u001b[36m2.42742\u001b[0m       \u001b[32m2.62952\u001b[0m      0.92314      0.23387  49.06s\n",
      "      3       \u001b[36m2.40458\u001b[0m       \u001b[32m2.59001\u001b[0m      0.92841      0.23991  49.14s\n",
      "      4       \u001b[36m2.39005\u001b[0m       \u001b[32m2.57159\u001b[0m      0.92941      0.24442  49.72s\n",
      "      5       \u001b[36m2.37953\u001b[0m       \u001b[32m2.55190\u001b[0m      0.93246      0.24622  49.61s\n",
      "Loop: 2 Net: 50\n",
      "# Neural Network with 212039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1       50\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.49117\u001b[0m       \u001b[32m2.64191\u001b[0m      0.94294      0.22203  49.16s\n",
      "      2       \u001b[36m2.42477\u001b[0m       \u001b[32m2.60339\u001b[0m      0.93139      0.23235  48.98s\n",
      "      3       \u001b[36m2.40151\u001b[0m       \u001b[32m2.58068\u001b[0m      0.93057      0.23755  49.27s\n",
      "      4       \u001b[36m2.38672\u001b[0m       \u001b[32m2.56235\u001b[0m      0.93146      0.24275  49.78s\n",
      "      5       \u001b[36m2.37565\u001b[0m       \u001b[32m2.54786\u001b[0m      0.93241      0.24419  49.36s\n",
      "Loop: 3 Net: 100\n",
      "# Neural Network with 244039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      100\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.48382\u001b[0m       \u001b[32m2.64931\u001b[0m      0.93753      0.22355  54.83s\n",
      "      2       \u001b[36m2.42095\u001b[0m       \u001b[32m2.60595\u001b[0m      0.92901      0.23517  56.97s\n",
      "      3       \u001b[36m2.39766\u001b[0m       \u001b[32m2.58293\u001b[0m      0.92827      0.24036  56.15s\n",
      "      4       \u001b[36m2.38263\u001b[0m       \u001b[32m2.56047\u001b[0m      0.93054      0.24525  56.71s\n",
      "      5       \u001b[36m2.37137\u001b[0m       \u001b[32m2.54814\u001b[0m      0.93063      0.24541  56.84s\n",
      "Loop: 4 Net: 200\n",
      "# Neural Network with 308039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      200\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.48412\u001b[0m       \u001b[32m2.64544\u001b[0m      0.93902      0.22339  63.53s\n",
      "      2       \u001b[36m2.42033\u001b[0m       \u001b[32m2.61241\u001b[0m      0.92648      0.23541  63.35s\n",
      "      3       \u001b[36m2.39687\u001b[0m       \u001b[32m2.58357\u001b[0m      0.92773      0.23990  63.78s\n",
      "      4       \u001b[36m2.38174\u001b[0m       \u001b[32m2.56127\u001b[0m      0.92990      0.24281  64.25s\n",
      "      5       \u001b[36m2.36994\u001b[0m       \u001b[32m2.54651\u001b[0m      0.93066      0.24577  64.45s\n",
      "Loop: 5 Net: 300\n",
      "# Neural Network with 372039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      300\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.48439\u001b[0m       \u001b[32m2.63652\u001b[0m      0.94230      0.22707  74.49s\n",
      "      2       \u001b[36m2.41965\u001b[0m       \u001b[32m2.60412\u001b[0m      0.92916      0.23637  75.00s\n",
      "      3       \u001b[36m2.39636\u001b[0m       \u001b[32m2.58911\u001b[0m      0.92555      0.24047  74.75s\n",
      "      4       \u001b[36m2.38070\u001b[0m       \u001b[32m2.56069\u001b[0m      0.92971      0.24467  75.28s\n",
      "      5       \u001b[36m2.36880\u001b[0m       \u001b[32m2.55099\u001b[0m      0.92858      0.24507  75.51s\n",
      "Loop: 6 Net: 400\n",
      "# Neural Network with 436039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      400\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.48539\u001b[0m       \u001b[32m2.64388\u001b[0m      0.94006      0.22643  85.50s\n",
      "      2       \u001b[36m2.41972\u001b[0m       \u001b[32m2.61232\u001b[0m      0.92627      0.23658  84.91s\n",
      "      3       \u001b[36m2.39593\u001b[0m       \u001b[32m2.58856\u001b[0m      0.92558      0.24161  83.40s\n",
      "      4       \u001b[36m2.38083\u001b[0m       \u001b[32m2.56376\u001b[0m      0.92865      0.24450  83.76s\n",
      "      5       \u001b[36m2.36864\u001b[0m       \u001b[32m2.55054\u001b[0m      0.92868      0.24792  84.06s\n",
      "Loop: 7 Net: 500\n",
      "# Neural Network with 500039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      500\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.48588\u001b[0m       \u001b[32m2.64136\u001b[0m      0.94114      0.22558  91.94s\n",
      "      2       \u001b[36m2.41959\u001b[0m       \u001b[32m2.60315\u001b[0m      0.92949      0.23670  92.26s\n",
      "      3       \u001b[36m2.39578\u001b[0m       \u001b[32m2.57734\u001b[0m      0.92955      0.24146  94.83s\n",
      "      4       \u001b[36m2.38038\u001b[0m       \u001b[32m2.55411\u001b[0m      0.93198      0.24531  95.14s\n",
      "      5       \u001b[36m2.36857\u001b[0m       \u001b[32m2.54829\u001b[0m      0.92947      0.24603  94.61s\n",
      "Loop: 8 Net: 600\n",
      "# Neural Network with 564039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      600\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.48447\u001b[0m       \u001b[32m2.65131\u001b[0m      0.93707      0.22685  102.55s\n",
      "      2       \u001b[36m2.41890\u001b[0m       \u001b[32m2.61328\u001b[0m      0.92562      0.23629  102.55s\n",
      "      3       \u001b[36m2.39513\u001b[0m       \u001b[32m2.57412\u001b[0m      0.93046      0.24160  102.43s\n",
      "      4       \u001b[36m2.37978\u001b[0m       \u001b[32m2.54946\u001b[0m      0.93344      0.24614  103.03s\n",
      "      5       \u001b[36m2.36812\u001b[0m       \u001b[32m2.53912\u001b[0m      0.93265      0.24769  103.13s\n",
      "Loop: 9 Net: 700\n",
      "# Neural Network with 628039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      700\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.48332\u001b[0m       \u001b[32m2.63016\u001b[0m      0.94417      0.23135  135.39s\n",
      "      2       \u001b[36m2.41850\u001b[0m       \u001b[32m2.59650\u001b[0m      0.93145      0.23632  135.21s\n",
      "      3       \u001b[36m2.39474\u001b[0m       \u001b[32m2.57074\u001b[0m      0.93154      0.24198  135.78s\n",
      "      4       \u001b[36m2.37935\u001b[0m       \u001b[32m2.54992\u001b[0m      0.93311      0.24541  136.18s\n",
      "      5       \u001b[36m2.36748\u001b[0m       \u001b[32m2.53158\u001b[0m      0.93518      0.24873  136.34s\n",
      "Loop: 10 Net: 800\n",
      "# Neural Network with 692039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      800\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.48532\u001b[0m       \u001b[32m2.63340\u001b[0m      0.94377      0.23024  144.98s\n",
      "      2       \u001b[36m2.41899\u001b[0m       \u001b[32m2.60212\u001b[0m      0.92962      0.23756  154.04s\n",
      "      3       \u001b[36m2.39505\u001b[0m       \u001b[32m2.57279\u001b[0m      0.93091      0.24368  149.00s\n",
      "      4       \u001b[36m2.37986\u001b[0m       \u001b[32m2.55662\u001b[0m      0.93086      0.24706  148.14s\n",
      "      5       \u001b[36m2.36816\u001b[0m       \u001b[32m2.54212\u001b[0m      0.93157      0.25025  148.23s\n",
      "Loop: 11 Net: 900\n",
      "# Neural Network with 756039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name       size\n",
      "---  -------  ------\n",
      "  0  input       299\n",
      "  1  dense0      600\n",
      "  2  dropout     600\n",
      "  3  dense1      900\n",
      "  4  output       39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  -------\n",
      "      1       \u001b[36m2.48510\u001b[0m       \u001b[32m2.64053\u001b[0m      0.94114      0.22769  156.86s\n",
      "      2       \u001b[36m2.41856\u001b[0m       \u001b[32m2.60300\u001b[0m      0.92914      0.23744  158.01s\n",
      "      3       \u001b[36m2.39468\u001b[0m       \u001b[32m2.58037\u001b[0m      0.92804      0.24344  158.76s\n",
      "      4       \u001b[36m2.37904\u001b[0m       \u001b[32m2.55476\u001b[0m      0.93122      0.24655  159.30s\n",
      "      5       \u001b[36m2.36720\u001b[0m       \u001b[32m2.54642\u001b[0m      0.92962      0.24758  160.14s\n"
     ]
    }
   ],
   "source": [
    "#### TEST WITH DIFFERENT LAYER SIZEZ\n",
    "#### Use 600 which did pretty good above.\n",
    "#### Use 0.3 did the best\n",
    "#### Iterate the 2nd Layer 10,20,50, 100s\n",
    "\n",
    "# Tried a more complicated net with layers... still doing the same thing.\n",
    "num_features = np_train_data.shape[1]\n",
    "netlist = [10, 20, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
    "\n",
    "nets = {}\n",
    "for x, y in enumerate(netlist):\n",
    "    print \"Loop: \" + str(x) + \" Net: \" + str(y)\n",
    "    this_net = NeuralNet(\n",
    "        layers=[  # more layers\n",
    "            ('input', layers.InputLayer),\n",
    "            ('dense0', layers.DenseLayer),\n",
    "            ('dropout', layers.DropoutLayer),\n",
    "            ('dense1', layers.DenseLayer),\n",
    "            ('output', layers.DenseLayer)\n",
    "        ],\n",
    "        # layer parameters:\n",
    "        input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "        dense0_num_units= 600,  # number of units in hidden layer\n",
    "        dropout_p= 0.3,  # Not sure what this does.....\n",
    "        dense1_num_units= y,  # number of units in hidden layer\n",
    "        output_num_units=39,  # 39 target values\n",
    "        output_nonlinearity=nonlinearities.softmax,\n",
    "\n",
    "        # optimization method:\n",
    "        update=nesterov_momentum,\n",
    "        update_learning_rate=0.02,\n",
    "        update_momentum=0.9,\n",
    "\n",
    "        regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "        max_epochs=5,  # we want to train this many epochs\n",
    "        verbose=1,\n",
    "        )\n",
    "\n",
    "    this_net.fit(np_train_data, train_labels_int)    \n",
    "#print nets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "____\n",
    "\n",
    "Old code below here\n",
    "____\n",
    "____\n",
    "____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 254)\n",
      "(878049,)\n",
      "# Neural Network with 147039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      254\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.67321\u001b[0m       \u001b[32m2.66736\u001b[0m      1.00219      0.20197  24.90s\n",
      "      2       \u001b[36m2.56639\u001b[0m       \u001b[32m2.64605\u001b[0m      0.96990      0.20978  24.59s\n",
      "      3       \u001b[36m2.54176\u001b[0m       \u001b[32m2.63234\u001b[0m      0.96559      0.21508  24.00s\n",
      "      4       \u001b[36m2.52726\u001b[0m       \u001b[32m2.62335\u001b[0m      0.96337      0.21664  24.68s\n",
      "      5       \u001b[36m2.51784\u001b[0m       \u001b[32m2.61697\u001b[0m      0.96212      0.21816  24.39s\n",
      "      6       \u001b[36m2.51080\u001b[0m       \u001b[32m2.61196\u001b[0m      0.96127      0.22045  23.91s\n",
      "      7       \u001b[36m2.50493\u001b[0m       \u001b[32m2.60775\u001b[0m      0.96057      0.22372  23.92s\n",
      "      8       \u001b[36m2.49969\u001b[0m       \u001b[32m2.60389\u001b[0m      0.95998      0.22694  23.90s\n",
      "      9       \u001b[36m2.49486\u001b[0m       \u001b[32m2.60038\u001b[0m      0.95942      0.22709  24.10s\n",
      "     10       \u001b[36m2.49034\u001b[0m       \u001b[32m2.59712\u001b[0m      0.95889      0.22724  23.78s\n",
      "     11       \u001b[36m2.48610\u001b[0m       \u001b[32m2.59406\u001b[0m      0.95838      0.22909  23.70s\n",
      "     12       \u001b[36m2.48213\u001b[0m       \u001b[32m2.59122\u001b[0m      0.95790      0.23008  23.79s\n",
      "     13       \u001b[36m2.47841\u001b[0m       \u001b[32m2.58860\u001b[0m      0.95743      0.23017  24.26s\n",
      "     14       \u001b[36m2.47490\u001b[0m       \u001b[32m2.58617\u001b[0m      0.95698      0.23180  24.22s\n",
      "     15       \u001b[36m2.47159\u001b[0m       \u001b[32m2.58384\u001b[0m      0.95656      0.23283  23.58s\n",
      "     16       \u001b[36m2.46845\u001b[0m       \u001b[32m2.58163\u001b[0m      0.95616      0.23391  24.92s\n",
      "     17       \u001b[36m2.46545\u001b[0m       \u001b[32m2.57954\u001b[0m      0.95577      0.23471  23.68s\n",
      "     18       \u001b[36m2.46256\u001b[0m       \u001b[32m2.57753\u001b[0m      0.95539      0.23510  23.62s\n",
      "     19       \u001b[36m2.45978\u001b[0m       \u001b[32m2.57558\u001b[0m      0.95504      0.23541  23.58s\n",
      "     20       \u001b[36m2.45708\u001b[0m       \u001b[32m2.57364\u001b[0m      0.95471      0.23588  24.69s\n",
      "     21       \u001b[36m2.45444\u001b[0m       \u001b[32m2.57175\u001b[0m      0.95438      0.23620  23.78s\n",
      "     22       \u001b[36m2.45186\u001b[0m       \u001b[32m2.56992\u001b[0m      0.95406      0.23663  23.62s\n",
      "     23       \u001b[36m2.44935\u001b[0m       \u001b[32m2.56819\u001b[0m      0.95373      0.23777  25.07s\n",
      "     24       \u001b[36m2.44691\u001b[0m       \u001b[32m2.56647\u001b[0m      0.95341      0.23841  25.80s\n",
      "     25       \u001b[36m2.44452\u001b[0m       \u001b[32m2.56480\u001b[0m      0.95310      0.23901  24.93s\n",
      "     26       \u001b[36m2.44220\u001b[0m       \u001b[32m2.56316\u001b[0m      0.95281      0.24013  25.19s\n",
      "     27       \u001b[36m2.43994\u001b[0m       \u001b[32m2.56159\u001b[0m      0.95251      0.24066  25.25s\n",
      "     28       \u001b[36m2.43773\u001b[0m       \u001b[32m2.56006\u001b[0m      0.95222      0.24114  28.35s\n",
      "     29       \u001b[36m2.43559\u001b[0m       \u001b[32m2.55862\u001b[0m      0.95192      0.24176  25.75s\n",
      "     30       \u001b[36m2.43351\u001b[0m       \u001b[32m2.55727\u001b[0m      0.95161      0.24204  24.77s\n",
      "     31       \u001b[36m2.43148\u001b[0m       \u001b[32m2.55592\u001b[0m      0.95131      0.24335  24.81s\n",
      "     32       \u001b[36m2.42950\u001b[0m       \u001b[32m2.55463\u001b[0m      0.95102      0.24384  24.97s\n",
      "     33       \u001b[36m2.42758\u001b[0m       \u001b[32m2.55342\u001b[0m      0.95072      0.24374  24.77s\n",
      "     34       \u001b[36m2.42571\u001b[0m       \u001b[32m2.55224\u001b[0m      0.95043      0.24414  25.17s\n",
      "     35       \u001b[36m2.42390\u001b[0m       \u001b[32m2.55107\u001b[0m      0.95015      0.24418  24.83s\n",
      "     36       \u001b[36m2.42213\u001b[0m       \u001b[32m2.54999\u001b[0m      0.94986      0.24461  24.75s\n",
      "     37       \u001b[36m2.42041\u001b[0m       \u001b[32m2.54899\u001b[0m      0.94956      0.24480  24.88s\n",
      "     38       \u001b[36m2.41873\u001b[0m       \u001b[32m2.54804\u001b[0m      0.94925      0.24496  24.67s\n",
      "     39       \u001b[36m2.41709\u001b[0m       \u001b[32m2.54712\u001b[0m      0.94895      0.24556  24.66s\n",
      "     40       \u001b[36m2.41551\u001b[0m       \u001b[32m2.54626\u001b[0m      0.94865      0.24607  24.67s\n",
      "     41       \u001b[36m2.41395\u001b[0m       \u001b[32m2.54540\u001b[0m      0.94836      0.24638  24.67s\n",
      "     42       \u001b[36m2.41244\u001b[0m       \u001b[32m2.54461\u001b[0m      0.94806      0.24606  24.66s\n",
      "     43       \u001b[36m2.41097\u001b[0m       \u001b[32m2.54381\u001b[0m      0.94778      0.24603  25.24s\n",
      "     44       \u001b[36m2.40954\u001b[0m       \u001b[32m2.54310\u001b[0m      0.94748      0.24644  24.95s\n",
      "     45       \u001b[36m2.40814\u001b[0m       \u001b[32m2.54240\u001b[0m      0.94719      0.24649  24.68s\n",
      "     46       \u001b[36m2.40678\u001b[0m       \u001b[32m2.54175\u001b[0m      0.94690      0.24660  24.63s\n",
      "     47       \u001b[36m2.40544\u001b[0m       \u001b[32m2.54116\u001b[0m      0.94659      0.24649  24.68s\n",
      "     48       \u001b[36m2.40415\u001b[0m       \u001b[32m2.54059\u001b[0m      0.94629      0.24658  24.62s\n",
      "     49       \u001b[36m2.40289\u001b[0m       \u001b[32m2.54006\u001b[0m      0.94600      0.24695  24.51s\n",
      "     50       \u001b[36m2.40166\u001b[0m       \u001b[32m2.53956\u001b[0m      0.94570      0.24715  24.57s\n",
      "     51       \u001b[36m2.40046\u001b[0m       \u001b[32m2.53906\u001b[0m      0.94541      0.24739  25.44s\n",
      "     52       \u001b[36m2.39929\u001b[0m       \u001b[32m2.53859\u001b[0m      0.94513      0.24753  24.69s\n",
      "     53       \u001b[36m2.39815\u001b[0m       \u001b[32m2.53816\u001b[0m      0.94484      0.24721  24.60s\n",
      "     54       \u001b[36m2.39703\u001b[0m       \u001b[32m2.53767\u001b[0m      0.94458      0.24727  24.56s\n",
      "     55       \u001b[36m2.39594\u001b[0m       \u001b[32m2.53729\u001b[0m      0.94429      0.24735  24.55s\n",
      "     56       \u001b[36m2.39488\u001b[0m       \u001b[32m2.53686\u001b[0m      0.94404      0.24731  24.59s\n",
      "     57       \u001b[36m2.39385\u001b[0m       \u001b[32m2.53648\u001b[0m      0.94377      0.24735  24.58s\n",
      "     58       \u001b[36m2.39283\u001b[0m       \u001b[32m2.53614\u001b[0m      0.94350      0.24798  24.62s\n",
      "     59       \u001b[36m2.39184\u001b[0m       \u001b[32m2.53578\u001b[0m      0.94324      0.24795  24.66s\n",
      "     60       \u001b[36m2.39086\u001b[0m       \u001b[32m2.53547\u001b[0m      0.94297      0.24811  25.02s\n",
      "     61       \u001b[36m2.38991\u001b[0m       \u001b[32m2.53518\u001b[0m      0.94270      0.24822  24.82s\n",
      "     62       \u001b[36m2.38898\u001b[0m       \u001b[32m2.53494\u001b[0m      0.94242      0.24837  24.57s\n",
      "     63       \u001b[36m2.38806\u001b[0m       \u001b[32m2.53467\u001b[0m      0.94216      0.24831  24.70s\n",
      "     64       \u001b[36m2.38717\u001b[0m       \u001b[32m2.53440\u001b[0m      0.94191      0.24835  25.59s\n",
      "     65       \u001b[36m2.38629\u001b[0m       \u001b[32m2.53419\u001b[0m      0.94164      0.24840  24.60s\n",
      "     66       \u001b[36m2.38543\u001b[0m       \u001b[32m2.53396\u001b[0m      0.94138      0.24903  25.10s\n",
      "     67       \u001b[36m2.38458\u001b[0m       \u001b[32m2.53375\u001b[0m      0.94113      0.24913  24.78s\n",
      "     68       \u001b[36m2.38375\u001b[0m       \u001b[32m2.53356\u001b[0m      0.94087      0.24907  24.57s\n",
      "     69       \u001b[36m2.38294\u001b[0m       \u001b[32m2.53333\u001b[0m      0.94063      0.24904  24.59s\n",
      "     70       \u001b[36m2.38215\u001b[0m       \u001b[32m2.53317\u001b[0m      0.94038      0.24907  24.57s\n",
      "     71       \u001b[36m2.38137\u001b[0m       \u001b[32m2.53304\u001b[0m      0.94012      0.24880  25.27s\n",
      "     72       \u001b[36m2.38061\u001b[0m       \u001b[32m2.53286\u001b[0m      0.93989      0.24857  24.75s\n",
      "     73       \u001b[36m2.37986\u001b[0m       \u001b[32m2.53269\u001b[0m      0.93966      0.24847  24.71s\n",
      "     74       \u001b[36m2.37912\u001b[0m       \u001b[32m2.53257\u001b[0m      0.93941      0.24845  24.72s\n",
      "     75       \u001b[36m2.37840\u001b[0m       \u001b[32m2.53245\u001b[0m      0.93917      0.24846  24.76s\n",
      "     76       \u001b[36m2.37768\u001b[0m       \u001b[32m2.53237\u001b[0m      0.93892      0.24834  24.58s\n",
      "     77       \u001b[36m2.37698\u001b[0m       \u001b[32m2.53223\u001b[0m      0.93869      0.24832  24.63s\n",
      "     78       \u001b[36m2.37629\u001b[0m       \u001b[32m2.53213\u001b[0m      0.93846      0.24812  24.67s\n",
      "     79       \u001b[36m2.37562\u001b[0m       \u001b[32m2.53201\u001b[0m      0.93824      0.24816  24.54s\n",
      "     80       \u001b[36m2.37496\u001b[0m       \u001b[32m2.53189\u001b[0m      0.93802      0.24808  24.60s\n",
      "     81       \u001b[36m2.37430\u001b[0m       \u001b[32m2.53183\u001b[0m      0.93778      0.24783  24.69s\n",
      "     82       \u001b[36m2.37367\u001b[0m       \u001b[32m2.53174\u001b[0m      0.93756      0.24785  25.10s\n",
      "     83       \u001b[36m2.37304\u001b[0m       \u001b[32m2.53170\u001b[0m      0.93733      0.24785  26.41s\n",
      "     84       \u001b[36m2.37242\u001b[0m       \u001b[32m2.53166\u001b[0m      0.93710      0.24759  25.89s\n",
      "     85       \u001b[36m2.37181\u001b[0m       \u001b[32m2.53157\u001b[0m      0.93689      0.24756  24.74s\n",
      "     86       \u001b[36m2.37121\u001b[0m       \u001b[32m2.53151\u001b[0m      0.93668      0.24748  24.69s\n",
      "     87       \u001b[36m2.37063\u001b[0m       \u001b[32m2.53150\u001b[0m      0.93645      0.24736  24.80s\n",
      "     88       \u001b[36m2.37005\u001b[0m       \u001b[32m2.53145\u001b[0m      0.93624      0.24714  24.61s\n",
      "     89       \u001b[36m2.36947\u001b[0m       \u001b[32m2.53137\u001b[0m      0.93604      0.24712  24.69s\n",
      "     90       \u001b[36m2.36890\u001b[0m       \u001b[32m2.53135\u001b[0m      0.93582      0.24704  24.59s\n",
      "     91       \u001b[36m2.36834\u001b[0m       \u001b[32m2.53133\u001b[0m      0.93561      0.24733  24.78s\n",
      "     92       \u001b[36m2.36779\u001b[0m       \u001b[32m2.53131\u001b[0m      0.93540      0.24744  24.56s\n",
      "     93       \u001b[36m2.36725\u001b[0m       \u001b[32m2.53127\u001b[0m      0.93520      0.24751  25.47s\n",
      "     94       \u001b[36m2.36672\u001b[0m       \u001b[32m2.53124\u001b[0m      0.93500      0.24753  25.32s\n",
      "     95       \u001b[36m2.36619\u001b[0m       \u001b[32m2.53119\u001b[0m      0.93481      0.24747  24.95s\n",
      "     96       \u001b[36m2.36567\u001b[0m       \u001b[32m2.53118\u001b[0m      0.93461      0.24734  26.04s\n",
      "     97       \u001b[36m2.36516\u001b[0m       \u001b[32m2.53116\u001b[0m      0.93442      0.24746  25.10s\n",
      "     98       \u001b[36m2.36466\u001b[0m       2.53118      0.93421      0.24737  24.98s\n",
      "     99       \u001b[36m2.36416\u001b[0m       2.53117      0.93402      0.24735  24.71s\n",
      "    100       \u001b[36m2.36367\u001b[0m       2.53118      0.93382      0.24713  24.79s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc1d0>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc150>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 254),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x10c3c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c1e2c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x112c910e0>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x113a6a950>],\n",
       "     output_nonlinearity=<function softmax at 0x10c09c398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c3cc210>,\n",
       "     update=<function nesterov_momentum at 0x10c1f59b0>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_data = np.array(pd.concat([data_XRs, data_YRs], axis=1))\n",
    "num_features = np_train_data.shape[1]\n",
    "#The accuracy is: 0.26692018327\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "#print len(train_data.columns)\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 295)\n",
      "(878049,)\n",
      "# Neural Network with 167539 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      295\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.63132\u001b[0m       \u001b[32m2.62853\u001b[0m      1.00106      0.22558  29.17s\n",
      "      2       \u001b[36m2.52393\u001b[0m       \u001b[32m2.60553\u001b[0m      0.96868      0.22639  28.61s\n",
      "      3       \u001b[36m2.49870\u001b[0m       \u001b[32m2.59233\u001b[0m      0.96388      0.22789  31.51s\n",
      "      4       \u001b[36m2.48395\u001b[0m       \u001b[32m2.58444\u001b[0m      0.96112      0.22916  26.67s\n",
      "      5       \u001b[36m2.47430\u001b[0m       \u001b[32m2.57896\u001b[0m      0.95942      0.23070  27.25s\n",
      "      6       \u001b[36m2.46697\u001b[0m       \u001b[32m2.57455\u001b[0m      0.95822      0.23279  28.06s\n",
      "      7       \u001b[36m2.46080\u001b[0m       \u001b[32m2.57073\u001b[0m      0.95724      0.23409  27.99s\n",
      "      8       \u001b[36m2.45531\u001b[0m       \u001b[32m2.56732\u001b[0m      0.95637      0.23547  27.81s\n",
      "      9       \u001b[36m2.45030\u001b[0m       \u001b[32m2.56427\u001b[0m      0.95556      0.23653  27.60s\n",
      "     10       \u001b[36m2.44568\u001b[0m       \u001b[32m2.56148\u001b[0m      0.95479      0.23744  26.93s\n",
      "     11       \u001b[36m2.44139\u001b[0m       \u001b[32m2.55897\u001b[0m      0.95405      0.23859  26.67s\n",
      "     12       \u001b[36m2.43737\u001b[0m       \u001b[32m2.55657\u001b[0m      0.95338      0.23904  29.72s\n",
      "     13       \u001b[36m2.43360\u001b[0m       \u001b[32m2.55431\u001b[0m      0.95274      0.23985  27.24s\n",
      "     14       \u001b[36m2.43002\u001b[0m       \u001b[32m2.55220\u001b[0m      0.95213      0.24055  27.29s\n",
      "     15       \u001b[36m2.42661\u001b[0m       \u001b[32m2.55019\u001b[0m      0.95154      0.24150  29.07s\n",
      "     16       \u001b[36m2.42333\u001b[0m       \u001b[32m2.54828\u001b[0m      0.95097      0.24213  30.24s\n",
      "     17       \u001b[36m2.42017\u001b[0m       \u001b[32m2.54638\u001b[0m      0.95043      0.24267  27.23s\n",
      "     18       \u001b[36m2.41712\u001b[0m       \u001b[32m2.54460\u001b[0m      0.94990      0.24355  27.33s\n",
      "     19       \u001b[36m2.41417\u001b[0m       \u001b[32m2.54290\u001b[0m      0.94938      0.24435  27.52s\n",
      "     20       \u001b[36m2.41131\u001b[0m       \u001b[32m2.54123\u001b[0m      0.94888      0.24495  27.29s\n",
      "     21       \u001b[36m2.40854\u001b[0m       \u001b[32m2.53967\u001b[0m      0.94837      0.24582  27.10s\n",
      "     22       \u001b[36m2.40586\u001b[0m       \u001b[32m2.53816\u001b[0m      0.94788      0.24640  27.91s\n",
      "     23       \u001b[36m2.40324\u001b[0m       \u001b[32m2.53670\u001b[0m      0.94739      0.24627  28.08s\n",
      "     24       \u001b[36m2.40069\u001b[0m       \u001b[32m2.53529\u001b[0m      0.94691      0.24670  4407.24s\n",
      "     25       \u001b[36m2.39821\u001b[0m       \u001b[32m2.53401\u001b[0m      0.94641      0.24680  28.19s\n",
      "     26       \u001b[36m2.39579\u001b[0m       \u001b[32m2.53274\u001b[0m      0.94593      0.24729  29.24s\n",
      "     27       \u001b[36m2.39343\u001b[0m       \u001b[32m2.53151\u001b[0m      0.94546      0.24782  29.82s\n",
      "     28       \u001b[36m2.39113\u001b[0m       \u001b[32m2.53035\u001b[0m      0.94498      0.24820  30.04s\n",
      "     29       \u001b[36m2.38889\u001b[0m       \u001b[32m2.52927\u001b[0m      0.94450      0.24825  30.78s\n",
      "     30       \u001b[36m2.38669\u001b[0m       \u001b[32m2.52817\u001b[0m      0.94404      0.24859  30.41s\n",
      "     31       \u001b[36m2.38454\u001b[0m       \u001b[32m2.52710\u001b[0m      0.94359      0.24884  31.39s\n",
      "     32       \u001b[36m2.38243\u001b[0m       \u001b[32m2.52611\u001b[0m      0.94312      0.24911  27.53s\n",
      "     33       \u001b[36m2.38036\u001b[0m       \u001b[32m2.52514\u001b[0m      0.94267      0.24948  28.76s\n",
      "     34       \u001b[36m2.37834\u001b[0m       \u001b[32m2.52425\u001b[0m      0.94220      0.24961  49.17s\n",
      "     35       \u001b[36m2.37637\u001b[0m       \u001b[32m2.52341\u001b[0m      0.94173      0.24987  29.33s\n",
      "     36       \u001b[36m2.37444\u001b[0m       \u001b[32m2.52259\u001b[0m      0.94127      0.25013  29.83s\n",
      "     37       \u001b[36m2.37254\u001b[0m       \u001b[32m2.52183\u001b[0m      0.94080      0.25020  26.78s\n",
      "     38       \u001b[36m2.37068\u001b[0m       \u001b[32m2.52103\u001b[0m      0.94036      0.25020  26.96s\n",
      "     39       \u001b[36m2.36885\u001b[0m       \u001b[32m2.52030\u001b[0m      0.93991      0.25027  30.87s\n",
      "     40       \u001b[36m2.36705\u001b[0m       \u001b[32m2.51957\u001b[0m      0.93947      0.25053  27.50s\n",
      "     41       \u001b[36m2.36528\u001b[0m       \u001b[32m2.51887\u001b[0m      0.93903      0.25089  27.18s\n",
      "     42       \u001b[36m2.36355\u001b[0m       \u001b[32m2.51823\u001b[0m      0.93858      0.25107  29.37s\n",
      "     43       \u001b[36m2.36184\u001b[0m       \u001b[32m2.51756\u001b[0m      0.93815      0.25125  30.24s\n",
      "     44       \u001b[36m2.36016\u001b[0m       \u001b[32m2.51698\u001b[0m      0.93770      0.25145  30.64s\n",
      "     45       \u001b[36m2.35852\u001b[0m       \u001b[32m2.51643\u001b[0m      0.93725      0.25168  30.63s\n",
      "     46       \u001b[36m2.35689\u001b[0m       \u001b[32m2.51588\u001b[0m      0.93681      0.25164  30.11s\n",
      "     47       \u001b[36m2.35530\u001b[0m       \u001b[32m2.51533\u001b[0m      0.93638      0.25181  29.28s\n",
      "     48       \u001b[36m2.35373\u001b[0m       \u001b[32m2.51483\u001b[0m      0.93594      0.25187  28.52s\n",
      "     49       \u001b[36m2.35218\u001b[0m       \u001b[32m2.51435\u001b[0m      0.93550      0.25205  28.39s\n",
      "     50       \u001b[36m2.35065\u001b[0m       \u001b[32m2.51388\u001b[0m      0.93507      0.25213  28.93s\n",
      "     51       \u001b[36m2.34915\u001b[0m       \u001b[32m2.51346\u001b[0m      0.93463      0.25210  29.59s\n",
      "     52       \u001b[36m2.34766\u001b[0m       \u001b[32m2.51303\u001b[0m      0.93420      0.25217  29.65s\n",
      "     53       \u001b[36m2.34620\u001b[0m       \u001b[32m2.51266\u001b[0m      0.93375      0.25241  30.83s\n",
      "     54       \u001b[36m2.34477\u001b[0m       \u001b[32m2.51230\u001b[0m      0.93332      0.25224  30.00s\n",
      "     55       \u001b[36m2.34335\u001b[0m       \u001b[32m2.51198\u001b[0m      0.93287      0.25257  29.21s\n",
      "     56       \u001b[36m2.34195\u001b[0m       \u001b[32m2.51170\u001b[0m      0.93242      0.25240  29.62s\n",
      "     57       \u001b[36m2.34057\u001b[0m       \u001b[32m2.51147\u001b[0m      0.93196      0.25228  28.76s\n",
      "     58       \u001b[36m2.33922\u001b[0m       \u001b[32m2.51120\u001b[0m      0.93151      0.25227  28.92s\n",
      "     59       \u001b[36m2.33787\u001b[0m       \u001b[32m2.51098\u001b[0m      0.93106      0.25253  29.89s\n",
      "     60       \u001b[36m2.33654\u001b[0m       \u001b[32m2.51077\u001b[0m      0.93061      0.25246  29.51s\n",
      "     61       \u001b[36m2.33523\u001b[0m       \u001b[32m2.51064\u001b[0m      0.93013      0.25251  29.18s\n",
      "     62       \u001b[36m2.33394\u001b[0m       \u001b[32m2.51044\u001b[0m      0.92969      0.25244  29.61s\n",
      "     63       \u001b[36m2.33267\u001b[0m       \u001b[32m2.51032\u001b[0m      0.92923      0.25240  30.16s\n",
      "     64       \u001b[36m2.33141\u001b[0m       \u001b[32m2.51018\u001b[0m      0.92878      0.25235  28.81s\n",
      "     65       \u001b[36m2.33016\u001b[0m       \u001b[32m2.51004\u001b[0m      0.92833      0.25244  29.82s\n",
      "     66       \u001b[36m2.32892\u001b[0m       \u001b[32m2.50993\u001b[0m      0.92789      0.25248  46.72s\n",
      "     67       \u001b[36m2.32770\u001b[0m       \u001b[32m2.50982\u001b[0m      0.92744      0.25237  30.61s\n",
      "     68       \u001b[36m2.32650\u001b[0m       \u001b[32m2.50974\u001b[0m      0.92699      0.25250  30.36s\n",
      "     69       \u001b[36m2.32531\u001b[0m       \u001b[32m2.50968\u001b[0m      0.92653      0.25259  31.22s\n",
      "     70       \u001b[36m2.32413\u001b[0m       \u001b[32m2.50963\u001b[0m      0.92608      0.25267  27.88s\n",
      "     71       \u001b[36m2.32296\u001b[0m       \u001b[32m2.50953\u001b[0m      0.92566      0.25256  29.52s\n",
      "     72       \u001b[36m2.32181\u001b[0m       \u001b[32m2.50949\u001b[0m      0.92521      0.25259  28.82s\n",
      "     73       \u001b[36m2.32068\u001b[0m       \u001b[32m2.50944\u001b[0m      0.92478      0.25265  27.80s\n",
      "     74       \u001b[36m2.31954\u001b[0m       \u001b[32m2.50942\u001b[0m      0.92434      0.25265  28.35s\n",
      "     75       \u001b[36m2.31843\u001b[0m       \u001b[32m2.50941\u001b[0m      0.92389      0.25269  29.45s\n",
      "     76       \u001b[36m2.31732\u001b[0m       \u001b[32m2.50934\u001b[0m      0.92348      0.25270  29.53s\n",
      "     77       \u001b[36m2.31623\u001b[0m       \u001b[32m2.50933\u001b[0m      0.92305      0.25267  28.33s\n",
      "     78       \u001b[36m2.31516\u001b[0m       2.50935      0.92261      0.25267  28.09s\n",
      "     79       \u001b[36m2.31409\u001b[0m       2.50938      0.92218      0.25270  28.99s\n",
      "     80       \u001b[36m2.31303\u001b[0m       2.50942      0.92174      0.25261  28.21s\n",
      "     81       \u001b[36m2.31199\u001b[0m       2.50939      0.92134      0.25285  28.68s\n",
      "     82       \u001b[36m2.31096\u001b[0m       2.50944      0.92091      0.25270  30.92s\n",
      "     83       \u001b[36m2.30995\u001b[0m       2.50953      0.92047      0.25258  30.43s\n",
      "     84       \u001b[36m2.30894\u001b[0m       2.50960      0.92004      0.25258  18999.85s\n",
      "     85       \u001b[36m2.30794\u001b[0m       2.50970      0.91961      0.25254  29.23s\n",
      "     86       \u001b[36m2.30695\u001b[0m       2.50979      0.91918      0.25241  29.50s\n",
      "     87       \u001b[36m2.30597\u001b[0m       2.50985      0.91877      0.25245  29.28s\n",
      "     88       \u001b[36m2.30500\u001b[0m       2.50997      0.91834      0.25255  27.56s\n",
      "     89       \u001b[36m2.30403\u001b[0m       2.51006      0.91792      0.25271  27.81s\n",
      "     90       \u001b[36m2.30308\u001b[0m       2.51015      0.91751      0.25261  29.33s\n",
      "     91       \u001b[36m2.30214\u001b[0m       2.51030      0.91708      0.25252  28.20s\n",
      "     92       \u001b[36m2.30121\u001b[0m       2.51040      0.91667      0.25247  27.98s\n",
      "     93       \u001b[36m2.30029\u001b[0m       2.51054      0.91625      0.25225  27.74s\n",
      "     94       \u001b[36m2.29937\u001b[0m       2.51072      0.91582      0.25214  27.73s\n",
      "     95       \u001b[36m2.29847\u001b[0m       2.51084      0.91542      0.25233  27.96s\n",
      "     96       \u001b[36m2.29758\u001b[0m       2.51095      0.91502      0.25228  28.06s\n",
      "     97       \u001b[36m2.29669\u001b[0m       2.51111      0.91461      0.25209  27.71s\n",
      "     98       \u001b[36m2.29582\u001b[0m       2.51128      0.91420      0.25209  27.66s\n",
      "     99       \u001b[36m2.29495\u001b[0m       2.51147      0.91379      0.25208  28.68s\n",
      "    100       \u001b[36m2.29410\u001b[0m       2.51159      0.91340      0.25211  27.92s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc1d0>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc150>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 295),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x10c3c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c1e2c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x114963878>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x114aeb710>],\n",
       "     output_nonlinearity=<function softmax at 0x10c09c398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c3cc210>,\n",
       "     update=<function nesterov_momentum at 0x10c1f59b0>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "# The accuracy is: 0.283077595897\n",
    "# Should run again stopping early @ 77 for better results\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "#print len(train_data.columns)\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 296)\n",
      "(884262, 308)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 296)\n",
      "(878049,)\n",
      "# Neural Network with 168039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      296\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.62875\u001b[0m       \u001b[32m2.62633\u001b[0m      1.00092      0.22510  29.49s\n",
      "      2       \u001b[36m2.52224\u001b[0m       \u001b[32m2.60240\u001b[0m      0.96920      0.22908  29.07s\n",
      "      3       \u001b[36m2.49736\u001b[0m       \u001b[32m2.58764\u001b[0m      0.96511      0.23085  30.53s\n",
      "      4       \u001b[36m2.48292\u001b[0m       \u001b[32m2.57807\u001b[0m      0.96309      0.23283  32.36s\n",
      "      5       \u001b[36m2.47333\u001b[0m       \u001b[32m2.57122\u001b[0m      0.96193      0.23478  30.58s\n",
      "      6       \u001b[36m2.46596\u001b[0m       \u001b[32m2.56577\u001b[0m      0.96110      0.23661  29.43s\n",
      "      7       \u001b[36m2.45976\u001b[0m       \u001b[32m2.56136\u001b[0m      0.96033      0.23805  27.92s\n",
      "      8       \u001b[36m2.45424\u001b[0m       \u001b[32m2.55749\u001b[0m      0.95963      0.23935  27.95s\n",
      "      9       \u001b[36m2.44921\u001b[0m       \u001b[32m2.55418\u001b[0m      0.95890      0.24044  28.26s\n",
      "     10       \u001b[36m2.44456\u001b[0m       \u001b[32m2.55124\u001b[0m      0.95818      0.24127  28.62s\n",
      "     11       \u001b[36m2.44022\u001b[0m       \u001b[32m2.54858\u001b[0m      0.95748      0.24231  28.57s\n",
      "     12       \u001b[36m2.43614\u001b[0m       \u001b[32m2.54617\u001b[0m      0.95679      0.24372  27.71s\n",
      "     13       \u001b[36m2.43229\u001b[0m       \u001b[32m2.54374\u001b[0m      0.95619      0.24480  30.99s\n",
      "     14       \u001b[36m2.42862\u001b[0m       \u001b[32m2.54142\u001b[0m      0.95562      0.24607  28.61s\n",
      "     15       \u001b[36m2.42513\u001b[0m       \u001b[32m2.53924\u001b[0m      0.95506      0.24678  28.15s\n",
      "     16       \u001b[36m2.42178\u001b[0m       \u001b[32m2.53721\u001b[0m      0.95450      0.24760  27.99s\n",
      "     17       \u001b[36m2.41856\u001b[0m       \u001b[32m2.53518\u001b[0m      0.95400      0.24827  29.83s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc1d0>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc150>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 296),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x10c3c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c1e2c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x114b3a758>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x1171d6290>],\n",
       "     output_nonlinearity=<function softmax at 0x10c09c398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c3cc210>,\n",
       "     update=<function nesterov_momentum at 0x10c1f59b0>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "# np_train_data = np.array(pd.concat([train_data[features], (data.Year < 2008) * 1, data_XRs, data_YRs], axis=1))\n",
    "\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "#print len(train_data.columns)\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 297)\n",
      "(878049,)\n",
      "# Neural Network with 168539 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      297\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.62592\u001b[0m       \u001b[32m2.62294\u001b[0m      1.00114      0.22713  29.77s\n",
      "      2       \u001b[36m2.51960\u001b[0m       \u001b[32m2.59705\u001b[0m      0.97018      0.22995  31.28s\n",
      "      3       \u001b[36m2.49565\u001b[0m       \u001b[32m2.58266\u001b[0m      0.96631      0.23189  30.75s\n",
      "      4       \u001b[36m2.48211\u001b[0m       \u001b[32m2.57427\u001b[0m      0.96420      0.23377  32.86s\n",
      "      5       \u001b[36m2.47307\u001b[0m       \u001b[32m2.56915\u001b[0m      0.96261      0.23547  28.59s\n",
      "      6       \u001b[36m2.46605\u001b[0m       \u001b[32m2.56581\u001b[0m      0.96112      0.23733  28.48s\n",
      "      7       \u001b[36m2.46002\u001b[0m       \u001b[32m2.56332\u001b[0m      0.95970      0.23939  29.78s\n",
      "      8       \u001b[36m2.45459\u001b[0m       \u001b[32m2.56129\u001b[0m      0.95834      0.24093  28.83s\n",
      "      9       \u001b[36m2.44958\u001b[0m       \u001b[32m2.55955\u001b[0m      0.95703      0.24215  28.69s\n",
      "     10       \u001b[36m2.44492\u001b[0m       \u001b[32m2.55793\u001b[0m      0.95582      0.24364  31.31s\n",
      "     11       \u001b[36m2.44056\u001b[0m       \u001b[32m2.55616\u001b[0m      0.95477      0.24489  30.43s\n",
      "     12       \u001b[36m2.43645\u001b[0m       \u001b[32m2.55446\u001b[0m      0.95380      0.24605  32.35s\n",
      "     13       \u001b[36m2.43256\u001b[0m       \u001b[32m2.55269\u001b[0m      0.95294      0.24704  29.12s\n",
      "     14       \u001b[36m2.42884\u001b[0m       \u001b[32m2.55100\u001b[0m      0.95211      0.24797  28.51s\n",
      "     15       \u001b[36m2.42528\u001b[0m       \u001b[32m2.54934\u001b[0m      0.95134      0.24900  28.80s\n",
      "     16       \u001b[36m2.42185\u001b[0m       \u001b[32m2.54758\u001b[0m      0.95065      0.24947  32.57s\n",
      "     17       \u001b[36m2.41856\u001b[0m       \u001b[32m2.54595\u001b[0m      0.94996      0.25033  28.59s\n",
      "     18       \u001b[36m2.41539\u001b[0m       \u001b[32m2.54426\u001b[0m      0.94935      0.25108  28.54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc1d0>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc150>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 297),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x10c3c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c1e2c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x12193a128>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x121938098>],\n",
       "     output_nonlinearity=<function softmax at 0x10c09c398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c3cc210>,\n",
       "     update=<function nesterov_momentum at 0x10c1f59b0>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "# np_train_data = np.array(pd.concat([train_data[features], (data.Year < 2008) * 1, (data.Year < 2010) * 1, data_XRs, data_YRs], axis=1))\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "#print len(train_data.columns)\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 298)\n",
      "(878049,)\n",
      "# Neural Network with 169039 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      298\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.62809\u001b[0m       \u001b[32m2.62996\u001b[0m      0.99929      0.22293  29.86s\n",
      "      2       \u001b[36m2.52001\u001b[0m       \u001b[32m2.60081\u001b[0m      0.96893      0.22731  31.13s\n",
      "      3       \u001b[36m2.49587\u001b[0m       \u001b[32m2.58352\u001b[0m      0.96607      0.23178  31.61s\n",
      "      4       \u001b[36m2.48171\u001b[0m       \u001b[32m2.57370\u001b[0m      0.96426      0.23485  35.16s\n",
      "      5       \u001b[36m2.47211\u001b[0m       \u001b[32m2.56818\u001b[0m      0.96259      0.23673  29.39s\n",
      "      6       \u001b[36m2.46465\u001b[0m       \u001b[32m2.56479\u001b[0m      0.96096      0.23867  28.68s\n",
      "      7       \u001b[36m2.45834\u001b[0m       \u001b[32m2.56239\u001b[0m      0.95939      0.23985  29.94s\n",
      "      8       \u001b[36m2.45272\u001b[0m       \u001b[32m2.56042\u001b[0m      0.95794      0.24137  29.30s\n",
      "      9       \u001b[36m2.44761\u001b[0m       \u001b[32m2.55861\u001b[0m      0.95662      0.24299  29.00s\n",
      "     10       \u001b[36m2.44287\u001b[0m       \u001b[32m2.55680\u001b[0m      0.95544      0.24424  28.97s\n",
      "     11       \u001b[36m2.43847\u001b[0m       \u001b[32m2.55493\u001b[0m      0.95442      0.24550  28.80s\n",
      "     12       \u001b[36m2.43433\u001b[0m       \u001b[32m2.55309\u001b[0m      0.95349      0.24651  29.12s\n",
      "     13       \u001b[36m2.43042\u001b[0m       \u001b[32m2.55134\u001b[0m      0.95261      0.24754  29.28s\n",
      "     14       \u001b[36m2.42669\u001b[0m       \u001b[32m2.54960\u001b[0m      0.95179      0.24877  28.72s\n",
      "     15       \u001b[36m2.42312\u001b[0m       \u001b[32m2.54787\u001b[0m      0.95104      0.24942  28.79s\n",
      "     16       \u001b[36m2.41969\u001b[0m       \u001b[32m2.54619\u001b[0m      0.95032      0.25034  28.69s\n",
      "     17       \u001b[36m2.41637\u001b[0m       \u001b[32m2.54457\u001b[0m      0.94962      0.25101  29.19s\n",
      "     18       \u001b[36m2.41317\u001b[0m       \u001b[32m2.54301\u001b[0m      0.94894      0.25175  28.79s\n",
      "     19       \u001b[36m2.41008\u001b[0m       \u001b[32m2.54149\u001b[0m      0.94829      0.25247  29.27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc1d0>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c3cc150>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 298),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x10c3c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c1e2c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x11c653fc8>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x11c659cf8>],\n",
       "     output_nonlinearity=<function softmax at 0x10c09c398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c3cc210>,\n",
       "     update=<function nesterov_momentum at 0x10c1f59b0>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "# np_train_data = np.array(pd.concat([train_data[features], (data.Year < 2006) * 1, (data.Year < 2008) * 1, (data.Year < 2010) * 1, data_XRs, data_YRs], axis=1))\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "#print len(train_data.columns)\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 299)\n",
      "(878049,)\n",
      "# Neural Network with 169539 learnable parameters\n",
      "\n",
      "## Layer information\n",
      "\n",
      "  #  name      size\n",
      "---  ------  ------\n",
      "  0  input      299\n",
      "  1  hidden     500\n",
      "  2  output      39\n",
      "\n",
      "  epoch    train loss    valid loss    train/val    valid acc  dur\n",
      "-------  ------------  ------------  -----------  -----------  ------\n",
      "      1       \u001b[36m2.62886\u001b[0m       \u001b[32m2.63097\u001b[0m      0.99920      0.22816  30.40s\n",
      "      2       \u001b[36m2.52144\u001b[0m       \u001b[32m2.60047\u001b[0m      0.96961      0.23028  28.56s\n",
      "      3       \u001b[36m2.49645\u001b[0m       \u001b[32m2.58330\u001b[0m      0.96638      0.23256  27.83s\n",
      "      4       \u001b[36m2.48195\u001b[0m       \u001b[32m2.57386\u001b[0m      0.96429      0.23344  27.19s\n",
      "      5       \u001b[36m2.47231\u001b[0m       \u001b[32m2.56853\u001b[0m      0.96254      0.23579  26.93s\n",
      "      6       \u001b[36m2.46488\u001b[0m       \u001b[32m2.56516\u001b[0m      0.96091      0.23799  28.16s\n",
      "      7       \u001b[36m2.45856\u001b[0m       \u001b[32m2.56277\u001b[0m      0.95934      0.23993  27.66s\n",
      "      8       \u001b[36m2.45292\u001b[0m       \u001b[32m2.56084\u001b[0m      0.95786      0.24158  27.92s\n",
      "      9       \u001b[36m2.44778\u001b[0m       \u001b[32m2.55915\u001b[0m      0.95648      0.24310  27.23s\n",
      "     10       \u001b[36m2.44303\u001b[0m       \u001b[32m2.55763\u001b[0m      0.95519      0.24446  27.36s\n",
      "     11       \u001b[36m2.43862\u001b[0m       \u001b[32m2.55617\u001b[0m      0.95401      0.24532  28.29s\n",
      "     12       \u001b[36m2.43447\u001b[0m       \u001b[32m2.55467\u001b[0m      0.95295      0.24619  27.28s\n",
      "     13       \u001b[36m2.43055\u001b[0m       \u001b[32m2.55317\u001b[0m      0.95197      0.24702  27.32s\n",
      "     14       \u001b[36m2.42682\u001b[0m       \u001b[32m2.55157\u001b[0m      0.95111      0.24788  27.32s\n",
      "     15       \u001b[36m2.42326\u001b[0m       \u001b[32m2.55001\u001b[0m      0.95029      0.24872  27.10s\n",
      "     16       \u001b[36m2.41984\u001b[0m       \u001b[32m2.54847\u001b[0m      0.94953      0.24963  27.50s\n",
      "     17       \u001b[36m2.41655\u001b[0m       \u001b[32m2.54706\u001b[0m      0.94876      0.25022  26.98s\n",
      "     18       \u001b[36m2.41336\u001b[0m       \u001b[32m2.54573\u001b[0m      0.94800      0.25062  27.49s\n",
      "     19       \u001b[36m2.41028\u001b[0m       \u001b[32m2.54435\u001b[0m      0.94731      0.25130  27.63s\n",
      "     20       \u001b[36m2.40729\u001b[0m       \u001b[32m2.54290\u001b[0m      0.94667      0.25193  27.99s\n",
      "     21       \u001b[36m2.40440\u001b[0m       \u001b[32m2.54151\u001b[0m      0.94605      0.25223  29.45s\n",
      "     22       \u001b[36m2.40160\u001b[0m       \u001b[32m2.54005\u001b[0m      0.94549      0.25274  28.75s\n",
      "     23       \u001b[36m2.39888\u001b[0m       \u001b[32m2.53877\u001b[0m      0.94490      0.25276  33.57s\n",
      "     24       \u001b[36m2.39625\u001b[0m       \u001b[32m2.53741\u001b[0m      0.94437      0.25348  29.29s\n",
      "     25       \u001b[36m2.39367\u001b[0m       \u001b[32m2.53615\u001b[0m      0.94382      0.25414  26.98s\n",
      "     26       \u001b[36m2.39118\u001b[0m       \u001b[32m2.53492\u001b[0m      0.94329      0.25474  32.27s\n",
      "     27       \u001b[36m2.38875\u001b[0m       \u001b[32m2.53389\u001b[0m      0.94272      0.25547  28.33s\n",
      "     28       \u001b[36m2.38637\u001b[0m       \u001b[32m2.53271\u001b[0m      0.94222      0.25583  29.21s\n",
      "     29       \u001b[36m2.38406\u001b[0m       \u001b[32m2.53153\u001b[0m      0.94175      0.25645  31.52s\n",
      "     30       \u001b[36m2.38180\u001b[0m       \u001b[32m2.53042\u001b[0m      0.94127      0.25667  27.78s\n",
      "     31       \u001b[36m2.37960\u001b[0m       \u001b[32m2.52935\u001b[0m      0.94080      0.25714  27.40s\n",
      "     32       \u001b[36m2.37744\u001b[0m       \u001b[32m2.52841\u001b[0m      0.94029      0.25757  27.39s\n",
      "     33       \u001b[36m2.37534\u001b[0m       \u001b[32m2.52742\u001b[0m      0.93983      0.25793  27.56s\n",
      "     34       \u001b[36m2.37328\u001b[0m       \u001b[32m2.52644\u001b[0m      0.93937      0.25814  27.09s\n",
      "     35       \u001b[36m2.37126\u001b[0m       \u001b[32m2.52560\u001b[0m      0.93889      0.25817  27.48s\n",
      "     36       \u001b[36m2.36929\u001b[0m       \u001b[32m2.52477\u001b[0m      0.93842      0.25850  27.65s\n",
      "     37       \u001b[36m2.36735\u001b[0m       \u001b[32m2.52399\u001b[0m      0.93794      0.25867  27.40s\n",
      "     38       \u001b[36m2.36545\u001b[0m       \u001b[32m2.52316\u001b[0m      0.93750      0.25883  27.33s\n",
      "     39       \u001b[36m2.36359\u001b[0m       \u001b[32m2.52242\u001b[0m      0.93704      0.25913  27.27s\n",
      "     40       \u001b[36m2.36176\u001b[0m       \u001b[32m2.52176\u001b[0m      0.93655      0.25942  27.21s\n",
      "     41       \u001b[36m2.35996\u001b[0m       \u001b[32m2.52124\u001b[0m      0.93603      0.25960  27.55s\n",
      "     42       \u001b[36m2.35819\u001b[0m       \u001b[32m2.52069\u001b[0m      0.93553      0.25979  27.77s\n",
      "     43       \u001b[36m2.35646\u001b[0m       \u001b[32m2.52016\u001b[0m      0.93504      0.26015  27.81s\n",
      "     44       \u001b[36m2.35475\u001b[0m       \u001b[32m2.51959\u001b[0m      0.93457      0.26045  27.20s\n",
      "     45       \u001b[36m2.35307\u001b[0m       \u001b[32m2.51920\u001b[0m      0.93406      0.26064  27.56s\n",
      "     46       \u001b[36m2.35143\u001b[0m       \u001b[32m2.51878\u001b[0m      0.93356      0.26070  27.39s\n",
      "     47       \u001b[36m2.34981\u001b[0m       \u001b[32m2.51844\u001b[0m      0.93304      0.26086  28.68s\n",
      "     48       \u001b[36m2.34821\u001b[0m       \u001b[32m2.51805\u001b[0m      0.93255      0.26092  29.48s\n",
      "     49       \u001b[36m2.34662\u001b[0m       \u001b[32m2.51769\u001b[0m      0.93205      0.26107  30.76s\n",
      "     50       \u001b[36m2.34507\u001b[0m       \u001b[32m2.51748\u001b[0m      0.93151      0.26111  28.68s\n",
      "     51       \u001b[36m2.34353\u001b[0m       \u001b[32m2.51718\u001b[0m      0.93101      0.26118  27.44s\n",
      "     52       \u001b[36m2.34202\u001b[0m       \u001b[32m2.51693\u001b[0m      0.93050      0.26137  30.69s\n",
      "     53       \u001b[36m2.34052\u001b[0m       \u001b[32m2.51669\u001b[0m      0.93000      0.26135  27.48s\n",
      "     54       \u001b[36m2.33906\u001b[0m       \u001b[32m2.51659\u001b[0m      0.92946      0.26132  29.99s\n",
      "     55       \u001b[36m2.33760\u001b[0m       \u001b[32m2.51649\u001b[0m      0.92891      0.26145  27.23s\n",
      "     56       \u001b[36m2.33616\u001b[0m       \u001b[32m2.51623\u001b[0m      0.92844      0.26152  28.66s\n",
      "     57       \u001b[36m2.33474\u001b[0m       \u001b[32m2.51604\u001b[0m      0.92794      0.26165  28.56s\n",
      "     58       \u001b[36m2.33335\u001b[0m       \u001b[32m2.51576\u001b[0m      0.92749      0.26174  28.79s\n",
      "     59       \u001b[36m2.33197\u001b[0m       \u001b[32m2.51560\u001b[0m      0.92701      0.26168  28.95s\n",
      "     60       \u001b[36m2.33061\u001b[0m       \u001b[32m2.51546\u001b[0m      0.92652      0.26173  27.55s\n",
      "     61       \u001b[36m2.32927\u001b[0m       \u001b[32m2.51534\u001b[0m      0.92603      0.26175  28.20s\n",
      "     62       \u001b[36m2.32795\u001b[0m       \u001b[32m2.51521\u001b[0m      0.92555      0.26181  28.21s\n",
      "     63       \u001b[36m2.32664\u001b[0m       \u001b[32m2.51519\u001b[0m      0.92504      0.26184  28.28s\n",
      "     64       \u001b[36m2.32535\u001b[0m       \u001b[32m2.51513\u001b[0m      0.92455      0.26199  28.36s\n",
      "     65       \u001b[36m2.32408\u001b[0m       2.51524      0.92400      0.26210  28.32s\n",
      "     66       \u001b[36m2.32282\u001b[0m       2.51521      0.92351      0.26211  28.22s\n",
      "     67       \u001b[36m2.32158\u001b[0m       2.51519      0.92302      0.26210  28.43s\n",
      "     68       \u001b[36m2.32034\u001b[0m       2.51528      0.92250      0.26207  28.23s\n",
      "     69       \u001b[36m2.31914\u001b[0m       2.51519      0.92205      0.26217  30.98s\n",
      "     70       \u001b[36m2.31794\u001b[0m       2.51526      0.92155      0.26217  28.97s\n",
      "     71       \u001b[36m2.31676\u001b[0m       2.51530      0.92107      0.26215  28.19s\n",
      "     72       \u001b[36m2.31559\u001b[0m       2.51542      0.92056      0.26211  29.40s\n",
      "     73       \u001b[36m2.31443\u001b[0m       2.51557      0.92004      0.26209  29.03s\n",
      "     74       \u001b[36m2.31329\u001b[0m       2.51569      0.91955      0.26209  31.04s\n",
      "     75       \u001b[36m2.31215\u001b[0m       2.51585      0.91904      0.26215  30.60s\n",
      "     76       \u001b[36m2.31103\u001b[0m       2.51589      0.91857      0.26219  28.93s\n",
      "     77       \u001b[36m2.30993\u001b[0m       2.51598      0.91810      0.26227  27.72s\n",
      "     78       \u001b[36m2.30883\u001b[0m       2.51606      0.91764      0.26230  32.63s\n",
      "     79       \u001b[36m2.30775\u001b[0m       2.51616      0.91717      0.26212  31.87s\n",
      "     80       \u001b[36m2.30669\u001b[0m       2.51635      0.91668      0.26212  32.08s\n",
      "     81       \u001b[36m2.30563\u001b[0m       2.51628      0.91629      0.26214  28.17s\n",
      "     82       \u001b[36m2.30459\u001b[0m       2.51650      0.91579      0.26212  29.07s\n",
      "     83       \u001b[36m2.30355\u001b[0m       2.51670      0.91531      0.26203  29.67s\n",
      "     84       \u001b[36m2.30253\u001b[0m       2.51683      0.91485      0.26198  28.94s\n",
      "     85       \u001b[36m2.30151\u001b[0m       2.51711      0.91435      0.26178  29.10s\n",
      "     86       \u001b[36m2.30051\u001b[0m       2.51730      0.91388      0.26181  29.40s\n",
      "     87       \u001b[36m2.29950\u001b[0m       2.51750      0.91341      0.26169  28.32s\n",
      "     88       \u001b[36m2.29852\u001b[0m       2.51766      0.91296      0.26159  30.95s\n",
      "     89       \u001b[36m2.29754\u001b[0m       2.51795      0.91246      0.26165  28.47s\n",
      "     90       \u001b[36m2.29656\u001b[0m       2.51819      0.91199      0.26182  30.91s\n",
      "     91       \u001b[36m2.29560\u001b[0m       2.51834      0.91155      0.26180  28.80s\n",
      "     92       \u001b[36m2.29465\u001b[0m       2.51844      0.91114      0.26178  27.88s\n",
      "     93       \u001b[36m2.29371\u001b[0m       2.51885      0.91062      0.26173  28.16s\n",
      "     94       \u001b[36m2.29278\u001b[0m       2.51909      0.91016      0.26173  28.00s\n",
      "     95       \u001b[36m2.29185\u001b[0m       2.51931      0.90971      0.26181  30.40s\n",
      "     96       \u001b[36m2.29094\u001b[0m       2.51951      0.90928      0.26166  28.98s\n",
      "     97       \u001b[36m2.29003\u001b[0m       2.51982      0.90881      0.26164  28.04s\n",
      "     98       \u001b[36m2.28913\u001b[0m       2.52018      0.90832      0.26153  29.61s\n",
      "     99       \u001b[36m2.28824\u001b[0m       2.52042      0.90788      0.26143  28.53s\n",
      "    100       \u001b[36m2.28735\u001b[0m       2.52085      0.90737      0.26141  27.63s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNet(X_tensor_type=None,\n",
       "     batch_iterator_test=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc210>,\n",
       "     batch_iterator_train=<nolearn.lasagne.base.BatchIterator object at 0x10c1cc190>,\n",
       "     custom_score=None, hidden_num_units=500, input_shape=(None, 299),\n",
       "     layers=[('input', <class 'lasagne.layers.input.InputLayer'>), ('hidden', <class 'lasagne.layers.dense.DenseLayer'>), ('output', <class 'lasagne.layers.dense.DenseLayer'>)],\n",
       "     loss=None, max_epochs=100, more_params={},\n",
       "     objective=<function objective at 0x10c1c5c80>,\n",
       "     objective_loss_function=<function categorical_crossentropy at 0x10c0e5c80>,\n",
       "     on_epoch_finished=[<nolearn.lasagne.handlers.PrintLog instance at 0x10c110b90>],\n",
       "     on_training_finished=[],\n",
       "     on_training_started=[<nolearn.lasagne.handlers.PrintLayerInfo instance at 0x118cca050>],\n",
       "     output_nonlinearity=<function softmax at 0x10b79f398>,\n",
       "     output_num_units=39, regression=False,\n",
       "     train_split=<nolearn.lasagne.base.TrainSplit object at 0x10c1cc250>,\n",
       "     update=<function nesterov_momentum at 0x10c0f89b0>,\n",
       "     update_learning_rate=0.002, update_momentum=0.9,\n",
       "     use_label_encoder=False, verbose=1,\n",
       "     y_tensor_type=TensorType(int32, vector))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features = ['Jan','Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM', '6PM', '7PM', '8PM', '9PM', '10PM', '11PM', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION', 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']\n",
    "# np_train_data = np.array(pd.concat([train_data[features], (data.Year < 2006) * 1, (data.Year < 2008) * 1, (data.Year < 2010) * 1, data_XR3s, data_YR3s], axis=1))\n",
    "# The accuracy is: 0.293386815542\n",
    "# Should run again stopping early @ 65 for better results\n",
    "num_features = np_train_data.shape[1]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        ('input', layers.InputLayer),\n",
    "        ('hidden', layers.DenseLayer),\n",
    "        ('output', layers.DenseLayer),\n",
    "        ],\n",
    "    # layer parameters:\n",
    "    input_shape=(None, num_features),  # 3 input pixels per batch\n",
    "    hidden_num_units=500,  # number of units in hidden layer\n",
    "    output_nonlinearity=nonlinearities.softmax,\n",
    "    output_num_units=39,  # 39 target values\n",
    "\n",
    "    # optimization method:\n",
    "    update=nesterov_momentum,\n",
    "    update_learning_rate=0.002,\n",
    "    update_momentum=0.9,\n",
    "\n",
    "    regression=False,  # flag to indicate we're not dealing with regression problem\n",
    "    max_epochs=100,  # we want to train this many epochs\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "print np_train_data.shape\n",
    "print train_labels.shape\n",
    "#print len(train_data.columns)\n",
    "net1.fit(np_train_data, train_labels_int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
